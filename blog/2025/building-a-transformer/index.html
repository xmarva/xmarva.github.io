<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PR5ZCQWED0"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PR5ZCQWED0");</script> <script src="/assets/js/h2-cards.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Building a Transformer | Eva Koroleva </title> <meta name="author" content="Eva Koroleva"> <meta name="description" content="Learn and implement the most iconic architecture in modern deep learning."> <meta name="keywords" content="machine-learning, deep-learning, python, neural-networks"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%BD&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xmarva.github.io/blog/2025/building-a-transformer/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eva</span> Koroleva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Building a Transformer</h1> <p class="post-meta"> Created in April 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning,</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers,</a>   <a href="/blog/tag/multihead-attention"> <i class="fa-solid fa-hashtag fa-sm"></i> multihead-attention,</a>   <a href="/blog/tag/positional-encoding"> <i class="fa-solid fa-hashtag fa-sm"></i> positional-encoding,</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   ·   <a href="/blog/category/featured-posts"> <i class="fa-solid fa-tag fa-sm"></i> featured-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="building-a-transformer">Building a Transformer</h2> <p>The Transformer architecture was a groundbreaking development in the field of sequence processing. Unlike traditional models like RNNs and LSTMs, which process data sequentially, the Transformer uses an attention mechanism that allows it to process the entire sequence in parallel. This dramatically speeds up training and improves performance.</p> <p>The key innovation of the Transformer is the use of self-attention, which enables the model to effectively take into account the context of each word or token, regardless of its position in the sequence. This architecture has become the foundation for many modern models, including BERT, GPT, and others, significantly improving the performance of natural language processing tasks.</p> <p>We’ll need to install a few libraries (and import even more), and as we go, the list will only grow. You can safely ignore these setup cells and just run them. Focus on the main code instead.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">torchdata</span><span class="o">==</span><span class="mf">0.3</span><span class="p">.</span><span class="mi">0</span> <span class="n">torchtext</span><span class="o">==</span><span class="mf">0.12</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2</span> <span class="n">altair</span><span class="o">==</span><span class="mf">5.5</span><span class="p">.</span><span class="mi">0</span> <span class="n">GPUtil</span><span class="o">==</span><span class="mf">1.4</span><span class="p">.</span><span class="mi">0</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">de_core_news_sm</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_sm</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">GPUtil</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">import</span> <span class="n">altair</span> <span class="k">as</span> <span class="n">alt</span>
<span class="kn">from</span> <span class="n">altair</span> <span class="kn">import</span> <span class="n">Chart</span>

<span class="n">alt</span><span class="p">.</span><span class="n">data_transformers</span><span class="p">.</span><span class="nf">disable_max_rows</span><span class="p">()</span>
</code></pre></div></div> <h2 id="positional-encoding">Positional Encoding</h2> <p>Before the Transformer architecture came along, sequence models like RNNs and LSTMs processed data step by step, inherently taking element order into account. But their inefficiency (due to sequential computation and difficulty with parallelization) drove the search for alternatives.</p> <p>The Transformer eliminated these limitations by introducing a fully parallel approach. But that parallelism introduced a new problem: if all tokens are processed simultaneously, how can the model know their order?</p> <p>To address this, researchers proposed <strong>Positional Encoding</strong> — a mechanism that encodes positional information into each element.</p> <p>Positional Encoding adds special signals to the token embeddings that depend on their position in the sequence. This helps the model distinguish between, say, the word “cat” at position 1 and “cat” at position 5 — even if their semantic embeddings are identical. The encoding formula uses a mix of sine and cosine functions with different frequencies:</p> \[PE_{(pos,\, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right), \quad PE_{(pos,\, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\] <p>where \(pos\) is the position in the sequence, \(d_{\text{model}}\) is the embedding dimension, and \(i\) is the index of the vector component.</p> <p>The core idea is that these sinusoidal functions allow the model to pay attention to <strong>relative positions</strong>.</p> <h3 id="why-does-this-formula-encode-relative-positions">Why does this formula encode relative positions?</h3> <p>Imagine every position in the sequence as a point on a number line. If we generate sine and cosine signals for position \(pos\), then for position \(pos + k\) those signals can be expressed using combinations of the originals. For example, using the angle addition formula:</p> \[\sin(pos + k) = \sin(pos)\cos(k) + \cos(pos)\sin(k),\] <p>This means a shift by \(k\) positions becomes a weighted sum of the original sine and cosine values. That lets the model naturally pick up on things like “a word three positions away” being related to the current one — even if it’s never seen sequences that long during training.</p> <p>The logarithmic frequency decay in the denominator, \(10000^{2i/d_{\text{model}}}\), ensures that different components of the position vector focus on different scales. For small \(i\) (i.e., early dimensions of the vector), the denominator is large, so the sine and cosine functions grow slowly with \(pos\). These low-frequency oscillations help distinguish broad regions of the sequence — like the beginning (positions 1–100) from the middle (positions 101–200). For large \(i\), the denominator shrinks, so the functions grow faster and produce high-frequency oscillations that capture fine-grained position differences — like 101 vs. 102.</p> <p>Alternating between sine and cosine for even and odd indices ensures unique positional encodings. If we used only sine, some positions could accidentally overlap because of its periodic nature (e.g., \(\sin(pos)\) and \(\sin(pos + 2\pi)\)). Including cosine for neighboring vector components breaks that symmetry: the combination of \(\sin(f(pos))\) and \(\cos(f(pos))\) across various frequencies \(f\) guarantees that every \(pos\) has a unique vector. Since sine and cosine are nearly orthogonal (their dot product is close to zero), their signals don’t interfere with the word embeddings, letting the model process semantics and position independently.</p> <p>The sum \(\text{Embedding} + PE\) works because both word embeddings and positional encodings have the same dimensionality, \(d_{\text{model}}\). This addition requires no learnable parameters — the model receives a unified signal where the meaning of a word is adjusted based on its position. Gradients flow cleanly through this operation since the derivative of a sum is just the sum of derivatives. As a result, during training, the model naturally learns to refine both the semantic embeddings and the use of positional signals (via the attention mechanism), without signal conflict.</p> <p>Researchers explored other options too, like learnable positional embeddings. But the sinusoidal approach proved more effective at generalizing to sequences longer than those seen during training. So, Positional Encoding became a well-balanced solution — expressive, efficient, and free of extra learnable parameters — making it a perfect fit for the Transformer’s parallel architecture.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span>
                             <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h2 id="multiheadattention">MultiHeadAttention</h2> <p><strong>MultiHeadAttention</strong> is the core of the Transformer architecture. Attention in Transformers emerged as a response to the limitations of earlier attention mechanisms used in seq2seq models. Initially, self-attention allowed each element in a sequence to interact with others by computing weighted sums of their features. But there was a problem: a single-head attention mechanism could only focus on <strong>one type of dependency</strong> — like syntactic relationships or semantic similarity. For complex tasks like translation, we need to capture <strong>multiple types of interactions</strong> at once: subject-verb agreement, anaphora, contextual synonyms, and more.</p> <p><strong>The solution</strong>: instead of one attention mechanism, use several parallel “heads,” each learning to capture its own type of dependency. Formally, for input vectors (embeddings) \(X \in \mathbb{R}^{n \times d_{\text{model}}}\) — where \(n\) is the sequence length and \(d_{\text{model}}\) is the embedding size — each head \(h\) projects \(X\) into three separate spaces: queries (\(Q_h\)), keys (\(K_h\)), and values (\(V_h\)), using learned weight matrices:</p> \[Q_h = X W_h^Q, \quad K_h = X W_h^K, \quad V_h = X W_h^V,\] <p>where \(W_h^Q, W_h^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\), \(W_h^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\), and \(d_k\), \(d_v\) are the dimensionalities of the key/query and value subspaces, respectively. This triplet (\(Q, K, V\)) mirrors concepts from information retrieval:</p> <ul> <li> <strong>Queries</strong> (\(Q\)) — what we’re looking for,</li> <li> <strong>Keys</strong> (\(K\)) — where we’re looking,</li> <li> <strong>Values</strong> (\(V\)) — what we retrieve.</li> </ul> <p>If we only had \(Q\) and \(K\), the model could measure similarity, but not transform or reweight information based on context. The \(V\) matrix introduces that flexibility — it allows the model to adapt the retrieved features.</p> <p>For each head, we compute <strong>scaled dot-product attention</strong>:</p> \[\text{Attention}(Q_h, K_h, V_h) = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k}}\right) V_h.\] <p><strong>Why softmax?</strong> Softmax turns unbounded similarity scores (logits) into a probability distribution where the attention weights sum to 1. This keeps outputs within a stable range and ensures the model focuses on the most relevant tokens.</p> <p><strong>Why scale by \(\sqrt{d_k}\)?</strong> Without scaling, when \(d_k\) is large, the dot product \(Q_h K_h^T\) can have high variance. This leads to extremely sharp softmax distributions, causing gradients to vanish and slowing training. Scaling by \(\sqrt{d_k}\) keeps gradients in a healthy range.</p> <p><strong>Combining heads</strong>: the outputs of all heads are concatenated and projected back to \(d_{\text{model}}\):</p> \[\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W^O,\] <p>where \(W^O \in \mathbb{R}^{H d_v \times d_{\text{model}}}\) is a learned projection matrix. Typically, we set \(d_k = d_v = d_{\text{model}} / H\) to keep the total computation manageable. For example, with \(d_{\text{model}} = 512\) and \(H = 8\), we get \(d_k = d_v = 64\).</p> <p><strong>Why this choice of dimensions?</strong></p> <ul> <li>If \(d_k\) and \(d_v\) stayed constant as \(H\) increased, the computation cost would grow quadratically: \(O(H n^2 d_k)\).</li> <li>By reducing them to \(d_{\text{model}} / H\), we keep the overall complexity at \(O(n^2 d_{\text{model}})\) — the same as single-head attention.</li> <li>The output projection \(W^O\) restores the dimensionality to \(d_{\text{model}}\), keeping it compatible with the rest of the Transformer stack.</li> </ul> <p><strong>Why this structure works:</strong></p> <ol> <li> <strong>Subspace separation</strong>: Each head operates in its own \(d_k\)-dimensional subspace, letting the model learn <strong>independent types of interactions</strong>. One head might track noun-adjective agreement, another might attend to pronoun references. The projection matrices \(W_h^Q, W_h^K, W_h^V\) effectively decompose the original embeddings into interpretable components.</li> <li> <strong>Parallelism</strong>: Independent heads allow for efficient parallel computation on GPUs.</li> <li> <strong>Interpretability</strong>: After training, analyzing attention weights per head reveals the kinds of patterns each one has learned.</li> </ol> <p><strong>Example computation for a single head:</strong><br> Let \(X\) be an embedding matrix of shape \(n \times d_{\text{model}}\). For head \(h\):</p> <ul> <li>\(Q_h = X W_h^Q\) → shape \(n \times d_k\)</li> <li>\(K_h = X W_h^K\) → shape \(n \times d_k\)</li> <li>\(V_h = X W_h^V\) → shape \(n \times d_v\)</li> </ul> <p>Then the attention matrix \(A_h = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k}}\right)\) (shape \(n \times n\)) is multiplied by \(V_h\), giving an output of shape \(n \times d_v\). Concatenating the outputs from all heads gives a \(n \times (H d_v)\) matrix, which is projected back to \(n \times d_{\text{model}}\) via \(W^O\).</p> <p>Preserving the <strong>dimensionality</strong> is critical: MultiHeadAttention outputs have the same shape \(d_{\text{model}}\) as the input, allowing seamless integration with other Transformer components (like normalization and feed-forward layers) without extra transformation. This consistency also helps stabilize gradients in deep models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">d_model</span><span class="p">,</span> <span class="sh">"</span><span class="s">d_model must be divisible by num_heads</span><span class="sh">"</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Q: [batch_size, num_heads, seq_len, head_dim]
</span>        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_k</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_v</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="feedforward">FeedForward</h2> <p>We need the position-wise Feed Forward layer in the Transformer to provide <strong>non-linear feature transformation</strong> after the attention step. While MultiHeadAttention effectively captures global dependencies between tokens, that alone isn’t enough for complex tasks like translation — the model also needs to combine the extracted patterns and transform them into new semantic representations.</p> <p>Each token in the sequence is processed <strong>independently</strong> through two linear layers. The first layer expands the dimensionality from \(d_{\text{model}}\) (e.g., 512) to \(d_{\text{ff}}\) (typically 2048), followed by a ReLU activation function:</p> \[\text{hidden} = \text{ReLU}(x W_1 + b_1),\] <p>where \(W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}\).</p> <p>Expanding the dimensionality by a factor of 4 (\(d_{\text{ff}} = 4d_{\text{model}}\)) gives the model enough capacity to learn non-obvious combinations of features. The second linear layer projects the representation back down to the original dimensionality:</p> \[\text{output} = \text{hidden} W_2 + b_2,\] <p>where \(W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}\). Dropout (typically with a rate like 0.1) is applied between the layers for regularization.</p> <p><strong>Why this design?</strong></p> <ul> <li> <strong>Non-linearity</strong>: ReLU breaks linearity, enabling the model to approximate complex functions. Without it, the two linear layers would collapse into a single matrix multiplication.</li> <li> <strong>Expansion and compression</strong>: Increasing the dimensionality creates a kind of “bottleneck” that forces the model to filter out noise and extract more abstract features. This is similar to how an autoencoder works — but without information loss, since the output returns to the original size.</li> <li> <strong>Position-wise independence</strong>: Processing each token separately helps compensate for any local information that might be diluted by the global attention mechanism. For example, in the phrase “blue ball”, attention may link the adjective to the noun, but the FFN refines their joint representation into a vector encoding both color and shape.</li> </ul> <p>The input and output of the FeedForward layer both have the same dimensionality, \(d_{\text{model}}\), which allows for stacking multiple Encoder/Decoder blocks. Dropout and residual connections (implemented outside this layer) help stabilize training in deep networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="encoderlayer">EncoderLayer</h2> <p>Before the introduction of the <strong>EncoderLayer</strong> in the Transformer, researchers faced a dilemma: how to combine global contextual understanding with local feature transformation, while maintaining stable training in deep networks. Earlier approaches like RNNs suffered from vanishing gradients, and convolutional networks required many layers to capture long-range dependencies. Self-attention solved the problem of modeling global context — but on its own, it couldn’t provide deep, hierarchical feature transformation. This raised the question: how can we structure sequential transformations so the model first identifies relationships between tokens, then “rethinks” them, all while staying robust as the network grows deeper?</p> <p>The <strong>EncoderLayer</strong> was the answer — a module that combines two essential stages. First, the input embeddings \(x\), already enriched with positional information (via Positional Encoding), are passed through <strong>MultiHeadAttention</strong>. Here, each token “asks” the rest of the sequence:</p> \[\text{attn\_output} = \text{MultiHeadAttention}(x, x, x, mask),\] <p>where <code class="language-plaintext highlighter-rouge">mask</code> is used to ignore future tokens (in the decoder) or padding tokens. This lets the model, for instance, link the pronoun “he” to the correct noun, even if they’re separated by dozens of words. But attention is a <strong>linear</strong> operation in feature space. To introduce <strong>non-linearity and depth</strong>, we follow up with a <strong>Feed Forward Network (FFN)</strong> — two linear layers with an intermediate dimensionality expansion:</p> \[\text{ffn\_output} = \text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2.\] <p>The FFN acts like the model’s “thought process”: it transforms the global dependencies identified by attention into new semantic representations. For example, if attention links “apple” and “green,” the FFN can encode that as a combined vector representing both fruit and color.</p> <p>But simply chaining these operations wasn’t enough. Deep networks often “forgot” the original inputs — gradients vanished, and features got distorted. This is where <strong>residual connections</strong> and <strong>layer normalization</strong> came in. After each sub-step (attention or FFN), the layer adds the original input \(x\) to the output and applies normalization:</p> \[x = \text{LayerNorm}(x + \text{Dropout}(\text{sublayer}(x))).\] <p>Residuals act like bridges, allowing gradients and raw input information to flow freely through even dozens of layers. LayerNorm stabilizes activation distributions by computing the mean and variance across the \(d_{\text{model}}\) dimensions — preventing exploding or vanishing values.</p> <p><strong>Why this order?</strong> If the FFN came before attention, the ReLU non-linearity could “break” the positional information that’s critical for self-attention. And the use of <strong>post-layer normalization</strong> (after the residual connection) instead of pre-normalization (before the sub-step) wasn’t arbitrary: in the original Transformer, this design helped gradients flow through both the transformed path and the original input path, balancing parameter updates.</p> <p><strong>Example</strong>: An embedding for the word “bank” may, after attention, be linked to “river” (bank as a shore) or “money” (bank as a financial institution). The FFN then transforms these associations into a context-specific representation. Residuals and normalization ensure that the signal remains stable. As this process repeats across multiple EncoderLayers, the model refines the meaning iteratively — like rereading a sentence and noticing new details each time.</p> <p>Historically, the EncoderLayer became the blueprint for scalability. It could be stacked N times (e.g., 6 or 12 layers), enabling deep models without collapsing gradients. The combination of self-attention and FFN turned out to be so effective that even today’s large language models — like GPT-4 — retain this core structure, merely enhancing it with new mechanisms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Self attention
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="decoderlayer">DecoderLayer</h2> <p>If the EncoderLayer in a Transformer learns to <strong>understand</strong> the input text by compressing its context into dense vectors, then the <strong>DecoderLayer</strong> was designed to <strong>generate output</strong> — word by word — while taking into account both previous predictions and the encoder’s information. Early approaches like seq2seq with attention already connected encoder and decoder, but their recurrent nature limited parallelism and made it harder to model complex dependencies. In the Transformer, the decoder had to be <strong>autoregressive</strong>, yet still <strong>parallelizable</strong> — and this is where <strong>masked self-attention</strong> combined with <strong>cross-attention</strong> became crucial.</p> <p>A <strong>DecoderLayer</strong> starts with a partially generated output sequence (e.g., the translation generated up to the current word). To ensure that the model doesn’t “peek” at future tokens, it uses <strong>masked self-attention</strong>:</p> \[\text{attn\_output} = \text{MultiHeadAttention}(x, x, x, tgt\_mask),\] <p>where \(tgt\_mask\) is an upper-triangular matrix with \(-\infty\) in positions corresponding to future tokens. When passed through softmax, these become zeros — effectively blocking attention to future positions. For instance, while generating the third word, the mask hides all tokens beyond the third, forcing the model to rely only on previously generated context.</p> <p>But self-attention isn’t enough — the decoder also needs to <strong>relate the output to the input</strong>. This is where <strong>cross-attention</strong> comes in: queries (\(Q\)) come from the decoder, while keys (\(K\)) and values (\(V\)) are taken from the encoder output:</p> \[\text{cross\_attn\_output} = \text{MultiHeadAttention}(x, enc\_output, enc\_output, src\_mask).\] <p>Here, \(src\_mask\) hides padding tokens from the source sequence. This step acts like an “interrogation” of the encoder: the decoder asks which parts of the input are relevant at this step in the output. For example, when translating the word “apple,” the decoder can use cross-attention to link it to either “яблоко” or “компания,” depending on context.</p> <p>After cross-attention, as in the encoder, comes a <strong>Feed Forward Network</strong> to inject non-linearity:</p> \[\text{ffn\_output} = \text{FFN}(x).\] <p>Each sublayer is wrapped with <strong>residual connections</strong> and <strong>layer normalization</strong>:</p> \[x = \text{LayerNorm}(x + \text{Dropout}(\text{sublayer}(x))),\] <p>ensuring gradient stability even in very deep networks.</p> <p><strong>Why three stages?</strong></p> <ol> <li> <strong>Masked self-attention</strong> isolates the already-generated portion of the sequence, simulating RNN-like autoregression.</li> <li> <strong>Cross-attention</strong> synchronizes encoder and decoder, letting the decoder “look into” the input — akin to alignment in statistical machine translation.</li> <li> <strong>FFN</strong> transforms the combined context into a decision — a final refinement before predicting the next token.</li> </ol> <p><strong>Example</strong>: When translating “I hit the bank” into Russian:</p> <ol> <li>Masked self-attention links “I hit” to “the,” while blocking future tokens.</li> <li>Cross-attention identifies whether “bank” aligns to “берег” (if the context is a river) or “банк” (if financial).</li> <li>The FFN processes this and outputs either “по берегу” or “в банк,” preserving grammar and meaning.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="c1"># Self attention (маскированное)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Cross attention (с выходом энкодера)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="transformer">Transformer</h2> <p>Once all the components of the Transformer — encoder, decoder, attention mechanisms, and positional encodings — were developed, the final task was to <strong>assemble them into a complete model</strong> capable of learning from sequence pairs (e.g., source text and translation). Early approaches like Seq2Seq already used an encoder-decoder separation, but their recurrent nature limited both parallelism and depth. The Transformer architecture, as implemented in code, emerged as a balance between expressiveness and computational efficiency.</p> <p><strong>Model assembly</strong> starts with turning tokens into vectors. The embeddings (<code class="language-plaintext highlighter-rouge">encoder_embedding</code> and <code class="language-plaintext highlighter-rouge">decoder_embedding</code>) map words into a \(d_{\text{model}}\)-dimensional space, while <code class="language-plaintext highlighter-rouge">positional_encoding</code> injects positional information:</p> <p>\(X_{\text{enc}} = \text{Embedding}(src) + \text{PositionalEncoding}(src),\)<br> \(X_{\text{dec}} = \text{Embedding}(tgt) + \text{PositionalEncoding}(tgt).\)</p> <p>Without positional encoding, the model wouldn’t be able to distinguish between permutations of words, since self-attention alone is order-invariant.</p> <p>Next, the encoder and decoder are built as <strong>stacks of layers</strong> (<code class="language-plaintext highlighter-rouge">num_layers</code>). Each layer in the encoder (<code class="language-plaintext highlighter-rouge">EncoderLayer</code>) progressively refines the input representations: self-attention extracts global dependencies, the FFN adds non-linearity, and residual connections with layer normalization ensure stability. Similarly, the decoder (<code class="language-plaintext highlighter-rouge">DecoderLayer</code>) applies masked self-attention, cross-attention to the encoder output, and the FFN — in that order. Repeating these layers allows the model to iteratively refine its understanding, as if “rereading” the data at different levels of abstraction.</p> <p>The <strong>final layer</strong> (<code class="language-plaintext highlighter-rouge">fc_out</code>) projects from \(d_{\text{model}}\) to the size of the target vocabulary. This projection interprets decoder vectors as logits — scores for each token in the vocabulary:</p> \[\text{output} = W_{\text{out}} \cdot \text{dec\_output} + b_{\text{out}}.\] <p>A softmax (not explicitly shown in code but implied in the loss function) converts these logits into a probability distribution, from which the next word is sampled or selected.</p> <p><strong>Why this particular structure?</strong></p> <ul> <li> <strong>Depth (<code class="language-plaintext highlighter-rouge">num_layers</code>)</strong>: Each layer captures different facets of the data. Early encoder layers may pick up syntax, while later ones capture semantics. In the decoder, lower layers focus on alignment with the encoder, while upper ones refine output grammar and fluency.</li> <li> <strong>Separate embeddings</strong>: Using different embedding matrices for source and target languages allows the model to work effectively in multilingual settings.</li> <li> <strong>Dimensional consistency</strong>: All components maintain the same dimensionality \(d_{\text{model}}\), which simplifies training — gradients flow freely through residual paths, and parameters update coherently.</li> <li> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">src_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">encoder_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">src_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">decoder_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">tgt_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="testing">Testing</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_transformer</span><span class="p">():</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">src_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Generate synthetic data
</span>    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    
    <span class="c1"># Generate masks (example)
</span>    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># No masking
</span>    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Causal mask
</span>
    <span class="c1"># Initialize the model
</span>    <span class="n">transformer</span> <span class="o">=</span> <span class="nc">Transformer</span><span class="p">(</span>
        <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">src_vocab_size</span><span class="p">,</span>
        <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">tgt_vocab_size</span><span class="p">,</span>
        <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span>
    <span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">1. Positional Encoding Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Before PE: mean=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x_pe</span> <span class="o">=</span> <span class="nf">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">After PE: mean=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PE Shape: </span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should be [1, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s">])</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">2. Multi-Head Attention Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">mha</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">k</span> <span class="o">=</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="nf">mha</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Attention output shape: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Encoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">encoder_layer</span> <span class="o">=</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">enc_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">enc_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder output shape: </span><span class="si">{</span><span class="n">enc_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">enc_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Data changed: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span><span class="si">}</span><span class="s"> (should be False)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">4. Decoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">decoder_layer</span> <span class="o">=</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder output shape: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">dec_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Output norm: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="nf">norm</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">5. Full Transformer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input data:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">src: </span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tgt: </span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nf">transformer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Output shape check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Expected shape: (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">tgt_vocab_size</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Actual shape:   </span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Gradient check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">dummy_loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="n">dummy_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">has_gradients</span> <span class="o">=</span> <span class="nf">any</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Gradients computed: </span><span class="si">{</span><span class="n">has_gradients</span><span class="si">}</span><span class="s"> (should be True)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">6. Model Parameters Check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">encoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">decoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Test completed!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/tokenization/">Understanding Byte-Pair Encoding Algorithm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/creative-ai/">Can AI Achieve True Creativity?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/lora-algorithm-for-llms/">Algebraic Foundations of Low-Rank Adaptation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llms-for-those-who-missed-out/">LLMs for Those Who Missed Out</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/python-code/">How to Write Good Python Code</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eva Koroleva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 24, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>