<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PR5ZCQWED0"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PR5ZCQWED0");</script> <script src="/assets/js/h2-cards.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Byte-Pair Encoding Algorithm | Eva Koroleva </title> <meta name="author" content="Eva Koroleva"> <meta name="description" content="Implement one of the most popular tokenization algorithms and learn how to use ready-made solutions."> <meta name="keywords" content="machine-learning, deep-learning, python, neural-networks, cognitive-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xmarva.github.io/blog/2025/tokenization/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eva</span> Koroleva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Byte-Pair Encoding Algorithm</h1> <p class="post-meta"> Created in April 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/tokenization"> <i class="fa-solid fa-hashtag fa-sm"></i> tokenization,</a>   <a href="/blog/tag/bpe"> <i class="fa-solid fa-hashtag fa-sm"></i> bpe,</a>   <a href="/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> algorithms,</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="tokenization">Tokenization</h2> <p><a href="https://www.kaggle.com/code/qmarva/1-bpe-tokenization-algorithm-eng?scriptVersionId=231677033" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=plastic&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"></a> <a href="https://colab.research.google.com/drive/1lmfuMdC8v-lXL_MuyC0uBewdLLCTQzCO?usp=sharing" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/Colab-F9AB00?style=plastic&amp;logo=google-colab&amp;logoColor=white" alt="Colab"></a></p> <p>Tokenization is a fundamental stage in natural language processing, the task of which is to split text into meaningful units (tokens).</p> <p>These units can be words, parts of words, or even characters. Historically, simple methods were used: splitting by spaces, regular expressions for extracting words and punctuation, manual rules for handling abbreviations. However, such approaches scaled poorly for languages with agglutinative morphology (e.g., Russian or Finnish) and complex word combinations.</p> <p>Traditional tokenization methods like space splitting or manual rules often prove ineffective in real-world scenarios: they struggle with typos, rare words, and multilingual texts. For example, words like “gooood” or mixed languages in a single sentence can break a classical tokenizer.</p> <p>In modern NLP, subword tokenization algorithms like <a href="https://arxiv.org/pdf/1508.07909" rel="external nofollow noopener" target="_blank">BPE (Byte Pair Encoding)</a> dominate, balancing the semantic integrity of tokens with efficient vocabulary usage. In this notebook, we will examine the BPE algorithm in detail and learn to work with tokenizers from the Hugging Face library.</p> <p>First, we will import all libraries and functions needed for this notebook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>
</code></pre></div></div> <hr> <h2 id="loading-data">Loading Data</h2> <p>For demonstration, we will load the parallel English-Russian <a href="https://arxiv.org/abs/1812.10464" rel="external nofollow noopener" target="_blank">Tatoeba</a> corpus from Artetxe et al. (2019) via the <a href="http://huggingface.co/docs/datasets/loading" rel="external nofollow noopener" target="_blank">Hugging Face Datasets</a> library.</p> <p><a href="https://tatoeba.org/en/sentences/index" rel="external nofollow noopener" target="_blank">Tatoeba</a> is a free collection of translated example sentences for language learners, available in over 400 languages. Its name comes from the Japanese phrase «tatoeba» (例えば), meaning “for example.” It is written and maintained by a community of volunteers through open collaboration. Individual contributors are known as Tatoebans.</p> <p>We will use only the English and Russian subsets. All examples in this dataset are short everyday phrases: “Let’s try something.” → “Давайте что-нибудь попробуем!”.</p> <p>This format is convenient for training transformers, which work with sequences of limited length. In this notebook, we will not delve into transformer architecture but focus on text data preprocessing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_translation_dataset</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading Tatoeba en-ru...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Helsinki-NLP/tatoeba</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="sh">"</span><span class="s">en</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="sh">"</span><span class="s">ru</span><span class="sh">"</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error while loading dataset: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Dataset structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Data sample:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EN: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">RU: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
    
    <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <hr> <h2 id="data-analysis">Data Analysis</h2> <p>Let’s take a quick look at the dataset to understand what we’re dealing with. We won’t dive deep into data analysis methods but will examine basic statistics.</p> <p>The <code class="language-plaintext highlighter-rouge">analyze_dataset</code> function shows that the average length of English sentences is 7.2 words, Russian — 6.2. The maximum lengths (30 and 28 words) indicate the presence of outliers that may require truncation.</p> <p>The histograms show right-skewed distributions: most sentences are shorter than 15 words. These observations influence model hyperparameter choices, e.g., <code class="language-plaintext highlighter-rouge">max_length=64</code> provides padding headroom even if actual sequences are shorter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>

    <span class="n">en_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">ru_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Analysis based on first </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s"> samples:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">English sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">English Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Russian Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="simple-tokenizer">Simple Tokenizer</h2> <p>Now we will write a <code class="language-plaintext highlighter-rouge">BaseTokenizer</code> class for text preprocessing, building a token vocabulary, and collecting token frequency statistics. This class will serve as the foundation for more complex tokenizers and provide a common structure for processing text data.</p> <p>We declare the class using the <a href="https://docs.python.org/3/library/dataclasses.html" rel="external nofollow noopener" target="_blank">@dataclass</a> decorator to auto-generate the constructor. Parameters we need: <code class="language-plaintext highlighter-rouge">language</code> (text language), <code class="language-plaintext highlighter-rouge">vocab_size</code> (max vocabulary size), <code class="language-plaintext highlighter-rouge">min_freq</code> (minimum frequency for including a token in the vocabulary), and <code class="language-plaintext highlighter-rouge">special_tokens</code> (list of special tokens).</p> <p>If <code class="language-plaintext highlighter-rouge">special_tokens</code> are not specified, default values are used: <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called immediately after object initialization. Here, we initialize the <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">id2token</code> dictionaries that map tokens to their numeric IDs. Special tokens must be added to the vocabulary first. For example, <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> gets ID 0, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> — 1, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">preprocess_text</code> method is used to preprocess text. We will convert text to lowercase and split it into tokens using a regular expression.</p> <p>The pattern <code class="language-plaintext highlighter-rouge">r"\w+[\w']*|['’][a-z]+|[^\w\s]"</code> captures:</p> <ul> <li>Words with apostrophes (e.g., <code class="language-plaintext highlighter-rouge">don't</code> → <code class="language-plaintext highlighter-rouge">["don't"]</code>).</li> <li>Contractions starting with an apostrophe (e.g., <code class="language-plaintext highlighter-rouge">'s</code> → <code class="language-plaintext highlighter-rouge">["'s"]</code>).</li> <li>Individual punctuation marks (e.g., <code class="language-plaintext highlighter-rouge">"!"</code> → <code class="language-plaintext highlighter-rouge">["!"]</code>).</li> </ul> <p>Note that the regex may not cover all edge cases (e.g., emojis or compound symbols), requiring modification for specific tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">get_stats</code> method collects token frequency statistics. For each text in the <code class="language-plaintext highlighter-rouge">examples</code> list, the <code class="language-plaintext highlighter-rouge">preprocess_text</code> function is called, then the <code class="language-plaintext highlighter-rouge">Counter</code> is updated.</p> <p>For example, the text <code class="language-plaintext highlighter-rouge">"Hello, world!"</code> returns a counter with keys <code class="language-plaintext highlighter-rouge">["hello", ",", "world", "!"]</code> and their frequencies. This method is used during tokenizer training to select tokens for the vocabulary based on <code class="language-plaintext highlighter-rouge">min_freq</code> and <code class="language-plaintext highlighter-rouge">vocab_size</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <p>Below, we consolidate all code into a single class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
</code></pre></div></div> <p>In reality, this basic approach has many drawbacks. For example, converting text to lowercase may lose case information. Additionally, this tokenization ignores word morphology, leading to issues with rare words or homonyms.</p> <p>Let’s write an <code class="language-plaintext highlighter-rouge">analyze_token_statistics</code> function to count unique tokens and their frequencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_stats</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Token statistics for </span><span class="si">{</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total unique tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Top 10 most frequent tokens:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">en_tokenizer</span><span class="p">)</span>
<span class="n">ru_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ru_tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>The difference in token counts for English (1337) and Russian (2065) stems from language features: Russian has richer morphology (endings, prefixes) and more word forms. The dominance of punctuation (. and , in the top) suggests the need for their pre-filtering or separate handling.</p> <p>Interestingly, the <code class="language-plaintext highlighter-rouge">"</code> token appears more frequently in English (146 times) — likely due to translation specifics in Tatoeba.</p> <p>Critically, this approach does not split words into subword units, leaving rare words intact and inflating vocabulary size. For comparison, we will explore the BPE tokenizer in subsequent experiments.</p> <hr> <h2 id="bpe-tokenization-algorithm">BPE Tokenization Algorithm</h2> <p>Now let’s examine how the <strong>BPE (Byte Pair Encoding)</strong> tokenizer works. The core idea is to iteratively merge the most frequent character or token pairs, gradually forming a subword vocabulary. This efficiently handles rare and complex words by splitting them into known components.</p> <h3 id="bpetokenizer-class">BPETokenizer Class</h3> <p>First, declare the class with the <code class="language-plaintext highlighter-rouge">@dataclass</code> decorator. Since it inherits from <code class="language-plaintext highlighter-rouge">BaseTokenizer</code>, it already includes parameters <code class="language-plaintext highlighter-rouge">language</code>, <code class="language-plaintext highlighter-rouge">vocab_size</code>, <code class="language-plaintext highlighter-rouge">min_freq</code>, and <code class="language-plaintext highlighter-rouge">special_tokens</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
</code></pre></div></div> <h3 id="initialization">Initialization</h3> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called after object creation. Here we:</p> <ol> <li>Call the parent <code class="language-plaintext highlighter-rouge">__post_init__</code> to initialize base structures like <code class="language-plaintext highlighter-rouge">token2id</code>.</li> <li>Add a <code class="language-plaintext highlighter-rouge">merges</code> dictionary to store character pairs and their merged versions (e.g., <code class="language-plaintext highlighter-rouge">('h', 'e')</code> → <code class="language-plaintext highlighter-rouge">'he'</code>).</li> <li>Initialize <code class="language-plaintext highlighter-rouge">vocab</code> with special tokens.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>
</code></pre></div></div> <h3 id="generating-character-pairs">Generating Character Pairs</h3> <p>The <code class="language-plaintext highlighter-rouge">get_pairs</code> method splits a word into consecutive character pairs. For example, the word <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code> returns pairs <code class="language-plaintext highlighter-rouge">[('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')]</code>. These pairs are analyzed during training to find the most frequent combinations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div> <hr> <h2 id="training-the-tokenizer">Training the Tokenizer</h2> <p>The <code class="language-plaintext highlighter-rouge">train</code> method is the core of BPE. It has several stages:</p> <p><strong>Collect Initial Statistics:</strong></p> <ul> <li>Split each token into characters and count character sequence frequencies. For example, token <code class="language-plaintext highlighter-rouge">"hello"</code> becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>, and its frequency increments the counter for <code class="language-plaintext highlighter-rouge">'h e l l o'</code>.</li> <li>Collect all unique characters from the text.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Adding Characters to Vocabulary:</strong></p> <ul> <li>Each unique character (e.g., <code class="language-plaintext highlighter-rouge">'h'</code>, <code class="language-plaintext highlighter-rouge">'e'</code>) is added to <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">vocab</code> if not already present. This ensures even individual characters have IDs.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Main Merge Loop:</strong></p> <ul> <li>Each iteration counts the frequency of all possible character pairs in the current word representations. For example, the word <code class="language-plaintext highlighter-rouge">'h e l l o'</code> has pairs <code class="language-plaintext highlighter-rouge">('h', 'e')</code>, <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, etc.</li> <li>Select the most frequent pair (e.g., <code class="language-plaintext highlighter-rouge">('l', 'l')</code> for <code class="language-plaintext highlighter-rouge">hello</code>) and create a new token <code class="language-plaintext highlighter-rouge">'ll'</code>.</li> <li>Update <code class="language-plaintext highlighter-rouge">merges</code>, <code class="language-plaintext highlighter-rouge">vocab</code>, <code class="language-plaintext highlighter-rouge">token2id</code>, and <code class="language-plaintext highlighter-rouge">id2token</code>.</li> <li>Recalculate word frequencies by replacing the selected pair with the new token. For example, <code class="language-plaintext highlighter-rouge">'h e l l o'</code> becomes <code class="language-plaintext highlighter-rouge">'h e ll o'</code> after merging <code class="language-plaintext highlighter-rouge">('l', 'l')</code>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
    <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="c1"># Count pair frequencies
</span>    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
        <span class="k">break</span>  <span class="c1"># No pairs left → stop
</span>
    <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

    <span class="c1"># Update vocabulary
</span>    <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

    <span class="c1"># Recalculate frequencies with new token
</span>    <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
        <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>
</code></pre></div></div> <p>If <code class="language-plaintext highlighter-rouge">num_merges</code> is too high and pairs are exhausted early, training stops. Progress is printed every 1000 iterations to track vocabulary growth.</p> <hr> <h3 id="text-tokenization">Text Tokenization</h3> <p>The <code class="language-plaintext highlighter-rouge">tokenize</code> method converts text to token IDs:</p> <ol> <li>Text is split into tokens via <code class="language-plaintext highlighter-rouge">preprocess_text</code>.</li> <li>The <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> special token is prepended.</li> <li>For each token (e.g., <code class="language-plaintext highlighter-rouge">"hello"</code>): <ul> <li>Characters are split into <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>.</li> <li>Merges from <code class="language-plaintext highlighter-rouge">merges</code> are applied iteratively. For example, if <code class="language-plaintext highlighter-rouge">('l', 'l')</code> is in <code class="language-plaintext highlighter-rouge">merges</code>, the character list becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'll', 'o']</code>, then remaining pairs are checked.</li> <li>Unknown characters (e.g., <code class="language-plaintext highlighter-rouge">'#'</code>) are replaced with <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</li> </ul> </li> <li>The <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is appended.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
            <span class="c1"># Find first available merge pair
</span>            <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                    <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Replace pair with new token
</span>            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

        <span class="c1"># Add final symbols to result
</span>        <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>During tokenization, merges are applied left-to-right, and the <strong>first</strong> available pair from <code class="language-plaintext highlighter-rouge">merges</code> is chosen. This can yield different results depending on the merge order. For example, if <code class="language-plaintext highlighter-rouge">merges</code> contains <code class="language-plaintext highlighter-rouge">('h', 'e')</code> and <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, the first encountered pair is merged.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
                <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>

        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training BPE tokenizer for </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
            <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

            <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
                <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>

            <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Merges completed: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_merges</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

            <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                        <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

            <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>When applying the tokenizer to convert text to tokens, the algorithm first splits text into base characters, then iteratively merges character pairs using the built merge dictionary. Each word in the text is represented as a sequence of subwords (or tokens) created during training.</p> <p>The number of merges (<code class="language-plaintext highlighter-rouge">num_merges</code> parameter) determines how many times the algorithm will merge characters into new tokens. More merges create larger, more informative tokens. However, excessive merges can lead to loss of fine-grained details.</p> <p>This algorithm performs well with large text corpora and helps models handle rare or unseen words by replacing them with subwords from more frequent character combinations. Additionally, BPE works with any language, even those with unusual or complex alphabets, as it starts from base characters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">80000</span>
<span class="n">train_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
<span class="n">en_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>
<span class="n">ru_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>

<span class="n">en_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">en_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">ru_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">ru_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">English vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Russian vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ru_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span>
<span class="n">ru_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">en_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">en_sample</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ru_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">ru_sample</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">)</span>
</code></pre></div></div> <p>Overall, BPE effectively addresses rare and complex words, improving tokenization quality and NLP model performance.</p> <p>However, even after training, artifacts remain. For example, “useless” splits into [“us”, “el”, “ess”], and “бесполезно” into [“бес”, “пол”, “ез”, “но”]. This stems from the limited number of merges and the lack of explicit morpheme boundary consideration in our educational implementation.</p> <p>In production tokenizers (e.g., Hugging Face’s), such issues are mitigated by pretraining on massive corpora and tens of thousands of merges.</p> <hr> <h2 id="batch-preparation">Batch Preparation</h2> <p>The <code class="language-plaintext highlighter-rouge">prepare_batch</code> function converts tokenized sequences into tensors suitable for training. Each sentence is padded to a fixed length (<code class="language-plaintext highlighter-rouge">max_length=64</code>) with the <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token, and attention masks tell the model to ignore these “empty” positions.</p> <p>For example, a sentence with 24 tokens becomes a vector of length 64, where the last 40 elements are zeros (ID <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>). Masking is critical for transformers, as the attention mechanism would otherwise account for meaningless padding tokens, distorting weights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
                 <span class="n">src_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">tgt_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="n">src_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">src_texts</span><span class="p">]</span>
    <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">tgt_texts</span><span class="p">]</span>

    <span class="n">src_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">src_masks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_masks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>

        <span class="n">src_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_pad</span><span class="p">)</span>
        <span class="n">tgt_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_pad</span><span class="p">)</span>
        <span class="n">src_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_mask</span><span class="p">)</span>
        <span class="n">tgt_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_masks</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_masks</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre></div></div> <hr> <h2 id="tokenizer-verification">Tokenizer verification</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Prepared batch shapes:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Example source tokens:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Corresponding mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">base_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Base tokenization: </span><span class="si">{</span><span class="n">base_tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Number of merges learned: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample merges (first 5):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">merged</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">merged</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample vocabulary items (first 10):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final tokenization:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoded tokens: </span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing English tokenizer:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr> <h2 id="hugging-face-tokenizers">Hugging Face Tokenizers</h2> <p>All this seems quite complex. Our current tokenizer works imperfectly and is slow. Fortunately, programmers avoid reinventing the wheel. In practice, it’s much easier to use a ready-made tokenizer via <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> from the <code class="language-plaintext highlighter-rouge">transformers</code> library.</p> <p>The <code class="language-plaintext highlighter-rouge">opus-mt-en-ru</code> model already has a pretrained BPE vocabulary optimized for the language pair. The tokenizer automatically adds special tokens, handles case, and rare symbols. When processing the dataset, the <code class="language-plaintext highlighter-rouge">map</code> function applies tokenization in parallel to all examples, speeding up work via batching.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Helsinki-NLP/opus-mt-en-ru</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
<span class="p">):</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="n">source_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>
        <span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>

        <span class="n">source_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">source_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="n">target_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">target_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">decoder_attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span>
        <span class="n">preprocess_function</span><span class="p">,</span>
        <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">column_names</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="comparing-tokenizers">Comparing Tokenizers</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Custom BPE Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of prepared batches:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">array</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (dtype: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sample data from first batch:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Source tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Target tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Source mask (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hugging Face Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset features: </span><span class="si">{</span><span class="n">processed_dataset</span><span class="p">.</span><span class="n">features</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of examples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">first_example</span> <span class="o">=</span> <span class="n">processed_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">First example details:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input IDs shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded input:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Labels shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded labels:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention mask sample:</span><span class="sh">"</span><span class="p">,</span> <span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>


<span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/antonio-damasio/">Somatic Marker Hypothesis by Antonio Damasio</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/inference-optimization/">LLM Inference Optimization</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/adapters/">PEFT Method Overview [implementing Adapters in PyTorch]</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/minds-as-computers/">Physical Symbol Systems and the Language of Thought</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/building-a-transformer/">Building a Transformer (Cross-Attention and MHA Explained)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eva Koroleva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 26, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>