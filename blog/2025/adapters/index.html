<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PR5ZCQWED0"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PR5ZCQWED0");</script> <script src="/assets/js/h2-cards.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PEFT Method Overview [implementing Adapters in PyTorch] | Eva Koroleva </title> <meta name="author" content="Eva Koroleva"> <meta name="description" content="Exploration of modern parameter-efficient fine-tuning techniques with PyTorch implementations and practical insights."> <meta name="keywords" content="machine-learning, deep-learning, python, neural-networks, cognitive-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xmarva.github.io/blog/2025/adapters/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eva</span> Koroleva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PEFT Method Overview [implementing Adapters in PyTorch]</h1> <p class="post-meta"> Created in May 11, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning,</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers,</a>   <a href="/blog/tag/peft"> <i class="fa-solid fa-hashtag fa-sm"></i> peft,</a>   <a href="/blog/tag/lora"> <i class="fa-solid fa-hashtag fa-sm"></i> lora,</a>   <a href="/blog/tag/adapters"> <i class="fa-solid fa-hashtag fa-sm"></i> adapters,</a>   <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a>   ·   <a href="/blog/category/featured-posts"> <i class="fa-solid fa-tag fa-sm"></i> featured-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In the rapidly evolving landscape <a href="https://xmarva.github.io/blog/2025/building-a-transformer/">transformer-based architectures</a>, a significant challenge has emerged: how do we customize these increasingly massive models for specific tasks without breaking the bank on computational resources?</p> <p>Enter Parameter-Efficient Fine-Tuning (PEFT), a family of techniques that has revolutionized how we adapt pre-trained models to downstream tasks.</p> <h2 id="the-fine-tuning-dilemma">The Fine-Tuning Dilemma</h2> <p>So, you’ve got access to a SoTA LM with billions of parameters.</p> <p>Perhaps it’s GPT-4, LLaMA 3, Mistral or Qwen. You want to adapt this model to a specialized domain like medical text analysis or legal document processing.</p> <p>The traditional approach would involve fine-tuning the entire model on your domain-specific data.</p> <p>Full fine-tuning comes with substantial costs:</p> <ol> <li> <strong>Computational Expense</strong>: Training billions of parameters requires significant GPU resources.</li> <li> <strong>Storage Overhead</strong>: Each fine-tuned version requires storing a complete copy of the model</li> <li> <strong>Catastrophic Forgetting</strong>: Aggressive fine-tuning might cause the model to lose its general capabilities</li> <li> <strong>Limited Scalability</strong>: Maintaining multiple specialized versions becomes unmanageable</li> </ol> <p>This is where PEFT techniques come to the rescue. Rather than updating all parameters, PEFT methods focus on adding and training a small number of parameters while keeping most of the pre-trained model frozen. This approach typically requires updating less than 1% of the parameters compared to full fine-tuning, while achieving comparable performance.</p> <p>Let’s understand most significant PEFT methods, their core principles, and implement them using PyTorch for better understanding. Then we’ll explore how to use these techniques with the Hugging Face <code class="language-plaintext highlighter-rouge">peft</code> library for practical applications.</p> <h2 id="general-overview-of-peft-methods">General Overview of PEFT methods</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/method-overview-480.webp 480w,/assets/img/posts/2025-05-11-adapters/method-overview-800.webp 800w,/assets/img/posts/2025-05-11-adapters/method-overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-05-11-adapters/method-overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Parameter-efficient fine-tuning methods taxonomy." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Parameter-efficient fine-tuning methods taxonomy. Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning.s </div> <hr> <h3 id="1-addition-based-methods"><strong>1. Addition-Based Methods</strong></h3> <p>These approaches add new, lightweight modules to a pre-trained model while keeping the original weights frozen. This is the most widely explored category and includes two main subtypes: <strong>adapters</strong> and <strong>soft prompts</strong>.</p> <ul> <li> <strong>Adapters</strong> (e.g., Bottleneck Adapters, Parallel Adapters): Small neural layers inserted within Transformer blocks that are trained while the rest of the model remains unchanged. Variants differ in placement, structure, and compression strategies.</li> <li> <strong>LoRA (Low-Rank Adaptation)</strong>: Instead of fine-tuning full weight matrices, LoRA introduces low-rank decompositions for weight updates (e.g., replacing a full-rank update with <code class="language-plaintext highlighter-rouge">W_down * W_up</code>).</li> <li> <strong>Prefix Tuning / Prompt Tuning</strong>: Add trainable vectors (prefixes or prompts) to the model’s input or internal layers. These methods steer model behavior without changing its core parameters.</li> <li> <strong>Soft Prompts</strong>: Instead of using discrete tokens, these train continuous embeddings that are prepended to the input. Can be applied to input embeddings or even across all Transformer layers.</li> </ul> <p>Despite adding new parameters, these methods often use significantly less memory and are more computationally efficient due to fewer gradients and optimizer states being updated.</p> <hr> <h3 id="2-selection-based-methods"><strong>2. Selection-Based Methods</strong></h3> <p>Selective approaches involve fine-tuning only a specific subset of the model’s original parameters, chosen either manually or via structural criteria.</p> <ul> <li> <strong>BitFit</strong>: Fine-tunes only the bias terms of the model, drastically reducing the number of parameters involved.</li> <li> <strong>IA³ (Infused Adapter by Inhibiting and Amplifying Activations)</strong>: Adds scalar gating parameters to control the flow of information through the attention and feed-forward layers.</li> <li> <strong>Layer-wise Selection</strong>: Fine-tunes only the top or bottom layers of the model, or focuses on specific components (e.g., attention vs. FFN).</li> <li> <strong>Sparse Fine-Tuning</strong>: Selects parameters to update based on certain criteria (e.g., magnitude or gradients), ignoring model structure. However, this poses practical challenges for current hardware.</li> </ul> <p>These methods are particularly useful when model updates must be extremely lightweight or constrained due to storage, bandwidth, or privacy concerns.</p> <hr> <h3 id="3-reparameterization-based-methods"><strong>3. Reparameterization-Based Methods</strong></h3> <p>These techniques re-structure the parameter space to enable efficient updates with fewer trainable weights.</p> <ul> <li> <strong>LoRA</strong> (also fits here): Uses low-rank matrices to model weight updates, greatly reducing parameter count.</li> <li> <strong>Compacter</strong>: Builds on adapters but compresses them using low-rank decomposition and parameter sharing.</li> <li> <strong>(IA)³</strong>: Combines gating and reparameterization ideas to modulate specific subcomponents of the model.</li> <li> <strong>KronA / Kron Adapter</strong>: Uses Kronecker product decomposition to represent weight updates with a favorable trade-off between expressiveness and size.</li> <li> <strong>Intrinsic SAID</strong>: Employs the Fastfood transform to apply updates within a low-rank subspace, based on the theory that fine-tuning operates within a lower-dimensional manifold.</li> </ul> <p>These methods often target attention-related weights like <code class="language-plaintext highlighter-rouge">W_Q</code>, <code class="language-plaintext highlighter-rouge">W_K</code>, <code class="language-plaintext highlighter-rouge">W_V</code>, where much of the model’s representational power lies.</p> <hr> <h3 id="4-hybrid-methods"><strong>4. Hybrid Methods</strong></h3> <p>Hybrid approaches combine strategies from multiple categories to balance trade-offs in memory, compute, and performance.</p> <ul> <li> <strong>MAM Adapter</strong>: Combines Adapters with Prompt Tuning for better modularity.</li> <li> <strong>UniPELT</strong>: Merges LoRA with Adapters and Prompts into a unified framework.</li> <li> <strong>Compacter++ / Kron Adapter</strong>: Combines adapter-based methods with Kronecker reparameterization to reduce the number of trainable parameters further.</li> </ul> <p>These methods allow researchers to adapt fine-tuning strategies to specific deployment constraints, whether that be edge devices, multi-task learning, or multilingual models.</p> <h2 id="bottleneck-adapters">Bottleneck Adapters</h2> <p>Adapters were among the first successful PEFT approaches, introduced in <a href="https://arxiv.org/abs/1902.00751" rel="external nofollow noopener" target="_blank">Parameter-Efficient Transfer Learning for NLP</a> by Houlsby et al. in 2019.</p> <p>The core idea is elegantly simple: insert small trainable modules into each layer of a pre-trained network while keeping the original parameters frozen.</p> <p><strong>Bottleneck adapters</strong> add lightweight feed-forward layers into each Transformer block. These adapter layers typically include:</p> <ul> <li>a <strong>down-projection matrix</strong> that reduces the hidden state dimension from \(d\) to a smaller dimension \(b\),</li> <li>a <strong>non-linear activation</strong> \(\sigma\),</li> <li>an <strong>up-projection matrix</strong> that expands the representation back to the original size \(d\), and</li> <li>a <strong>residual connection</strong>, so the original input is added back after transformation:</li> </ul> \[\text{Adapter}(x) = x + W_{\text{up}} \, \sigma(W_{\text{down}} x)\] <p>Depending on the specific configuration, these adapter layers can be placed at various points inside the Transformer block. Other components like residual connections, layer normalizations, activation functions, and the size of the bottleneck layer can also be customized.</p> <p>The <strong>most important hyperparameter</strong> in this setup is the <strong>bottleneck dimension</strong> \(b\). Rather than setting \(b\) directly, it’s usually defined through a parameter called <code class="language-plaintext highlighter-rouge">reduction_factor</code>. This factor represents the ratio between the hidden layer size \(d\) and the bottleneck size \(b\), given by:</p> \[b = \frac{d}{\text{reduction\_factor}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        A bottleneck adapter module that can be inserted into a transformer.
        
        It projects hidden states down to a lower-dimensional space and then 
        back up again, with non-linearity and dropout in between. This helps 
        the model adapt to new tasks without updating the original transformer.
        
        Args:
            hidden_size: The dimension of the model</span><span class="sh">'</span><span class="s">s hidden states (e.g., 768 for BERT-base)
            adapter_size: The smaller bottleneck dimension (e.g., 64)
            dropout_rate: Regularization to improve generalization
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">down_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>  <span class="c1"># d -&gt; b
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>  <span class="c1"># non-linearity
</span>        <span class="n">self</span><span class="p">.</span><span class="n">up_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>    <span class="c1"># b -&gt; d
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Initialize adapter weights — not learned from pretraining, so good init is important!
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="c1"># Store original input for residual connection
</span>        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># Apply adapter: down-project -&gt; non-linear -&gt; up-project -&gt; dropout
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_project</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_project</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add residual and normalize
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>But how do we integrate adapters into a pre-trained model?</p> <p>Let’s see how to modify a standard transformer layer to include our bottleneck adapter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdapterTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        A wrapper around an existing transformer layer that adds adapters after
        attention and after the feed-forward sublayers.

        Args:
            transformer_layer: One layer from a pre-trained transformer (e.g., BERTLayer)
            adapter_size: Bottleneck size for the adapters
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">transformer_layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span><span class="p">.</span><span class="n">all_head_size</span>  <span class="c1"># Model-specific
</span>
        <span class="c1"># Freeze all transformer weights (we don’t train them)
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="c1"># Add bottleneck adapters at two key places:
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_adapter</span> <span class="o">=</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn_adapter</span> <span class="o">=</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Standard attention (output of frozen pre-trained layer)
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Inject adapter after attention
</span>        <span class="n">adapted_attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_adapter</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>

        <span class="c1"># Apply frozen feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">adapted_attention</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">adapted_attention</span><span class="p">)</span>

        <span class="c1"># Inject second adapter after feed-forward
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn_adapter</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>All set, and we need to load pre-trained model and wrap it’s target layers with this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define adapter size
</span><span class="n">adapter_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Wrap all encoder layers with adapter-enabled versions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">)):</span>
    <span class="n">original_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nc">AdapterTransformerLayer</span><span class="p">(</span><span class="n">original_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
<span class="c1"># Check that only adapters will be trained
</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trainable parameters: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Now you can tokenize input and train like usual.
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are lightweight and powerful.</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></div> <p>Now, I’m gonna show how to use adapters in transformers library. It’s faster, easier, and production-tested.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pip install adapter-transformers
</span>
<span class="c1"># `BertAdapterModel` = a special version of BERT that allows adapter injection.
</span><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertAdapterModel</span>
<span class="kn">from</span> <span class="n">transformers.adapters</span> <span class="kn">import</span> <span class="n">AdapterConfig</span>

<span class="c1"># Load model and tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertAdapterModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define adapter configuration
</span><span class="n">config</span> <span class="o">=</span> <span class="n">AdapterConfig</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">pfeiffer</span><span class="sh">"</span><span class="p">,</span>                    <span class="c1"># Adapter type: "pfeiffer", "houlsby", etc.
</span>    <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>          <span class="c1"># Bottleneck size (768 / 16 = 48)
</span>    <span class="n">leave_out</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>            <span class="c1"># Skip layer 0 and 11 (i.e., don't inject there)
</span>    <span class="n">non_linearity</span><span class="o">=</span><span class="sh">"</span><span class="s">gelu</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add adapter with a custom name
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Activate + train this adapter
</span><span class="n">model</span><span class="p">.</span><span class="nf">train_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#Tokenize Input and Forward Pass
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are efficient!</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Last hidden state (batch_size, seq_len, hidden_dim)
</span><span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Add a Classification Head (for downstream tasks)
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_classification_head</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Switch to training mode
</span><span class="n">model</span><span class="p">.</span><span class="nf">train_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Forward pass for classification
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are awesome!</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span>

<span class="c1"># Training Only Adapter Parameters
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>

<span class="c1"># Save / Load Adapters Separately
</span>
<span class="c1"># Save adapter after training
</span><span class="n">model</span><span class="p">.</span><span class="nf">save_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">saved/my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Load later into another model
</span><span class="n">model</span><span class="p">.</span><span class="nf">load_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">saved/my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">load_as</span><span class="o">=</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">set_active_adapters</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="parallel-adapters">Parallel Adapters</h2> <p>While bottleneck adapters are inserted sequentially in the model’s architecture, <strong>parallel Adapters</strong> inject adapter modules in parallel with the main feed-forward layers in each Transformer block, instead of sequentially. This means that the output of the adapter is added to the output of the feed-forward network, not to its input.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/parallel-adapter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/parallel-adapter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/parallel-adapter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-05-11-adapters/parallel-adapter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Parallel adapter." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Parallel Adapter </div> <p>Let \(x\) be the input to the Transformer block. The original feed-forward output is \(\mathrm{FFN}(x)\), and the adapter path is:</p> \[\mathrm{Adapter}(x) = W_\text{up} \, \sigma(W_\text{down} \, x)\] <p>The final output becomes:</p> \[y = \mathrm{FFN}(x) + \mathrm{Adapter}(x)\] <p>This allows the adapter to independently learn task-specific modifications without disrupting the main path.</p> <p>The parallel design has a slight computational overhead but can better preserve the pre-trained representations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ParallelAdapter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        
        <span class="c1"># Initialize weights
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        
        <span class="c1"># Scale factor - can be trained or fixed
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_project</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_project</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Scale the adapter output and add to original
</span>        <span class="k">return</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">x</span>
</code></pre></div></div> <p>The integration into a transformer layer would be similar to the bottleneck adapter, but the adapter would be applied in parallel rather than sequentially.</p> <h2 id="low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</h2> <p><a href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><strong>LoRA (Low-Rank Adaptation)</strong></a> introduced by <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">Hu et al. (2021)</a> replaces or augments weight matrices with low-rank decompositions. Instead of fine-tuning a full matrix \(W \in \mathbb{R}^{d \times d}\), LoRA learns two smaller matrices:</p> \[W' = W + A B \quad \text{with} \quad A \in \mathbb{R}^{d \times r}, \; B \in \mathbb{R}^{r \times d}\] <p>Where \(r \ll d\) (typically \(r = 8\) or \(4\)). This drastically reduces the number of trainable parameters. LoRA is usually applied to the attention projection layers (query/key/value/output).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/lora-adapter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/lora-adapter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/lora-adapter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-05-11-adapters/lora-adapter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LoRA Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> LoRA Adapter </div> <p>Intuitively, LoRA adds a “low-rank path” through which task-specific information can flow, while keeping the rest of the model fixed.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        LoRA implementation for linear layers.
        
        Args:
            in_features: Input dimension
            out_features: Output dimension
            rank: Rank of the low-rank decomposition
            alpha: Scaling factor for the LoRA contribution
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">rank</span>
        
        <span class="c1"># LoRA weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        
        <span class="c1"># Initialize weights
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_B</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># LoRA contribution: scaling * (x @ A) @ B
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">)</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span>
</code></pre></div></div> <p>Now, let’s apply LoRA to a pre-trained linear layer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">linear_layer</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Wraps a pre-trained linear layer with LoRA functionality.
        
        Args:
            linear_layer: The pre-trained nn.Linear module to adapt
            rank: Rank of the low-rank decomposition
            alpha: Scaling factor
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear_layer</span>
        
        <span class="c1"># Freeze original weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="c1"># Add LoRA components
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lora</span> <span class="o">=</span> <span class="nc">LoRALayer</span><span class="p">(</span>
            <span class="n">linear_layer</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> 
            <span class="n">linear_layer</span><span class="p">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Combine original output with LoRA contribution
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>The genius of LoRA is in its efficiency.</p> <p>If the original weight matrix has dimensions n×m, full fine-tuning would require updating n×m parameters. With LoRA, using a rank r, we only need to update r×(n+m) parameters. For large matrices where r « min(n,m), this represents a massive reduction in trainable parameters.</p> <h3 id="applying-lora-to-a-transformer">Applying LoRA to a Transformer</h3> <p>In practice, LoRA is typically applied to specific weight matrices within a transformer, most commonly the query and value projection matrices in attention layers. Here’s how to adapt a transformer’s attention mechanism with LoRA:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>

<span class="k">def</span> <span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">q_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">v_proj</span><span class="sh">"</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">
    Apply LoRA to specific modules in a transformer model.
    
    Args:
        model: A Hugging Face transformer model
        rank: Rank for LoRA decomposition
        alpha: Scaling factor
        target_modules: List of module names to apply LoRA to
    </span><span class="sh">"""</span>
    <span class="c1"># First, freeze all parameters
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="c1"># Then apply LoRA to target modules
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">any</span><span class="p">(</span><span class="n">target_name</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="n">target_modules</span><span class="p">):</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="c1"># Get the parent module
</span>                <span class="n">parent_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">child_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">parent_module</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_submodule</span><span class="p">(</span><span class="n">parent_name</span><span class="p">)</span>
                
                <span class="c1"># Replace with LoRA version
</span>                <span class="n">lora_layer</span> <span class="o">=</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
                <span class="nf">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">lora_layer</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">lora_model</span> <span class="o">=</span> <span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div> <h3 id="quantized-lora-qlora">Quantized LoRA (QLoRA)</h3> <p><a href="https://arxiv.org/abs/2305.14314" rel="external nofollow noopener" target="_blank">QLoRA</a>, takes LoRA’s efficiency to the next level by combining it with quantization techniques. The key insight is to keep the base model in a quantized format (typically 4-bit precision) while applying LoRA adapters in full precision.</p> <p>QLoRA has been a game-changer for democratizing LLM fine-tuning, enabling the adaptation of models with over 70 billion parameters on a single consumer GPU.</p> <h2 id="prefix-tuning-virtual-tokens-in-hidden-space">Prefix Tuning: Virtual Tokens in Hidden Space</h2> <p>Now let’s shift our focus to another family of PEFT methods that operate by introducing trainable tokens to the input sequence or hidden states: Prefix Tuning and Prompt Tuning.</p> <p>Prefix Tuning, introduced by <a href="https://arxiv.org/abs/2101.00190" rel="external nofollow noopener" target="_blank">Li and Liang (2021)</a>, prepends a small number of learned key-value vectors (“prefixes”) to the attention mechanism.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/prefix-tuning-480.webp 480w,/assets/img/posts/2025-05-11-adapters/prefix-tuning-800.webp 800w,/assets/img/posts/2025-05-11-adapters/prefix-tuning-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-05-11-adapters/prefix-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Prefix Tuning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Prefix Tuning </div> <p>Instead of modifying weights, it expands the input to attention as:</p> \[\text{Attention}(\text{prefix} + x)\] <p>This means the model sees the learned prefix as a pseudo-context for every input, influencing the attention output without changing the underlying Transformer parameters.</p> <p>Prefix tuning is powerful for generation tasks like summarization or translation where modifying the attention context is sufficient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrefixTuningModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">prefix_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of Prefix Tuning.
        
        Args:
            hidden_size: Model</span><span class="sh">'</span><span class="s">s hidden size
            prefix_length: Number of virtual tokens to add
            num_layers: Number of transformer layers
            num_heads: Number of attention heads
            head_dim: Dimension of each attention head
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prefix_length</span> <span class="o">=</span> <span class="n">prefix_length</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        
        <span class="c1"># Create a prefix for each layer for both key and value states
</span>        <span class="c1"># Shape: [num_layers, 2, prefix_length, num_heads, head_dim]
</span>        <span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">prefix_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Initialize with a small standard deviation
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key_value_states</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Prepend prefix to key and value states for a specific layer.
        
        Args:
            key_value_states: Tuple of (key, value) states from the model
            layer_idx: Current transformer layer index
        </span><span class="sh">"""</span>
        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">key_value_states</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get the prefix for the current layer
</span>        <span class="c1"># Shape: [2, prefix_length, num_heads, head_dim]
</span>        <span class="n">prefix</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
        
        <span class="c1"># Extract key and value prefixes
</span>        <span class="n">key_prefix</span> <span class="o">=</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_prefix</span> <span class="o">=</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Reshape to match model's key and value shapes
</span>        <span class="c1"># From: [batch_size, prefix_length, num_heads, head_dim]
</span>        <span class="c1"># To: [batch_size, num_heads, prefix_length, head_dim]
</span>        <span class="n">key_prefix</span> <span class="o">=</span> <span class="n">key_prefix</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value_prefix</span> <span class="o">=</span> <span class="n">value_prefix</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="c1"># Concatenate with original states
</span>        <span class="c1"># Original shape: [batch_size, num_heads, seq_length, head_dim]
</span>        <span class="n">new_key_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">key_prefix</span><span class="p">,</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">new_value_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">value_prefix</span><span class="p">,</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="nf">return </span><span class="p">(</span><span class="n">new_key_states</span><span class="p">,</span> <span class="n">new_value_states</span><span class="p">)</span>
</code></pre></div></div> <p>To integrate this with a transformer model, we need to modify each attention layer to incorporate the prefixes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrefixTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">prefix_module</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prefix_module</span> <span class="o">=</span> <span class="n">prefix_module</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        
        <span class="c1"># Freeze the original layer
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Extract the attention module (implementation depends on model architecture)
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span>
        
        <span class="c1"># Prepare key, query, value states as in the original attention
</span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        
        <span class="c1"># Reshape for multi-head attention
</span>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="n">head_size</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="n">num_attention_heads</span>
        
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Apply prefix
</span>        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">prefix_module</span><span class="p">((</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_idx</span><span class="p">)</span>
        
        <span class="c1"># Update attention mask for the additional prefix tokens
</span>        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prefix_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> 
                <span class="mi">1</span><span class="p">,</span> 
                <span class="mi">1</span><span class="p">,</span> 
                <span class="n">self</span><span class="p">.</span><span class="n">prefix_module</span><span class="p">.</span><span class="n">prefix_length</span><span class="p">,</span> 
                <span class="n">device</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prefix_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="c1"># Calculate attention scores and outputs
</span>        <span class="c1"># (Implementation depends on the specific attention mechanism)
</span>        <span class="c1"># ...
</span>        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>The above implementation is conceptual and would need to be adapted based on the specific transformer architecture you’re working with.</p> <h3 id="prompt-tuning">Prompt Tuning</h3> <p><a href="https://arxiv.org/abs/2104.08691" rel="external nofollow noopener" target="_blank">Prompt Tuning</a>, can be seen as a simplified version of Prefix Tuning. Rather than adding virtual tokens at every layer, Prompt Tuning only prepends trainable embeddings to the input sequence embeddings at the first layer.</p> <p>Here’s a straightforward implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PromptTuning</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">prompt_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of Prompt Tuning.
        
        Args:
            model: The pre-trained transformer model
            prompt_length: Number of virtual tokens to add
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_length</span> <span class="o">=</span> <span class="n">prompt_length</span>
        
        <span class="c1"># Freeze model parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Get embedding dimension from the model
</span>        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Create soft prompt embeddings
</span>        <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
        
        <span class="c1"># Initialize with embeddings of random tokens from the vocabulary
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">random_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">prompt_length</span><span class="p">,))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">[</span><span class="n">random_indices</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">attention_mask</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get input embeddings
</span>        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">()(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">"</span><span class="s">inputs_embeds</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Expand soft prompts for batch size and prepend to input embeddings
</span>        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Adjust attention mask for the added prompt tokens
</span>        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prompt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prompt_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Forward pass through the model without input_ids
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
</code></pre></div></div> <h2 id="bitfit-adapter">BitFit Adapter</h2> <p>BitFit, proposed by <a href="https://arxiv.org/abs/2106.10199" rel="external nofollow noopener" target="_blank">Zaken et al. (2021)</a>, takes a radically different approach from the methods we’ve discussed so far. Instead of adding new parameters, BitFit selectively trains only the bias terms in the pre-trained model, leaving all other parameters frozen.</p> <p>Despite its extreme parameter efficiency, BitFit has shown good performance across various tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_bitfit_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Apply BitFit to a transformer model by only training bias terms.
    
    Args:
        model: A PyTorch model
    </span><span class="sh">"""</span>
    <span class="c1"># First, freeze all parameters
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="c1"># Then unfreeze only bias parameters
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
    
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <p>The implementation is remarkably simple, yet BitFit can achieve competitive performance while training less than 0.1% of the original model parameters in many cases.</p> <h2 id="ia-infused-adapter-by-inhibiting-and-amplifying-inner-activations">IA³: Infused Adapter by Inhibiting and Amplifying Inner Activations</h2> <p><strong>IA³ (Input-Aware Activation Adjustment)</strong> by <a href="https://arxiv.org/abs/2205.05638" rel="external nofollow noopener" target="_blank">Liu et al. (2022)</a>, modifies the element-wise activation scale and bias <em>after</em> each linear transformation. For a layer with output \(x\), IA³ computes:</p> \[x' = \alpha \cdot x + \beta\] <p>Here, \(\alpha\) and \(\beta\) are trainable parameters. This is similar to fine-tuning just the scale and shift of activations and can be extremely efficient.</p> <p>IA³ is useful when slight shifts in activation distributions are enough to steer the model to the new task.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/ia3-480.webp 480w,/assets/img/posts/2025-05-11-adapters/ia3-800.webp 800w,/assets/img/posts/2025-05-11-adapters/ia3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-05-11-adapters/ia3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="IA3 Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> IA3 Adapter </div> <p>Let’s check how it looks like in the code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of IA³ scaling vectors.
        
        Args:
            hidden_size: Dimension to scale
            ia3_type: Where to apply IA³ (</span><span class="sh">'</span><span class="s">feed_forward</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">attention_output</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">attention_value</span><span class="sh">'</span><span class="s">)
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ia3_type</span> <span class="o">=</span> <span class="n">ia3_type</span>
        
        <span class="c1"># Create scaling vectors initialized to ones
</span>        <span class="k">if</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For the output of the feed-forward layer
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_output</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For scaling attention outputs
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For scaling value vectors in attention
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Apply scaling to input tensor.
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For attention values, we reshape for broadcasting across batch and seq dimensions
</span>            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For feed-forward and attention outputs
</span>            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span>
</code></pre></div></div> <p>Integrating IA³ with a transformer model requires injecting the scaling at specific points:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IA3TransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        
        <span class="c1"># Freeze original parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Add IA³ modules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_value_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention_output_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_output</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Extract components (implementation is model-specific)
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span>
        
        <span class="c1"># Compute query, key, value projections
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        
        <span class="c1"># Apply IA³ to value projections
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_value_ia3</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        
        <span class="c1"># Compute attention
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Apply IA³ to attention output
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_output_ia3</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Apply IA³ to feed-forward output
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward_ia3</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>IA³’s efficiency is remarkable: for a model with hidden size h, it adds only 3h parameters per layer, compared to the millions in the original layer.</p> <h2 id="compacter-kronecker-products-for-ultimate-efficiency">Compacter: Kronecker Products for Ultimate Efficiency</h2> <p><strong>Compacter</strong>, proposed by <a href="https://arxiv.org/abs/2106.04647" rel="external nofollow noopener" target="_blank">Mahabadi et al. (2021)</a>, builds on the adapter idea, but instead of learning full matrices for down/up projection, it composes them from Kronecker products of smaller matrices.</p> \[W = W_1 \otimes W_2\] <p>This gives an expressive yet parameter-efficient formulation. Compacter adapters can learn more complex transformations than simple low-rank matrices without adding much overhead.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/compacter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/compacter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/compacter-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-05-11-adapters/compacter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Compacter Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Compacter Adapter </div> <p>Let’s implement Compacter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Parameterized Hypercomplex Multiplication using Kronecker products.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span> <span class="o">=</span> <span class="n">factorized_phm</span>
        
        <span class="c1"># Calculate dimensions for the factors
</span>        <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">in_features</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>
        
        <span class="c1"># Ensure dimensions are compatible with factorization
</span>        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">==</span> <span class="n">in_features</span><span class="p">,</span> \
            <span class="sh">"</span><span class="s">Input features must be a perfect square for factorization</span><span class="sh">"</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">==</span> <span class="n">out_features</span><span class="p">,</span> \
            <span class="sh">"</span><span class="s">Output features must be a perfect square for factorization</span><span class="sh">"</span>
        
        <span class="k">if</span> <span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># Factorized representation using shared factors
</span>            <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Full Kronecker factors
</span>            <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
        
        <span class="c1"># Initialize parameters
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">_init_parameters</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">_init_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Initialize the parameters with small random values.</span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">ones_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">kronecker_product</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Compute the Kronecker product of matrices A and B.
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s3</span><span class="p">,</span> <span class="n">s4</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">B</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Reshape for matrix multiplication
</span>        <span class="n">A_reshaped</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span> <span class="o">*</span> <span class="n">s2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">B_reshaped</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s3</span> <span class="o">*</span> <span class="n">s4</span><span class="p">)</span>
        
        <span class="c1"># Perform outer product
</span>        <span class="n">kron_prod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">A_reshaped</span><span class="p">,</span> <span class="n">B_reshaped</span><span class="p">)</span>
        
        <span class="c1"># Reshape to get the final Kronecker product
</span>        <span class="k">return</span> <span class="n">kron_prod</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">s3</span><span class="p">,</span> <span class="n">s4</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span> <span class="o">*</span> <span class="n">s3</span><span class="p">,</span> <span class="n">s2</span> <span class="o">*</span> <span class="n">s4</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass using PHM.
        x: Input tensor of shape [batch_size, in_features]
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Compute the weight matrix using PHM
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># Using factorized representation
</span>            <span class="n">weight</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rank</span><span class="p">):</span>
                <span class="c1"># Apply scaling factor
</span>                <span class="n">kronecker_factor</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">kronecker_product</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">weight</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">*</span> <span class="n">kronecker_factor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Using full representation
</span>            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rank</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Apply the weight matrix - handle factorized vs full differently
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># For factorized version, we already have batch-specific weights
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">weight</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For the full version, we use simple matrix multiply
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">weight</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compacter adapter implementation using PHM for weight parameterization.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adapter_size</span> <span class="o">=</span> <span class="n">adapter_size</span>
        
        <span class="c1"># Down projection using PHM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="n">factorized_phm</span><span class="p">)</span>
        
        <span class="c1"># Activation function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        
        <span class="c1"># Up projection using PHM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="n">factorized_phm</span><span class="p">)</span>
        
        <span class="c1"># Additional components
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Scaling factor for the adapter output
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass through the Compacter adapter.
        </span><span class="sh">"""</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        
        <span class="c1"># Down projection with PHM
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Up projection with PHM
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Apply scaling and add residual
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># Integrating Compacter with a transformer layer would be similar to the adapter implementation
</span><span class="k">class</span> <span class="nc">CompacterTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">transformer_layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span><span class="p">.</span><span class="n">all_head_size</span>  <span class="c1"># Model specific
</span>        
        <span class="c1"># Freeze original parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="c1"># Add Compacter adapters
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_adapter</span> <span class="o">=</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn_adapter</span> <span class="o">=</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Original attention mechanism
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Apply attention adapter
</span>        <span class="n">adapted_attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_adapter</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Original feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">adapted_attention</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">adapted_attention</span><span class="p">)</span>
        
        <span class="c1"># Apply ffn adapter
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn_adapter</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/minds-as-computers/">Physical Symbol Systems and the Language of Thought</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/building-a-transformer/">Building a Transformer (Cross-Attention and MHA Explained)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/tokenization/">Understanding Byte-Pair Encoding Algorithm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/creative-ai/">Can AI Achieve True Creativity?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/lora-algorithm-for-llms/">Algebraic Foundations of Low-Rank Adaptation</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eva Koroleva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>