<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PR5ZCQWED0"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PR5ZCQWED0");</script> <script src="/assets/js/h2-cards.js"></script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLMs for Those Who Missed Out | Eva Koroleva </title> <meta name="author" content="Eva Koroleva"> <meta name="description" content="Let's talk about large language models. Once again."> <meta name="keywords" content="machine-learning, deep-learning, python, neural-networks, cognitive-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eva</span> Koroleva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLMs for Those Who Missed Out</h1> <p class="post-meta"> Created in April 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp,</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   ·   <a href="/blog/category/old-posts"> <i class="fa-solid fa-tag fa-sm"></i> old-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>An LLM (large language model) consists of just two files:</p> <ul> <li>Large file with billions of parameters (weights)</li> <li>Small file with about 500 lines of code to run the model</li> </ul> <p>LLM doesn’t take up much space and doesn’t require extensive computing power.</p> <p>For example, the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf" rel="external nofollow noopener" target="_blank">Llama-2–70b model</a> from Meta, which is open source and has 70 billion parameters, is just 140 GB and can be run locally on a MacBook without internet access. You can <a href="https://www.llama.com/llama-downloads/" rel="external nofollow noopener" target="_blank">download this model</a> from Meta’s website and use it for free.</p> <p>However, you need significant computing resources to get these parameters (train the model).</p> <h2 id="how-to-train-a-model">How to Train a Model</h2> <h3 id="stage-1-pretraining-training-the-base-model">Stage 1: Pretraining (Training the base model)</h3> <p>Think of this process as compressing the Internet into a neural network, similar to how a ZIP file compresses documents into an archive.</p> <p>Here’s what you need to get a model like Llama-2:</p> <ul> <li>Take a “slice of the internet,” approximately ten terabytes of text in size</li> <li>Use a cluster of ~6,000 GPUs (specialized graphics processors used for heavy computations)</li> <li>Compress the text into the neural network. This takes about 12 days and costs around $2M</li> <li>Acquire the base model (the file with parameters)</li> </ul> <p>The primary function of the base model is to predict the next word. You input a sequence of words, and it outputs the next most likely word based on the texts it was trained on.</p> <p>For example, for the phrase “cat sat on a,” the LLM might predict the word “mat” with 97% probability.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/0-480.webp 480w,/assets/img/posts/2024-04-24-llms/0-800.webp 800w,/assets/img/posts/2024-04-24-llms/0-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2024-04-24-llms/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image from the presentation illustrating how the neural network works" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image from the presentation illustrating how the neural network works </div> <p>That’s basically how compression works: if you can predict the next word accurately, you can use this information to compress the original data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/1-480.webp 480w,/assets/img/posts/2024-04-24-llms/1-800.webp 800w,/assets/img/posts/2024-04-24-llms/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2024-04-24-llms/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="I checked, and ChatGPT indeed responds with “mat”" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> I checked, and ChatGPT indeed responds with “mat” </div> <p>However, unlike a ZIP file, where compression is lossless, LLM “compresses” the internet with losses. This is why models can hallucinate: they make up information that doesn’t actually exist but appears plausible (similar to what they have seen in the training data).</p> <p>Moreover, models can sometimes produce completely unexpected things. For example, ChatGPT knows the answer to the question, “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer), but it doesn’t know the answer to the question, “Who is Mary Lee Pfeiffer’s son?”</p> <p>This isn’t a typical database that simply stores information; it’s a different format we don’t fully understand.</p> <p>This clearly illustrates that we don’t quite grasp how this whole thing works and can only see the results it produces.</p> <h3 id="stage-2-finetuning-additional-training">Stage 2: Finetuning (Additional training)</h3> <p>The base model isn’t very practical for everyday use. We don’t just want to receive continuous word sequences; we want to ask questions and get answers.</p> <p>This requires finetuning — a process in which we develop an assistant model that answers questions.</p> <p>The training process for the assistant model is similar to that of the base model, but now we train the model not on internet texts but on data we manually collect. Companies hire people who write questions and answer them.</p> <p>If the training of the base model occurs on huge volumes of often low-quality text, the training of the assistant model involves comparatively smaller volumes (say, 100,000 documents), but they are all of very high quality.</p> <p>After such finetuning, the model understands that when asked a question, it should respond in the style of a helpful assistant.</p> <p>Unlike the training of the base model, which is conducted at most 2–3 times a year, finetuning can be done regularly, even weekly, as it is much less costly.</p> <h3 id="stage-3-optional-comparisons">Stage 3 (optional): Comparisons</h3> <p>In many cases it’s easier not to write a response from scratch but to compare several different answer options and choose the best one.</p> <p>The data from these comparisons is used to train the model further.</p> <p>At OpenAI, this is called RLHF (Reinforcement Learning from Human Feedback).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/2-480.webp 480w,/assets/img/posts/2024-04-24-llms/2-800.webp 800w,/assets/img/posts/2024-04-24-llms/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2024-04-24-llms/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Difference between Open and Closed Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Difference between Open and Closed Models. Source: Chatbot leaderboards </div> <p>Models with open weights, such as Llama-2, are still inferior compared to proprietary models like GPT-4 and Claude.</p> <p>However, proprietary models cannot be downloaded for personal use or further training; they can only be operated through a web interface or an API (though the introduction of Custom GPTs at OpenAI is a first step towards customization)</p> <h2 id="capabilities-of-llms-using-chatgpt-as-an-example">Capabilities of LLMs (using ChatGPT as an example)</h2> <p>Prompted by an LLM, it can understand what the user needs and use external tools for this:</p> <ul> <li>Search the internet if the user needs up-to-date information (goes to Bing, retrieves a list of the latest links for the query, copies the full text of each link into its context window, and summarizes the information)</li> <li>Use a calculator to do calculations</li> <li>Use a Python library to draw graphs</li> <li>Draw images using DALL-E</li> <li>Write code</li> </ul> <p>Moreover, LLMs can be multimodal. They can recognize text, voice, images, or video and respond with voice, images, or video.</p> <h2 id="can-llms-make-decisions">Can LLMs Make Decisions?</h2> <p>There’s a well-known book by Kahneman, “Thinking, Fast and Slow.”</p> <p>The main idea of the book is that there are two systems in the brain: System 1, which has fast, automatic reactions, and System 2, which is slower, rational, and conscious and makes complex decisions.</p> <p>For example, the answer to question 2+2 comes from System 1 because this knowledge is automatic for us. But calculating 17×24 requires System 2.</p> <p>If we apply these terms, current LLMs only possess System 1. They can only provide the most likely next word in real time.</p> <p>It would be great if we could come to an LLM and say: here’s my question; you can think for 30 minutes, but I need a very accurate and high-quality answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/3-480.webp 480w,/assets/img/posts/2024-04-24-llms/3-800.webp 800w,/assets/img/posts/2024-04-24-llms/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2024-04-24-llms/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Thought tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Thought tree </div> <p>No models can do that yet. But it would be desirable for a model to have something like a “thought tree,” through which it could navigate, analyze the results, go back and try again until it achieves a result it is most confident about.</p> <h2 id="can-llms-train-themselves">Can LLMs Train Themselves?</h2> <p>There’s a famous case where AlphaGo (a program that plays Go) was trained in two stages:</p> <p>First, it was trained on human games and learned to play very well. Then it began to train itself-playing against itself, trying to maximize the likelihood of winning-and significantly improved its quality.</p> <p>But in LLMs, we’re only at stage 1-training only occurs on materials created by humans.</p> <p>Why is this?</p> <p>In the game of Go, there is a very clear criterion for success-a won game- and you can train the model to maximize the likelihood of winning.</p> <p>In LLMs, the criteria are not so obvious. It’s not clear how to assess whether the result is good.</p> <p>Such criteria can be found in some narrow topics, but in general, it’s still hard to imagine.</p> <h2 id="what-llms-will-be-able-to-do-in-a-few-years">What LLMs Will Be Able to Do in a Few Years</h2> <p>Finally, a small forecast.</p> <p>In a few years, LLMs:</p> <ul> <li>Will have more knowledge than any human on all subjects</li> <li>Can operate on the internet</li> <li>Can use existing software infrastructure (calculator, Python, mouse/keyboard)</li> <li>Can see and generate images and videos</li> <li>Can hear and speak, and also generate music</li> <li>Can think for a long period using System 2</li> <li>Can train themselves in areas where there are clear criteria</li> <li>Can be customized and refined for specific tasks. Many versions will exist in app stores</li> <li>Can interact with other LLMs</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/minds-as-computers/">Physical Symbol Systems and the Language of Thought</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/building-a-transformer/">Building a Transformer (Cross-Attention and MHA Explained)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/tokenization/">Understanding Byte-Pair Encoding Algorithm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/creative-ai/">Can AI Achieve True Creativity?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/lora-algorithm-for-llms/">Algebraic Foundations of Low-Rank Adaptation</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eva Koroleva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 03, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>