<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LoRA PEFT algorithm for LLM | Eva Koroleva </title> <meta name="author" content="Eva Koroleva"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="machine-learning, deep-leadning, python, neural-networks"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%AA%BD&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Eva</span> Koroleva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">LoRA PEFT algorithm for LLM</h1> <p class="post-meta"> Created in December 30, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp,</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm,</a>   <a href="/blog/tag/lora"> <i class="fa-solid fa-hashtag fa-sm"></i> lora</a>   ·   <a href="/blog/category/let-s"> <i class="fa-solid fa-tag fa-sm"></i> Let's</a>   <a href="/blog/category/look"> <i class="fa-solid fa-tag fa-sm"></i> look</a>   <a href="/blog/category/at"> <i class="fa-solid fa-tag fa-sm"></i> at</a>   <a href="/blog/category/the"> <i class="fa-solid fa-tag fa-sm"></i> the</a>   <a href="/blog/category/linear"> <i class="fa-solid fa-tag fa-sm"></i> linear</a>   <a href="/blog/category/algebra"> <i class="fa-solid fa-tag fa-sm"></i> algebra</a>   <a href="/blog/category/behind"> <i class="fa-solid fa-tag fa-sm"></i> behind</a>   <a href="/blog/category/popular"> <i class="fa-solid fa-tag fa-sm"></i> popular</a>   <a href="/blog/category/algorithm"> <i class="fa-solid fa-tag fa-sm"></i> algorithm</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Large language models are amazing, but for some practical tasks, they require fine-tuning. However, because of their size and the high cost of training on GPUs, traditional fine-tuning methods have become impractical.</p> <p>PEFT makes it faster and cheaper to adapt these models. The main idea behind these approaches is to train only a small subset of the model’s parameters while keeping the rest frozen.</p> <p>In this article, I’ll dive into how Low-Rank Adaptation (LoRA) works.</p> <p>Let’s start with some key linear algebra terms and properties that are essential to understanding the algorithm.</p> <h2 id="matrix">Matrix</h2> <p>A matrix $A$ is a rectangular array of numbers arranged in rows and columns. The element of a matrix at the intersection of the $i$-th row and the $j$-th column is denoted as $a_{ij}$ or $A_{ij}$.</p> <p>In the context of neural networks, a weight matrix is a matrix whose elements represent the weights between neurons in adjacent layers. Each number in the matrix indicates the strength of the connection between two neurons: the larger the number, the stronger the connection.</p> <h2 id="rank-of-a-matrix">Rank of a Matrix</h2> <p>The rank of a matrix $A$ (denoted as $\text{rank}(A)$ or $r(A)$) is the maximum number of linearly independent rows or columns in the matrix. Roughly speaking, it reflects how much information the matrix actually contains. If the rank is low, it means the matrix is compressed, and many rows/columns are linear combinations of others.</p> <p>The rank of a matrix can range from 0 (for a zero matrix) to the smaller of the number of rows and columns in the matrix. If $A$ is a matrix of size $m \times n$, then $0 \le \text{rank}(A) \le \min(m, n)$.</p> <p>A matrix $A$ of size $m \times n$ is called a full-rank matrix if its rank equals the smaller of its dimensions: $\text{rank}(A) = \min(m, n)$.</p> <p>A matrix $A$ of size $m \times n$ is called a rank-deficient matrix if its rank is less than the smaller of its dimensions: $\text{rank}(A) &lt; \min(m, n)$.</p> <h2 id="rank-of-the-product-and-decomposition">Rank of the Product and Decomposition</h2> <p>The rank of the product of two matrices cannot exceed the rank of either matrix: if $A$ is a matrix of size $m \times n$ and $B$ is a matrix of size $n \times p$, then $\text{rank}(AB) \le \min(\text{rank}(A), \text{rank}(B))$.</p> <p>Rank Decomposition is the process of breaking down complex information into simpler components.</p> <p>Any matrix $A$ with rank $r$ can be represented as the product of two smaller matrices: $A = U V^T$, where $U$ is a matrix of size $m \times r$ and $V$ is a matrix of size $n \times r$. This decomposition is also known as <strong>low-rank factorization</strong>.</p> <p>There are various methods to compute such decompositions, such as Singular Value Decomposition (SVD).</p> <h2 id="low-rank-adaptation-lora-hypothesis">Low-Rank Adaptation (LoRA) Hypothesis</h2> <p>Previous research has shown that overparameterized large models often have low intrinsic dimensionality. The core idea of LoRA is that weight changes during model adaptation also have low intrinsic rank/dimensionality. Specifically, if $W_{n \times k}$ represents the weights of a layer and $\Delta W_{n \times k}$ represents the weight changes during adaptation, the authors hypothesize that $\Delta W_{n \times k}$ is a low-rank matrix.</p> <p><em>So, you don’t need to change all the weights of a layer to adapt the model; it’s enough to tweak only a small subset.</em></p> <h3 id="why-does-this-make-sense">Why does this make sense?</h3> <p>LLMs are trained to capture general representations of their domain.</p> <p>These models learn a rich set of features, which allows them to solve various tasks with decent accuracy even without fine-tuning.</p> <p>However, when adapting such a model to a specific task or dataset, it’s often sufficient to refine or retrain only a few features. This means that the update matrix ($\Delta W$) can have a low rank.</p> <h2 id="method">Method</h2> <p>LoRA constrains the rank of the update matrix $\Delta W$ using rank decomposition. It represents $\Delta W_{n \times k}$ as the product of two low-rank matrices, $B_{n \times r}$ and $A_{r \times k}$, where $r \ll \min(n, k)$. This means that the forward pass of the layer, originally $Wx$, is modified to $Wx + BAx$.</p> <ul> <li>The matrix $A$ is randomly initialized with Gaussian values.</li> <li>The matrix $B$ is initialized to 0, so $BA = 0$ at the start of training.</li> <li>The $BA$ update is further scaled by a factor of $\alpha / r$.</li> </ul> <h3 id="practical-benefits">Practical Benefits:</h3> <ol> <li> <strong>Reduced training time and memory usage:</strong> Using this method, only $r(n + k)$ parameters need to be tuned during model adaptation. Since $r \ll \min(n, k)$, this is significantly fewer than the $nk$ parameters that would otherwise need to be updated. This drastically reduces the time and memory required for fine-tuning.</li> <li> <strong>No additional inference overhead:</strong> In production, you can precompute $W’ = W + BA$ and use the result as usual, ensuring no added latency during inference.</li> <li> <strong>Easier task switching:</strong> Replacing only the LoRA weights instead of all parameters allows for cheaper and faster task switching. You can create multiple fine-tuned models and quickly switch between them.</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llms-for-those-who-missed-out/">LLMs for Those Who Missed Out</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/python-code/">How to Write Good Python Code</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Eva Koroleva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>