<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://xmarva.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xmarva.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-11T15:32:40+00:00</updated><id>https://xmarva.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for exploring the depths of natural and artificial intelligence. </subtitle><entry><title type="html">PEFT - Adapter Overview [from Bottleneck to LoRA]</title><link href="https://xmarva.github.io/blog/2025/adapters/" rel="alternate" type="text/html" title="PEFT - Adapter Overview [from Bottleneck to LoRA]"/><published>2025-05-11T10:00:00+00:00</published><updated>2025-05-11T10:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/adapters</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/adapters/"><![CDATA[<p>In the rapidly evolving landscape <a href="https://xmarva.github.io/blog/2025/building-a-transformer/">transformer-based architectures</a>, a significant challenge has emerged: how do we customize these increasingly massive models for specific tasks without breaking the bank on computational resources?</p> <p>Enter Parameter-Efficient Fine-Tuning (PEFT), a family of techniques that has revolutionized how we adapt pre-trained models to downstream tasks.</p> <h2 id="the-fine-tuning-dilemma">The Fine-Tuning Dilemma</h2> <p>So, you’ve got access to a SoTA LM with billions of parameters.</p> <p>Perhaps it’s GPT-4, LLaMA 3, Mistral or Qwen. You want to adapt this model to a specialized domain like medical text analysis or legal document processing.</p> <p>The traditional approach would involve fine-tuning the entire model on your domain-specific data.</p> <p>Full fine-tuning comes with substantial costs:</p> <ol> <li><strong>Computational Expense</strong>: Training billions of parameters requires significant GPU resources.</li> <li><strong>Storage Overhead</strong>: Each fine-tuned version requires storing a complete copy of the model</li> <li><strong>Catastrophic Forgetting</strong>: Aggressive fine-tuning might cause the model to lose its general capabilities</li> <li><strong>Limited Scalability</strong>: Maintaining multiple specialized versions becomes unmanageable</li> </ol> <p>This is where PEFT techniques come to the rescue. Rather than updating all parameters, PEFT methods focus on adding and training a small number of parameters while keeping most of the pre-trained model frozen. This approach typically requires updating less than 1% of the parameters compared to full fine-tuning, while achieving comparable performance.</p> <p>Let’s understand most significant PEFT methods, their core principles, and implement them using PyTorch for better understanding. Then we’ll explore how to use these techniques with the Hugging Face <code class="language-plaintext highlighter-rouge">peft</code> library for practical applications.</p> <h2 id="general-overview-of-peft-methods">General Overview of PEFT methods</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/method-overview-480.webp 480w,/assets/img/posts/2025-05-11-adapters/method-overview-800.webp 800w,/assets/img/posts/2025-05-11-adapters/method-overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/method-overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Parameter-efficient fine-tuning methods taxonomy." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Parameter-efficient fine-tuning methods taxonomy. Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning.s </div> <hr/> <h3 id="1-addition-based-methods"><strong>1. Addition-Based Methods</strong></h3> <p>These approaches add new, lightweight modules to a pre-trained model while keeping the original weights frozen. This is the most widely explored category and includes two main subtypes: <strong>adapters</strong> and <strong>soft prompts</strong>.</p> <ul> <li><strong>Adapters</strong> (e.g., Bottleneck Adapters, Parallel Adapters): Small neural layers inserted within Transformer blocks that are trained while the rest of the model remains unchanged. Variants differ in placement, structure, and compression strategies.</li> <li><strong>LoRA (Low-Rank Adaptation)</strong>: Instead of fine-tuning full weight matrices, LoRA introduces low-rank decompositions for weight updates (e.g., replacing a full-rank update with <code class="language-plaintext highlighter-rouge">W_down * W_up</code>).</li> <li><strong>Prefix Tuning / Prompt Tuning</strong>: Add trainable vectors (prefixes or prompts) to the model’s input or internal layers. These methods steer model behavior without changing its core parameters.</li> <li><strong>Soft Prompts</strong>: Instead of using discrete tokens, these train continuous embeddings that are prepended to the input. Can be applied to input embeddings or even across all Transformer layers.</li> </ul> <p>Despite adding new parameters, these methods often use significantly less memory and are more computationally efficient due to fewer gradients and optimizer states being updated.</p> <hr/> <h3 id="2-selection-based-methods"><strong>2. Selection-Based Methods</strong></h3> <p>Selective approaches involve fine-tuning only a specific subset of the model’s original parameters, chosen either manually or via structural criteria.</p> <ul> <li><strong>BitFit</strong>: Fine-tunes only the bias terms of the model, drastically reducing the number of parameters involved.</li> <li><strong>IA³ (Infused Adapter by Inhibiting and Amplifying Activations)</strong>: Adds scalar gating parameters to control the flow of information through the attention and feed-forward layers.</li> <li><strong>Layer-wise Selection</strong>: Fine-tunes only the top or bottom layers of the model, or focuses on specific components (e.g., attention vs. FFN).</li> <li><strong>Sparse Fine-Tuning</strong>: Selects parameters to update based on certain criteria (e.g., magnitude or gradients), ignoring model structure. However, this poses practical challenges for current hardware.</li> </ul> <p>These methods are particularly useful when model updates must be extremely lightweight or constrained due to storage, bandwidth, or privacy concerns.</p> <hr/> <h3 id="3-reparameterization-based-methods"><strong>3. Reparameterization-Based Methods</strong></h3> <p>These techniques re-structure the parameter space to enable efficient updates with fewer trainable weights.</p> <ul> <li><strong>LoRA</strong> (also fits here): Uses low-rank matrices to model weight updates, greatly reducing parameter count.</li> <li><strong>Compacter</strong>: Builds on adapters but compresses them using low-rank decomposition and parameter sharing.</li> <li><strong>(IA)³</strong>: Combines gating and reparameterization ideas to modulate specific subcomponents of the model.</li> <li><strong>KronA / Kron Adapter</strong>: Uses Kronecker product decomposition to represent weight updates with a favorable trade-off between expressiveness and size.</li> <li><strong>Intrinsic SAID</strong>: Employs the Fastfood transform to apply updates within a low-rank subspace, based on the theory that fine-tuning operates within a lower-dimensional manifold.</li> </ul> <p>These methods often target attention-related weights like <code class="language-plaintext highlighter-rouge">W_Q</code>, <code class="language-plaintext highlighter-rouge">W_K</code>, <code class="language-plaintext highlighter-rouge">W_V</code>, where much of the model’s representational power lies.</p> <hr/> <h3 id="4-hybrid-methods"><strong>4. Hybrid Methods</strong></h3> <p>Hybrid approaches combine strategies from multiple categories to balance trade-offs in memory, compute, and performance.</p> <ul> <li><strong>MAM Adapter</strong>: Combines Adapters with Prompt Tuning for better modularity.</li> <li><strong>UniPELT</strong>: Merges LoRA with Adapters and Prompts into a unified framework.</li> <li><strong>Compacter++ / Kron Adapter</strong>: Combines adapter-based methods with Kronecker reparameterization to reduce the number of trainable parameters further.</li> </ul> <p>These methods allow researchers to adapt fine-tuning strategies to specific deployment constraints, whether that be edge devices, multi-task learning, or multilingual models.</p> <h2 id="bottleneck-adapters">Bottleneck Adapters</h2> <p>Adapters were among the first successful PEFT approaches, introduced in <a href="https://arxiv.org/abs/1902.00751">Parameter-Efficient Transfer Learning for NLP</a> by Houlsby et al. in 2019.</p> <p>The core idea is elegantly simple: insert small trainable modules into each layer of a pre-trained network while keeping the original parameters frozen.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/bottleneck-480.webp 480w,/assets/img/posts/2025-05-11-adapters/bottleneck-800.webp 800w,/assets/img/posts/2025-05-11-adapters/bottleneck-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/bottleneck.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visualization of possible adapter configurations with corresponding dictionary keys." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of possible adapter configurations with corresponding dictionary keys. </div> <p><strong>Bottleneck adapters</strong> add lightweight feed-forward layers into each Transformer block. These adapter layers typically include:</p> <ul> <li>a <strong>down-projection matrix</strong> that reduces the hidden state dimension from \(d\) to a smaller dimension \(b\),</li> <li>a <strong>non-linear activation</strong> \(\sigma\),</li> <li>an <strong>up-projection matrix</strong> that expands the representation back to the original size \(d\), and</li> <li>a <strong>residual connection</strong>, so the original input is added back after transformation:</li> </ul> \[\text{Adapter}(x) = x + W_{\text{up}} \, \sigma(W_{\text{down}} x)\] <p>Depending on the specific configuration, these adapter layers can be placed at various points inside the Transformer block. Other components like residual connections, layer normalizations, activation functions, and the size of the bottleneck layer can also be customized.</p> <p>The <strong>most important hyperparameter</strong> in this setup is the <strong>bottleneck dimension</strong> \(b\). Rather than setting \(b\) directly, it’s usually defined through a parameter called <code class="language-plaintext highlighter-rouge">reduction_factor</code>. This factor represents the ratio between the hidden layer size \(d\) and the bottleneck size \(b\), given by:</p> \[b = \frac{d}{\text{reduction\_factor}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertModel</span><span class="p">,</span> <span class="n">BertTokenizer</span>

<span class="k">class</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        A bottleneck adapter module that can be inserted into a transformer.
        
        It projects hidden states down to a lower-dimensional space and then 
        back up again, with non-linearity and dropout in between. This helps 
        the model adapt to new tasks without updating the original transformer.
        
        Args:
            hidden_size: The dimension of the model</span><span class="sh">'</span><span class="s">s hidden states (e.g., 768 for BERT-base)
            adapter_size: The smaller bottleneck dimension (e.g., 64)
            dropout_rate: Regularization to improve generalization
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">down_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>  <span class="c1"># d -&gt; b
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>  <span class="c1"># non-linearity
</span>        <span class="n">self</span><span class="p">.</span><span class="n">up_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>    <span class="c1"># b -&gt; d
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Initialize adapter weights — not learned from pretraining, so good init is important!
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="c1"># Store original input for residual connection
</span>        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># Apply adapter: down-project -&gt; non-linear -&gt; up-project -&gt; dropout
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_project</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_project</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add residual and normalize
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div> <p>But how do we integrate adapters into a pre-trained model?</p> <p>Let’s see how to modify a standard transformer layer to include our bottleneck adapter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdapterTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        A wrapper around an existing transformer layer that adds adapters after
        attention and after the feed-forward sublayers.

        Args:
            transformer_layer: One layer from a pre-trained transformer (e.g., BERTLayer)
            adapter_size: Bottleneck size for the adapters
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">transformer_layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span><span class="p">.</span><span class="n">all_head_size</span>  <span class="c1"># Model-specific
</span>
        <span class="c1"># Freeze all transformer weights (we don’t train them)
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="c1"># Add bottleneck adapters at two key places:
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_adapter</span> <span class="o">=</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn_adapter</span> <span class="o">=</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Standard attention (output of frozen pre-trained layer)
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Inject adapter after attention
</span>        <span class="n">adapted_attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_adapter</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>

        <span class="c1"># Apply frozen feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">adapted_attention</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">adapted_attention</span><span class="p">)</span>

        <span class="c1"># Inject second adapter after feed-forward
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn_adapter</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>All set, and we need to load pre-trained model and wrap it’s target layers with this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define adapter size
</span><span class="n">adapter_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Wrap all encoder layers with adapter-enabled versions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">)):</span>
    <span class="n">original_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nc">AdapterTransformerLayer</span><span class="p">(</span><span class="n">original_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
<span class="c1"># Check that only adapters will be trained
</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trainable parameters: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Now you can tokenize input and train like usual.
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are lightweight and powerful.</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></div> <p>Now, I’m gonna show how to use adapters in transformers library. It’s faster, easier, and production-tested.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pip install adapter-transformers
</span>
<span class="c1"># `BertAdapterModel` = a special version of BERT that allows adapter injection.
</span><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertAdapterModel</span>
<span class="kn">from</span> <span class="n">transformers.adapters</span> <span class="kn">import</span> <span class="n">AdapterConfig</span>

<span class="c1"># Load model and tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertAdapterModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define adapter configuration
</span><span class="n">config</span> <span class="o">=</span> <span class="n">AdapterConfig</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">pfeiffer</span><span class="sh">"</span><span class="p">,</span>                    <span class="c1"># Adapter type: "pfeiffer", "houlsby", etc.
</span>    <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>          <span class="c1"># Bottleneck size (768 / 16 = 48)
</span>    <span class="n">leave_out</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>            <span class="c1"># Skip layer 0 and 11 (i.e., don't inject there)
</span>    <span class="n">non_linearity</span><span class="o">=</span><span class="sh">"</span><span class="s">gelu</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add adapter with a custom name
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Activate + train this adapter
</span><span class="n">model</span><span class="p">.</span><span class="nf">train_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#Tokenize Input and Forward Pass
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are efficient!</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Last hidden state (batch_size, seq_len, hidden_dim)
</span><span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Add a Classification Head (for downstream tasks)
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_classification_head</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Switch to training mode
</span><span class="n">model</span><span class="p">.</span><span class="nf">train_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Forward pass for classification
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are awesome!</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span>

<span class="c1"># Training Only Adapter Parameters
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>

<span class="c1"># Save / Load Adapters Separately
</span>
<span class="c1"># Save adapter after training
</span><span class="n">model</span><span class="p">.</span><span class="nf">save_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">saved/my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Load later into another model
</span><span class="n">model</span><span class="p">.</span><span class="nf">load_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">saved/my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">load_as</span><span class="o">=</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">set_active_adapters</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="parallel-adapters">Parallel Adapters</h2> <p>While bottleneck adapters are inserted sequentially in the model’s architecture, <strong>parallel Adapters</strong> inject adapter modules in parallel with the main feed-forward layers in each Transformer block, instead of sequentially. This means that the output of the adapter is added to the output of the feed-forward network, not to its input.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/parallel-adapter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/parallel-adapter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/parallel-adapter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/parallel-adapter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Parallel adapter." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Parallel Adapter </div> <p>Let \(x\) be the input to the Transformer block. The original feed-forward output is \(\mathrm{FFN}(x)\), and the adapter path is:</p> \[\mathrm{Adapter}(x) = W_\text{up} \, \sigma(W_\text{down} \, x)\] <p>The final output becomes:</p> \[y = \mathrm{FFN}(x) + \mathrm{Adapter}(x)\] <p>This allows the adapter to independently learn task-specific modifications without disrupting the main path.</p> <p>The parallel design has a slight computational overhead but can better preserve the pre-trained representations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ParallelAdapter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        
        <span class="c1"># Initialize weights
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        
        <span class="c1"># Scale factor - can be trained or fixed
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_project</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_project</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Scale the adapter output and add to original
</span>        <span class="k">return</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">x</span>
</code></pre></div></div> <p>The integration into a transformer layer would be similar to the bottleneck adapter, but the adapter would be applied in parallel rather than sequentially.</p> <h2 id="low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</h2> <p><a href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><strong>LoRA (Low-Rank Adaptation)</strong></a> introduced by <a href="https://arxiv.org/abs/2106.09685">Hu et al. (2021)</a> replaces or augments weight matrices with low-rank decompositions. Instead of fine-tuning a full matrix \(W \in \mathbb{R}^{d \times d}\), LoRA learns two smaller matrices:</p> \[W' = W + A B \quad \text{with} \quad A \in \mathbb{R}^{d \times r}, \; B \in \mathbb{R}^{r \times d}\] <p>Where \(r \ll d\) (typically \(r = 8\) or \(4\)). This drastically reduces the number of trainable parameters. LoRA is usually applied to the attention projection layers (query/key/value/output).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/lora-adapter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/lora-adapter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/lora-adapter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/lora-adapter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LoRA Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> LoRA Adapter </div> <p>Intuitively, LoRA adds a “low-rank path” through which task-specific information can flow, while keeping the rest of the model fixed.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        LoRA implementation for linear layers.
        
        Args:
            in_features: Input dimension
            out_features: Output dimension
            rank: Rank of the low-rank decomposition
            alpha: Scaling factor for the LoRA contribution
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">rank</span>
        
        <span class="c1"># LoRA weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        
        <span class="c1"># Initialize weights
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_B</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># LoRA contribution: scaling * (x @ A) @ B
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">)</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span>
</code></pre></div></div> <p>Now, let’s apply LoRA to a pre-trained linear layer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">linear_layer</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Wraps a pre-trained linear layer with LoRA functionality.
        
        Args:
            linear_layer: The pre-trained nn.Linear module to adapt
            rank: Rank of the low-rank decomposition
            alpha: Scaling factor
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear_layer</span>
        
        <span class="c1"># Freeze original weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="c1"># Add LoRA components
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lora</span> <span class="o">=</span> <span class="nc">LoRALayer</span><span class="p">(</span>
            <span class="n">linear_layer</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> 
            <span class="n">linear_layer</span><span class="p">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Combine original output with LoRA contribution
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>The genius of LoRA is in its efficiency.</p> <p>If the original weight matrix has dimensions n×m, full fine-tuning would require updating n×m parameters. With LoRA, using a rank r, we only need to update r×(n+m) parameters. For large matrices where r « min(n,m), this represents a massive reduction in trainable parameters.</p> <h3 id="applying-lora-to-a-transformer">Applying LoRA to a Transformer</h3> <p>In practice, LoRA is typically applied to specific weight matrices within a transformer, most commonly the query and value projection matrices in attention layers. Here’s how to adapt a transformer’s attention mechanism with LoRA:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>

<span class="k">def</span> <span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">q_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">v_proj</span><span class="sh">"</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">
    Apply LoRA to specific modules in a transformer model.
    
    Args:
        model: A Hugging Face transformer model
        rank: Rank for LoRA decomposition
        alpha: Scaling factor
        target_modules: List of module names to apply LoRA to
    </span><span class="sh">"""</span>
    <span class="c1"># First, freeze all parameters
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="c1"># Then apply LoRA to target modules
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">any</span><span class="p">(</span><span class="n">target_name</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="n">target_modules</span><span class="p">):</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="c1"># Get the parent module
</span>                <span class="n">parent_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">child_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">parent_module</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_submodule</span><span class="p">(</span><span class="n">parent_name</span><span class="p">)</span>
                
                <span class="c1"># Replace with LoRA version
</span>                <span class="n">lora_layer</span> <span class="o">=</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
                <span class="nf">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">lora_layer</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">lora_model</span> <span class="o">=</span> <span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div> <h3 id="quantized-lora-qlora">Quantized LoRA (QLoRA)</h3> <p><a href="https://arxiv.org/abs/2305.14314">QLoRA</a>, takes LoRA’s efficiency to the next level by combining it with quantization techniques. The key insight is to keep the base model in a quantized format (typically 4-bit precision) while applying LoRA adapters in full precision.</p> <p>QLoRA has been a game-changer for democratizing LLM fine-tuning, enabling the adaptation of models with over 70 billion parameters on a single consumer GPU.</p> <h2 id="prefix-tuning-virtual-tokens-in-hidden-space">Prefix Tuning: Virtual Tokens in Hidden Space</h2> <p>Now let’s shift our focus to another family of PEFT methods that operate by introducing trainable tokens to the input sequence or hidden states: Prefix Tuning and Prompt Tuning.</p> <p>Prefix Tuning, introduced by <a href="https://arxiv.org/abs/2101.00190">Li and Liang (2021)</a>, prepends a small number of learned key-value vectors (“prefixes”) to the attention mechanism.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/prefix-tuning-480.webp 480w,/assets/img/posts/2025-05-11-adapters/prefix-tuning-800.webp 800w,/assets/img/posts/2025-05-11-adapters/prefix-tuning-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/prefix-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Prefix Tuning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Prefix Tuning </div> <p>Instead of modifying weights, it expands the input to attention as:</p> \[\text{Attention}(\text{prefix} + x)\] <p>This means the model sees the learned prefix as a pseudo-context for every input, influencing the attention output without changing the underlying Transformer parameters.</p> <p>Prefix tuning is powerful for generation tasks like summarization or translation where modifying the attention context is sufficient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrefixTuningModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">prefix_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of Prefix Tuning.
        
        Args:
            hidden_size: Model</span><span class="sh">'</span><span class="s">s hidden size
            prefix_length: Number of virtual tokens to add
            num_layers: Number of transformer layers
            num_heads: Number of attention heads
            head_dim: Dimension of each attention head
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prefix_length</span> <span class="o">=</span> <span class="n">prefix_length</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        
        <span class="c1"># Create a prefix for each layer for both key and value states
</span>        <span class="c1"># Shape: [num_layers, 2, prefix_length, num_heads, head_dim]
</span>        <span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">prefix_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Initialize with a small standard deviation
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key_value_states</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Prepend prefix to key and value states for a specific layer.
        
        Args:
            key_value_states: Tuple of (key, value) states from the model
            layer_idx: Current transformer layer index
        </span><span class="sh">"""</span>
        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">key_value_states</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get the prefix for the current layer
</span>        <span class="c1"># Shape: [2, prefix_length, num_heads, head_dim]
</span>        <span class="n">prefix</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
        
        <span class="c1"># Extract key and value prefixes
</span>        <span class="n">key_prefix</span> <span class="o">=</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_prefix</span> <span class="o">=</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Reshape to match model's key and value shapes
</span>        <span class="c1"># From: [batch_size, prefix_length, num_heads, head_dim]
</span>        <span class="c1"># To: [batch_size, num_heads, prefix_length, head_dim]
</span>        <span class="n">key_prefix</span> <span class="o">=</span> <span class="n">key_prefix</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value_prefix</span> <span class="o">=</span> <span class="n">value_prefix</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="c1"># Concatenate with original states
</span>        <span class="c1"># Original shape: [batch_size, num_heads, seq_length, head_dim]
</span>        <span class="n">new_key_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">key_prefix</span><span class="p">,</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">new_value_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">value_prefix</span><span class="p">,</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="nf">return </span><span class="p">(</span><span class="n">new_key_states</span><span class="p">,</span> <span class="n">new_value_states</span><span class="p">)</span>
</code></pre></div></div> <p>To integrate this with a transformer model, we need to modify each attention layer to incorporate the prefixes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrefixTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">prefix_module</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prefix_module</span> <span class="o">=</span> <span class="n">prefix_module</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        
        <span class="c1"># Freeze the original layer
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Extract the attention module (implementation depends on model architecture)
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span>
        
        <span class="c1"># Prepare key, query, value states as in the original attention
</span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        
        <span class="c1"># Reshape for multi-head attention
</span>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="n">head_size</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="n">num_attention_heads</span>
        
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Apply prefix
</span>        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">prefix_module</span><span class="p">((</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_idx</span><span class="p">)</span>
        
        <span class="c1"># Update attention mask for the additional prefix tokens
</span>        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prefix_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> 
                <span class="mi">1</span><span class="p">,</span> 
                <span class="mi">1</span><span class="p">,</span> 
                <span class="n">self</span><span class="p">.</span><span class="n">prefix_module</span><span class="p">.</span><span class="n">prefix_length</span><span class="p">,</span> 
                <span class="n">device</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prefix_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="c1"># Calculate attention scores and outputs
</span>        <span class="c1"># (Implementation depends on the specific attention mechanism)
</span>        <span class="c1"># ...
</span>        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>The above implementation is conceptual and would need to be adapted based on the specific transformer architecture you’re working with.</p> <h3 id="prompt-tuning">Prompt Tuning</h3> <p><a href="https://arxiv.org/abs/2104.08691">Prompt Tuning</a>, can be seen as a simplified version of Prefix Tuning. Rather than adding virtual tokens at every layer, Prompt Tuning only prepends trainable embeddings to the input sequence embeddings at the first layer.</p> <p>Here’s a straightforward implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PromptTuning</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">prompt_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of Prompt Tuning.
        
        Args:
            model: The pre-trained transformer model
            prompt_length: Number of virtual tokens to add
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_length</span> <span class="o">=</span> <span class="n">prompt_length</span>
        
        <span class="c1"># Freeze model parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Get embedding dimension from the model
</span>        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Create soft prompt embeddings
</span>        <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
        
        <span class="c1"># Initialize with embeddings of random tokens from the vocabulary
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">random_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">prompt_length</span><span class="p">,))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">[</span><span class="n">random_indices</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">attention_mask</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get input embeddings
</span>        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">()(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">"</span><span class="s">inputs_embeds</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Expand soft prompts for batch size and prepend to input embeddings
</span>        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Adjust attention mask for the added prompt tokens
</span>        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prompt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prompt_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Forward pass through the model without input_ids
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
</code></pre></div></div> <h2 id="bitfit-adapter">BitFit Adapter</h2> <p>BitFit, proposed by <a href="https://arxiv.org/abs/2106.10199">Zaken et al. (2021)</a>, takes a radically different approach from the methods we’ve discussed so far. Instead of adding new parameters, BitFit selectively trains only the bias terms in the pre-trained model, leaving all other parameters frozen.</p> <p>Despite its extreme parameter efficiency, BitFit has shown good performance across various tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_bitfit_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Apply BitFit to a transformer model by only training bias terms.
    
    Args:
        model: A PyTorch model
    </span><span class="sh">"""</span>
    <span class="c1"># First, freeze all parameters
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="c1"># Then unfreeze only bias parameters
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
    
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <p>The implementation is remarkably simple, yet BitFit can achieve competitive performance while training less than 0.1% of the original model parameters in many cases.</p> <h2 id="ia-infused-adapter-by-inhibiting-and-amplifying-inner-activations">IA³: Infused Adapter by Inhibiting and Amplifying Inner Activations</h2> <p><strong>IA³ (Input-Aware Activation Adjustment)</strong> by <a href="https://arxiv.org/abs/2205.05638">Liu et al. (2022)</a>, modifies the element-wise activation scale and bias <em>after</em> each linear transformation. For a layer with output \(x\), IA³ computes:</p> \[x' = \alpha \cdot x + \beta\] <p>Here, \(\alpha\) and \(\beta\) are trainable parameters. This is similar to fine-tuning just the scale and shift of activations and can be extremely efficient.</p> <p>IA³ is useful when slight shifts in activation distributions are enough to steer the model to the new task.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/ia3-480.webp 480w,/assets/img/posts/2025-05-11-adapters/ia3-800.webp 800w,/assets/img/posts/2025-05-11-adapters/ia3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/ia3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="IA3 Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> IA3 Adapter </div> <p>Let’s check how it looks like in the code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of IA³ scaling vectors.
        
        Args:
            hidden_size: Dimension to scale
            ia3_type: Where to apply IA³ (</span><span class="sh">'</span><span class="s">feed_forward</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">attention_output</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">attention_value</span><span class="sh">'</span><span class="s">)
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ia3_type</span> <span class="o">=</span> <span class="n">ia3_type</span>
        
        <span class="c1"># Create scaling vectors initialized to ones
</span>        <span class="k">if</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For the output of the feed-forward layer
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_output</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For scaling attention outputs
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For scaling value vectors in attention
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Apply scaling to input tensor.
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For attention values, we reshape for broadcasting across batch and seq dimensions
</span>            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For feed-forward and attention outputs
</span>            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span>
</code></pre></div></div> <p>Integrating IA³ with a transformer model requires injecting the scaling at specific points:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IA3TransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        
        <span class="c1"># Freeze original parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Add IA³ modules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_value_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention_output_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_output</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Extract components (implementation is model-specific)
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span>
        
        <span class="c1"># Compute query, key, value projections
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        
        <span class="c1"># Apply IA³ to value projections
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_value_ia3</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        
        <span class="c1"># Compute attention
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Apply IA³ to attention output
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_output_ia3</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Apply IA³ to feed-forward output
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward_ia3</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>IA³’s efficiency is remarkable: for a model with hidden size h, it adds only 3h parameters per layer, compared to the millions in the original layer.</p> <h2 id="compacter-kronecker-products-for-ultimate-efficiency">Compacter: Kronecker Products for Ultimate Efficiency</h2> <p><strong>Compacter</strong>, proposed by <a href="https://arxiv.org/abs/2106.04647">Mahabadi et al. (2021)</a>, builds on the adapter idea, but instead of learning full matrices for down/up projection, it composes them from Kronecker products of smaller matrices.</p> \[W = W_1 \otimes W_2\] <p>This gives an expressive yet parameter-efficient formulation. Compacter adapters can learn more complex transformations than simple low-rank matrices without adding much overhead.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/compacter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/compacter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/compacter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/compacter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Compacter Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Compacter Adapter </div> <p>Let’s implement Compacter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Parameterized Hypercomplex Multiplication using Kronecker products.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span> <span class="o">=</span> <span class="n">factorized_phm</span>
        
        <span class="c1"># Calculate dimensions for the factors
</span>        <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">in_features</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>
        
        <span class="c1"># Ensure dimensions are compatible with factorization
</span>        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">==</span> <span class="n">in_features</span><span class="p">,</span> \
            <span class="sh">"</span><span class="s">Input features must be a perfect square for factorization</span><span class="sh">"</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">==</span> <span class="n">out_features</span><span class="p">,</span> \
            <span class="sh">"</span><span class="s">Output features must be a perfect square for factorization</span><span class="sh">"</span>
        
        <span class="k">if</span> <span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># Factorized representation using shared factors
</span>            <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Full Kronecker factors
</span>            <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
        
        <span class="c1"># Initialize parameters
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">_init_parameters</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">_init_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Initialize the parameters with small random values.</span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">ones_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">kronecker_product</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Compute the Kronecker product of matrices A and B.
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s3</span><span class="p">,</span> <span class="n">s4</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">B</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Reshape for matrix multiplication
</span>        <span class="n">A_reshaped</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span> <span class="o">*</span> <span class="n">s2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">B_reshaped</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s3</span> <span class="o">*</span> <span class="n">s4</span><span class="p">)</span>
        
        <span class="c1"># Perform outer product
</span>        <span class="n">kron_prod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">A_reshaped</span><span class="p">,</span> <span class="n">B_reshaped</span><span class="p">)</span>
        
        <span class="c1"># Reshape to get the final Kronecker product
</span>        <span class="k">return</span> <span class="n">kron_prod</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">s3</span><span class="p">,</span> <span class="n">s4</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span> <span class="o">*</span> <span class="n">s3</span><span class="p">,</span> <span class="n">s2</span> <span class="o">*</span> <span class="n">s4</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass using PHM.
        x: Input tensor of shape [batch_size, in_features]
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Compute the weight matrix using PHM
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># Using factorized representation
</span>            <span class="n">weight</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rank</span><span class="p">):</span>
                <span class="c1"># Apply scaling factor
</span>                <span class="n">kronecker_factor</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">kronecker_product</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">weight</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">*</span> <span class="n">kronecker_factor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Using full representation
</span>            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rank</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Apply the weight matrix - handle factorized vs full differently
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># For factorized version, we already have batch-specific weights
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">weight</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For the full version, we use simple matrix multiply
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">weight</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compacter adapter implementation using PHM for weight parameterization.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adapter_size</span> <span class="o">=</span> <span class="n">adapter_size</span>
        
        <span class="c1"># Down projection using PHM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="n">factorized_phm</span><span class="p">)</span>
        
        <span class="c1"># Activation function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        
        <span class="c1"># Up projection using PHM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="n">factorized_phm</span><span class="p">)</span>
        
        <span class="c1"># Additional components
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Scaling factor for the adapter output
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass through the Compacter adapter.
        </span><span class="sh">"""</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        
        <span class="c1"># Down projection with PHM
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Up projection with PHM
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Apply scaling and add residual
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># Integrating Compacter with a transformer layer would be similar to the adapter implementation
</span><span class="k">class</span> <span class="nc">CompacterTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">transformer_layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span><span class="p">.</span><span class="n">all_head_size</span>  <span class="c1"># Model specific
</span>        
        <span class="c1"># Freeze original parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="c1"># Add Compacter adapters
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_adapter</span> <span class="o">=</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn_adapter</span> <span class="o">=</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Original attention mechanism
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Apply attention adapter
</span>        <span class="n">adapted_attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_adapter</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Original feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">adapted_attention</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">adapted_attention</span><span class="p">)</span>
        
        <span class="c1"># Apply ffn adapter
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn_adapter</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="machine-learning,"/><category term="transformers,"/><category term="peft,"/><category term="lora,"/><category term="adapters,"/><category term="fine-tuning"/><summary type="html"><![CDATA[Exploration of modern parameter-efficient fine-tuning techniques with PyTorch implementations and practical insights.]]></summary></entry><entry><title type="html">Physical Symbol Systems and the Language of Thought</title><link href="https://xmarva.github.io/blog/2025/minds-as-computers/" rel="alternate" type="text/html" title="Physical Symbol Systems and the Language of Thought"/><published>2025-05-01T15:00:00+00:00</published><updated>2025-05-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/minds-as-computers</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/minds-as-computers/"><![CDATA[<h2 id="the-physical-symbol-system-hypothesis">The Physical Symbol System Hypothesis</h2> <p>Imagine your brain as a sophisticated computer processing symbols. Not metaphorically, but literally—manipulating physical structures that represent the world around you. This is the essence of the <em>physical symbol system hypothesis</em>, an influential theory proposed by cognitive scientists Allen Newell and Herbert Simon as a fundamental framework for understanding intelligence.</p> <p>Much like how biologists rely on the cell doctrine or geologists on plate tectonics, cognitive scientists have used this hypothesis as their north star. It’s a starting point that frames how we think about thinking itself. Newell and Simon’s hypothesis states that “a physical symbol system has the necessary and sufficient means for general intelligent action.” In other words, if you want intelligence, you need a system that can process symbols—and if you have a system that can process symbols properly, you’ll get intelligence.</p> <p>But what exactly constitutes a physical symbol system? According to Newell and Simon, such a system needs:</p> <ol> <li>Symbols that can be physically instantiated</li> <li>Symbol structures composed of these basic symbols</li> <li>Processes for manipulating these symbols and structures</li> <li>The ability to interpret and produce new symbols</li> </ol> <p>Consider a simple example: the problem of getting foxes, chickens, and grain across a river in a boat that can only carry one item at a time (without leaving foxes alone with chickens or chickens alone with grain). To solve this puzzle, you need to mentally represent the objects and their relationships, consider possible moves, and evaluate potential outcomes. According to the physical symbol system hypothesis, your brain accomplishes this by physically manipulating symbols that represent foxes, chickens, boats, and so on.</p> <p>At its core, this hypothesis is about taking information, encoding it into symbols, and transforming those symbols according to specific rules. That spreadsheet calculation you just ran? That’s a simple example of symbol manipulation. Your morning deliberation about whether to have coffee or tea? According to this hypothesis, that too is symbol manipulation—just happening in the wetware of your brain rather than silicon.</p> <h2 id="the-language-of-thought-hypothesis">The Language of Thought Hypothesis</h2> <p>Philosopher and cognitive scientist Jerry Fodor took the physical symbol system hypothesis further with his <em>language of thought hypothesis</em>. His proposal is both elegant and radical: we think in sentences—not English or Mandarin sentences, but sentences in a special mental language he sometimes called “Mentalese.”</p> <p>What makes this mental language special? Unlike natural languages with their ambiguities and inconsistencies, the language of thought is supposed to be precise and logical—more like the formal languages used in mathematics and logic than the messy languages we speak. This language doesn’t need to be learned; Fodor argues it’s innate, built into our cognitive architecture.</p> <p>Fodor’s argument begins with an observation about how we explain human behavior. We routinely explain and predict what people do by attributing beliefs and desires to them. When I say you rushed into the water because you believed someone was drowning and wanted to save them, I’m describing internal mental states that caused your behavior. Fodor claims this “belief-desire psychology” works so well because it’s actually true—we really do have beliefs and desires that cause our actions.</p> <p>But this raises a puzzle: how can mental states cause physical actions? How can the <em>content</em> or meaning of your beliefs affect anything physical? If I believe “the door is open” and desire “to close the door,” how does the meaning of these thoughts actually move my muscles?</p> <p>This is what philosophers call the problem of “causation by content,” and it’s at the heart of Fodor’s language of thought hypothesis.</p> <h2 id="syntax-semantics-and-the-computer-model">Syntax, Semantics, and the Computer Model</h2> <p>Fodor’s solution draws on an analogy with computers. When you type “2+3” into a calculator, the machine doesn’t understand addition or numbers. It simply follows rules for manipulating symbols. Yet somehow, it consistently produces “5” as an output.</p> <p>This works because computers are designed to manipulate symbols based on their “formal properties” (their shape or syntax) in ways that respect their “semantic properties” (their meaning). The computer doesn’t know what “2” means, but it’s programmed to manipulate the symbol “2” according to rules that consistently produce the right results.</p> <p>Fodor argues that our brains work in a similar way. Sentences in the language of thought are physical symbol structures that can be viewed either syntactically (in terms of their physical form) or semantically (in terms of what they represent). The brain processes these structures according to their syntax, while remaining blind to their semantics. Yet because the system is properly designed, these syntactic manipulations respect semantic relationships.</p> <p>Consider a more detailed example:</p> <ol> <li>We start with two basic symbols in our language of thought: “Ga” (meaning “Georgina is tall”) and “Fa” (meaning “Georgina has red hair”).</li> <li>Our brain contains rules for transforming these symbols. One rule might be: if you have two symbols “S” and “T,” you can form a new symbol “(S &amp; T).”</li> <li>Applying this rule to our initial symbols gives us “(Ga &amp; Fa)” (meaning “Georgina is tall and has red hair”).</li> <li>Another rule might be: if you have a symbol containing a name, you can replace the name with a variable “x” and add “∃x” (meaning “there exists an x such that…”) at the beginning.</li> <li>Applying this to “(Ga &amp; Fa)” gives us “∃x (Gx &amp; Fx)” (meaning “There exists someone who is tall and has red hair”).</li> </ol> <p>According to Fodor, your brain processes these as physical symbol structures, applying transformation rules without directly “seeing” their meaning. Yet the transformations reliably preserve truth, allowing you to draw valid conclusions about the world. This is possible because the language of thought is a formal system where syntax tracks semantics, much like in formal logic.</p> <p>Fodor likens this to how in formal logic, syntactic deducibility (having a formal proof) corresponds to semantic entailment (truth preservation). His key insight is that this correspondence allows purely physical systems to implement what appears to be reasoning about meanings.</p> <h2 id="the-chinese-room-argument">The Chinese Room Argument</h2> <p>But can symbol manipulation alone really produce genuine understanding and intelligence? Philosopher John Searle famously challenged this idea with his “Chinese Room” thought experiment.</p> <p>Imagine yourself locked in a room with nothing but an enormous instruction manual written in English. Through one window, you receive papers with Chinese symbols. You consult your manual, which tells you which Chinese symbols to send back through another window based solely on the shapes of the symbols you received. Despite knowing no Chinese whatsoever, you could theoretically provide appropriate responses to any Chinese question.</p> <p>To outside observers, the room appears to understand Chinese perfectly—it passes what Alan Turing proposed as the “Turing Test” for machine intelligence. If you ask a question in Chinese, you get an appropriate answer in Chinese. Yet you, inside the room, understand nothing about the meaning of these symbols. You’re just following rules for matching patterns.</p> <p>Searle’s argument cuts to the heart of computational theories of mind: if you don’t understand Chinese despite implementing the program, how could a computer understand anything by implementing essentially the same program? The Chinese Room seems to satisfy the conditions of the physical symbol system hypothesis—it manipulates symbols according to rules—yet it lacks understanding. Therefore, Searle concludes, the physical symbol system hypothesis must be wrong.</p> <p>Searle’s challenge is particularly pointed because it grants the physical symbol system hypothesis everything it seems to want. The Chinese Room manipulates symbols perfectly—it gives all the right outputs for the inputs it receives. It’s just that this manipulation doesn’t seem to produce understanding. The person in the room is just “pushing symbols around” without comprehending what they mean.</p> <h2 id="responses-to-the-chinese-room">Responses to the Chinese Room</h2> <p>Defenders of computational approaches have offered various responses to Searle’s challenge:</p> <p>The <strong>systems reply</strong> argues that while you as the person in the room don’t understand Chinese, the system as a whole (you plus the instruction manual plus the room) does understand. Understanding emerges at the system level, not the component level. This is analogous to how neurons individually don’t understand language, yet the brain as a whole does. The individual components of a system need not possess the properties of the system itself.</p> <p>Searle counters this by suggesting we internalize the entire system—imagine memorizing the entire instruction manual so that it’s all in your head. You’re now the entire system, yet you still don’t understand Chinese. You’re just following memorized rules for symbol manipulation.</p> <p>The <strong>robot reply</strong> suggests that understanding requires embodiment and causal connections to the world. A Chinese Room embedded in a robot that can interact with the world—seeing Chinese characters, handling Chinese objects, speaking with Chinese speakers—would genuinely understand Chinese through these grounded interactions.</p> <p>Searle remains unconvinced. He argues that adding sensors and motors doesn’t bridge the fundamental gap between syntax and semantics. A robot could be programmed to stop when it “sees” the Chinese character for “stop,” but this wouldn’t mean it understands what “stop” means any more than a trained pigeon understands a stop sign.</p> <p>Some philosophers and cognitive scientists have proposed other responses. The <strong>biological reply</strong> suggests that perhaps understanding requires specific biological processes that non-biological systems cannot replicate. The <strong>learning reply</strong> argues that a system that developed its symbol-processing capacities through learning (rather than being hand-programmed) might genuinely understand.</p> <h2 id="the-symbol-grounding-problem">The Symbol-Grounding Problem</h2> <p>This debate leads us to what cognitive scientists call the <em>symbol-grounding problem</em>: How do symbols become meaningful in the first place? How does the symbol “cat” connect to actual cats in the world?</p> <p>In a computer program, symbols get their meaning from the programmers who create them. The binary code “01100011 01100001 01110100” represents “cat” because programmers decided it should. But if our minds are symbol systems, where do our symbols get their meaning? There’s no programmer assigning meanings to our mental symbols.</p> <p>One approach, associated with philosophers like Fred Dretske and Ruth Millikan, suggests that meaning comes from causal connections between symbols and the world. Your mental symbol for “cat” means cat because it’s reliably caused by encounters with cats. Another approach, championed by philosopher Wilfrid Sellars, argues that symbols get their meaning from their role in a network of inferential relationships with other symbols.</p> <p>The symbol-grounding problem isn’t unique to artificial systems—it’s equally mysterious how our own thoughts connect to reality. When you think about cats, what exactly makes that thought <em>about</em> cats rather than dogs or airplanes? This problem of “intentionality” or “aboutness” has puzzled philosophers for centuries.</p> <h2 id="implications-for-artificial-intelligence">Implications for Artificial Intelligence</h2> <p>The debate between Searle and proponents of the physical symbol system hypothesis has profound implications for artificial intelligence. If Searle is right, then no amount of symbol manipulation, no matter how sophisticated, will ever produce genuine understanding or consciousness. AI systems might simulate intelligence, but they would be what Searle calls “weak AI”—useful tools that mimic intelligence without possessing it.</p> <p>On the other hand, if the physical symbol system hypothesis is correct, then there is no in-principle barrier to creating “strong AI”—artificial systems that genuinely understand and are conscious in the same way humans are. The challenge would be merely technical: figuring out the right symbols and rules for manipulating them.</p> <p>Recent advances in AI, particularly in deep learning and neural networks, have complicated this picture. Modern AI systems like large language models don’t explicitly manipulate symbols according to explicit rules. Instead, they learn statistical patterns from vast amounts of data. Some researchers argue this approach might sidestep Searle’s objections by grounding symbols in patterns of use rather than explicit rules.</p> <h2 id="beyond-symbol-systems">Beyond Symbol Systems?</h2> <p>The physical symbol system hypothesis and the language of thought hypothesis remain influential in cognitive science, but they’re not the only approaches. Alternative frameworks include:</p> <p><strong>Connectionism</strong>: This approach models cognition as emerging from networks of simple processing units (artificial neurons) rather than explicit symbol manipulation. Connectionists argue that intelligence emerges from the patterns of activation across these networks, not from rule-based symbol processing.</p> <p><strong>Embodied Cognition</strong>: This view holds that cognition is fundamentally shaped by the body’s interactions with the environment. Rather than abstract symbol manipulation happening in a disembodied mind, thinking is grounded in sensorimotor experiences.</p> <p><strong>Predictive Processing</strong>: This newer framework suggests the brain is fundamentally a prediction machine, constantly generating and updating predictions about sensory inputs. Intelligence emerges from the process of minimizing prediction errors rather than manipulating symbols.</p> <p>These alternative approaches don’t necessarily contradict the insights of the physical symbol system hypothesis entirely. They might be seen as offering different levels of explanation or as complementary perspectives on the complex phenomenon of intelligence.</p> <p>The debate continues, and with it, our understanding of what it means to think, to understand, and to be intelligent grows deeper and more nuanced. Perhaps the most fascinating aspect of cognitive science is that in studying minds, we are studying ourselves—turning the very tools we’re investigating back upon the investigators. Whether computational approaches can solve the fundamental challenges posed by Searle and others remains an open question at the frontier of cognitive science and philosophy of mind.</p>]]></content><author><name></name></author><category term="featured-posts"/><category term="cognitive-science,"/><category term="philosophy-of-mind,"/><category term="symbolic-systems,"/><category term="language-of-thought"/><summary type="html"><![CDATA[Exploration of the physical hypothesis of symbolic systems, the language of thought hypothesis, and philosophical challenges]]></summary></entry><entry><title type="html">Building a Transformer (Cross-Attention and MHA Explained)</title><link href="https://xmarva.github.io/blog/2025/building-a-transformer/" rel="alternate" type="text/html" title="Building a Transformer (Cross-Attention and MHA Explained)"/><published>2025-04-16T15:00:00+00:00</published><updated>2025-04-16T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/building-a-transformer</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/building-a-transformer/"><![CDATA[<p><a href="https://www.kaggle.com/code/qmarva/implementing-transformer-en"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=plastic&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"/></a> <a href="https://colab.research.google.com/drive/1m34XYFZZTt-jbHo2OXlUfxR33zvFbXJZ?usp=sharing"><img src="https://img.shields.io/badge/Colab-F9AB00?style=plastic&amp;logo=google-colab&amp;logoColor=white" alt="Colab"/></a></p> <h2 id="building-a-transformer">Building a Transformer</h2> <p>The Transformer architecture marked a revolutionary step in sequence processing. Unlike traditional models such as RNNs and LSTMs, which handle data sequentially, the Transformer uses an attention mechanism that enables parallel processing of the entire sequence. This significantly accelerates training and improves performance.</p> <p>The key innovation of the Transformer is the use of <strong>self-attention</strong>, which allows the model to effectively take into account the context of each word or token, regardless of its position in the sequence. This architecture has become the foundation of many modern models, including BERT, GPT, and others, and has greatly improved the quality of solutions in the field of natural language processing.</p> <p>We’ll need to install several libraries (and import even more), and the number of dependencies will only grow as we move forward. You can ignore these setup cells and just run them — the main focus should be on the core code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">torchdata</span><span class="o">==</span><span class="mf">0.3</span><span class="p">.</span><span class="mi">0</span> <span class="n">torchtext</span><span class="o">==</span><span class="mf">0.12</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2</span> <span class="n">altair</span><span class="o">==</span><span class="mf">5.5</span><span class="p">.</span><span class="mi">0</span> <span class="n">GPUtil</span><span class="o">==</span><span class="mf">1.4</span><span class="p">.</span><span class="mi">0</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">de_core_news_sm</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_sm</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">GPUtil</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">import</span> <span class="n">altair</span> <span class="k">as</span> <span class="n">alt</span>
<span class="kn">from</span> <span class="n">altair</span> <span class="kn">import</span> <span class="n">Chart</span>

<span class="n">alt</span><span class="p">.</span><span class="n">data_transformers</span><span class="p">.</span><span class="nf">disable_max_rows</span><span class="p">()</span>
</code></pre></div></div> <h2 id="positional-encoding">Positional Encoding</h2> <p>Transformer models work with numbers. To process text, it must be converted into a numerical format that the model can understand and work with. The first step is to convert text into tokens — for more details, see the notebook on tokenization. Tokens represented as vectors are called <strong>embeddings</strong>.</p> <p><strong>Embeddings</strong> are numerical vectors that capture the semantic meaning of words or subwords.</p> <p>Before the Transformer architecture, sequence-processing models like RNNs and LSTMs handled data sequentially, inherently preserving the order of elements. However, their computational inefficiency due to step-by-step processing and poor parallelization led researchers to seek alternatives.</p> <p>The Transformer overcame these limitations with a fully parallel approach. However, without positional information, the model wouldn’t be able to distinguish between sentences with the same words in different orders. In Russian, word relationships are often expressed via case endings, but in English and many other languages, word order is crucial.</p> <p><strong>Positional Encoding</strong> is the mechanism in the Transformer architecture that enables the model to account for the order of words in a sequence.</p> <p>Positional Encoding adds special signals (positional encodings) to the token embeddings based on their position in the sequence. These encodings have the same dimensionality as the embeddings so that they can be summed together. This additional information allows the model to distinguish, for instance, between the word “cat” at position 1 and “cat” at position 5, even if their semantic embeddings are identical. The encoding uses a formula that combines sine and cosine functions at different frequencies:</p> \[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)\] <p>where \(pos\) is the position, \(d_{\text{model}}\) is the embedding dimensionality, and \(i\) is the index of the vector dimension.</p> <p>The core idea is that these sinusoidal functions allow the model to pay attention to <strong>relative positions</strong>.</p> <h3 id="why-does-this-strange-formula-encode-relative-positions">Why does this strange formula encode relative positions?</h3> <h4 id="first-the-model-can-generalize-to-sequences-longer-than-those-seen-during-training">First, the model can generalize to sequences longer than those seen during training.</h4> <p>Imagine that each position in a sequence is a point on a number line. If we generate signals for position \(pos\) using sine and cosine, then the signals for position \(pos + k\) can be expressed as a combination of the original values. For example, using the angle addition formula:</p> \[\sin(pos + k) = \sin(pos)\cos(k) + \cos(pos)\sin(k)\] <p>A shift of \(k\) positions can be expressed as a weighted sum of the original sine and cosine values. This allows the model to infer that a word “three positions later” is related to the original word, even if it never saw such a long sequence during training.</p> <h4 id="second-the-distance-between-any-two-time-steps-is-consistent-across-the-sequence">Second, the distance between any two time steps is consistent across the sequence.</h4> <p>The logarithmic decay of frequencies in the term \(10000^{2i/d_{\text{model}}}\) ensures that different dimensions of the positional vector capture different levels of positional detail. For small \(i\) (early vector components), the denominator becomes large, causing the sine and cosine arguments to grow slowly with \(pos\). This creates low-frequency oscillations that help distinguish between distant positions — for example, the beginning of the text (positions 1–100) versus the middle (positions 101–200). For larger \(i\), the denominator shrinks, the argument grows faster, and high-frequency oscillations emerge, encoding fine-grained differences between neighboring positions (e.g., 101 and 102).</p> <h4 id="third-this-formula-yields-unique-encodings-for-each-position">Third, this formula yields unique encodings for each position.</h4> <p>Alternating sine and cosine for even and odd indices solves the uniqueness issue. If we used only sine, different positions might accidentally match due to the periodicity of the function (e.g., \(\sin(pos)\) and \(\sin(pos + 2\pi)\)). Adding cosine for neighboring vector components eliminates this symmetry: the combination of \(\sin(f(pos))\) and \(\cos(f(pos))\) across different frequencies \(f\) ensures that each position \(pos\) has a unique vector. The orthogonality of sine and cosine (their dot product is close to zero) minimizes overlap with word embeddings, allowing the model to separately process semantics and position.</p> <hr/> <p>The sum \(\text{Embedding} + PE\) is possible because word embeddings and positional encodings have the same dimensionality \(d_{\text{model}}\). This addition requires no trainable parameters: the model receives a combined signal where the word’s semantics are modulated by its position. Gradients flow through this operation without distortion, as the derivative of a sum is the sum of the derivatives. As a result, during training, the model automatically learns to adjust both the semantic embeddings and the use of positional information (via attention), without conflicting signals.</p> <p>While it’s also possible to use <strong>learned positional embeddings</strong>, the sinusoidal version was chosen in the original paper because it enables the model to extrapolate to sequence lengths not seen during training. Experiments have shown that both versions yield nearly identical results.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span>
                             <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h2 id="attention-self-attention-multi-head-attention">Attention, Self-Attention, Multi-Head Attention</h2> <h3 id="attention">Attention</h3> <p><strong>Attention</strong> is a mechanism that allows a model to weigh the importance of different elements in the input sequence.</p> <p>It can be described as a function that takes a <strong>query</strong> and a set of <strong>key-value</strong> pairs, and produces an output — a weighted sum of the values. The weight assigned to each value is computed based on a compatibility function between the query and the corresponding key.</p> <p>Imagine you’re at a table with 50 experts. At the start, none of them knows anything about themselves or each other, but their goal during the meeting is to figure out:</p> <ul> <li>\(V\): what they themselves know (their <strong>Value</strong> — knowledge/opinion),</li> <li>\(K\): the best way to describe what they’re good at (their <strong>Key</strong>),</li> <li>\(Q\): the best way to express what information they’re looking for (their <strong>Query</strong>).</li> </ul> <p>If we used only \(Q\) and \(K\), the model wouldn’t be able to transform the discovered dependencies into new features. The matrix \(V\) adds flexibility, allowing the model to reweight values according to context.</p> <p>Let’s say you’re one of those experts. You have a question (query), such as:</p> <blockquote> <p>“I need an opinion on Japanese cars.”</p> </blockquote> <p>You look around. Each expert has published a short description (key), for example:</p> <ul> <li>“I’m a mechanic specializing in Japanese cars”</li> <li>“I’m a chef who knows Italian cuisine”</li> <li>“I’m a driver who owned a Subaru”</li> </ul> <p>You compare your query against the keys of the others. If someone’s key matches well, you pay more attention to their value (opinion). You’ll likely give the most weight to the mechanic and less to the driver.</p> <p>As training progresses, you refine your query. Maybe next time, you realize you’re not interested in Japanese cars, but in <strong>Italian sewing machines</strong>. And it turns out the chef, initially thinking they specialize in Italian food, actually knows sewing machines well.</p> <p>So, you update the attention weights accordingly and learn to listen to the right expert.</p> <hr/> <h3 id="self-attention"><strong>Self-Attention</strong></h3> <p><strong>Self-attention</strong> is a type of attention mechanism used in Transformers where the queries, keys, and values come from the same sequence. The original Transformer uses <strong>Scaled Dot-Product Attention</strong>, which works as follows:</p> <ol> <li><strong>Create query, key, and value vectors</strong>. For each input vector \(x\) (e.g., a word embedding), three vectors are computed:</li> </ol> \[Q = xW_q,\quad K = xW_k,\quad V = xW_v\] <p>Here, \(W_q\), \(W_k\), and \(W_v\) are trainable weight matrices. The dimensions of \(Q\) and \(K\) must match: \(d_k\).</p> <ol> <li><strong>Compute scores.</strong> For each query vector \(Q_i\) (corresponding to position \(i\)), scores are computed with all keys \(K_j\) using the dot product:</li> </ol> \[\text{score}(i, j) = Q_i \cdot K_j^T\] <ol> <li><strong>Scale the scores.</strong> To prevent large dot product values with high dimensions, the scores are divided by \(\sqrt{d_k}\):</li> </ol> \[\text{scaled\_score}(i, j) = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}\] <ol> <li><strong>Apply Softmax.</strong> Each row of scores is passed through Softmax for normalization:</li> </ol> \[\alpha_{ij} = \text{softmax}\left( \frac{Q_i \cdot K_j^T}{\sqrt{d_k}} \right)\] <p>The resulting \(\alpha_{ij}\) are the attention weights.</p> <ol> <li><strong>Compute the weighted sum of values.</strong> Each value vector \(V_j\) is multiplied by the attention weight \(\alpha_{ij}\) and aggregated:</li> </ol> \[\text{Attention}(Q_i, K, V) = \sum_j \alpha_{ij} V_j\] <ol> <li><strong>Form the output vector.</strong> The result is a vector containing contextual information relevant to position \(i\). For the entire sequence:</li> </ol> \[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V\] <p>In practice, all computations are done in parallel using matrix operations, making the mechanism efficient and scalable.</p> <hr/> <h3 id="multi-head-attention"><strong>Multi-Head Attention</strong></h3> <p><strong>Multi-Head Attention</strong> is an extension of self-attention. While single-head attention focuses on one type of dependency (e.g., syntax or semantics), multi-head attention enables the model to capture multiple aspects of context simultaneously: grammatical relationships, anaphora, semantic parallels, etc.</p> <p>Let the input be an embedding matrix \(X \in \mathbb{R}^{n \times d_{\text{model}}}\), where \(n\) is the sequence length and \(d_{\text{model}}\) is the embedding dimension.</p> <p>The idea remains the same: for each attention head \(h \in {1, \dots, H}\), the input \(X\) is projected into queries, keys, and values via trainable matrices:</p> \[Q_h = X W_h^Q,\quad K_h = X W_h^K,\quad V_h = X W_h^V\] <p>Typically, \(d_k = d_v = \frac{d_{\text{model}}}{H}\) so that concatenating all heads results in the original dimension \(d_{\text{model}}\).</p> <p>Each head performs standard attention:</p> \[\text{Attention}_h(Q_h, K_h, V_h) = \text{softmax}\left( \frac{Q_h K_h^T}{\sqrt{d_k}} \right) V_h\] <p>The outputs from all \(H\) heads are concatenated along the last dimension:</p> \[\text{Concat}( \text{head}_1, \dots, \text{head}_H ) \in \mathbb{R}^{n \times (H \cdot d_v)}\] <p>This combined output is projected back into \(d_{\text{model}}\) using a final linear layer:</p> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_H) W^O\] <p>where \(W^O \in \mathbb{R}^{(H \cdot d_v) \times d_{\text{model}}}\) is a trainable weight matrix of the final output projection layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">d_model</span><span class="p">,</span> <span class="sh">"</span><span class="s">d_model must be divisible by num_heads</span><span class="sh">"</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Q: [batch_size, num_heads, seq_len, head_dim]
</span>        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_k</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_v</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="feedforward">FeedForward</h2> <p>Imagine that after passing through the Multi-Head Attention mechanism, the information for each word or token has become richer and more contextualized. Attention has blended information from different tokens to better understand each one in the context of the sentence. But now, this enriched information needs to be <strong>processed and refined individually</strong> for each token.</p> <p>That’s the role of the <strong>FeedForward Network (FFN)</strong>, which comes after the attention layer in each encoder and decoder block.</p> <p>An FFN consists of two linear transformations. Between these two linear layers, there’s a non-linear activation function — usually <strong>ReLU</strong>. Simply put, it’s a small two-layer neural network.</p> <p>One of the key features of the FFN in the Transformer is that it is <strong>applied position-wise</strong>. This means the <em>same</em> feedforward network is applied <strong>independently</strong> to the representation of <strong>each token</strong> in the sequence.</p> <p>The dimension of the inner layer in the FFN is typically <strong>larger</strong> than the model dimension (\(d_{\text{model}}\)). In the original <em>“Attention Is All You Need”</em> paper, this inner dimension (\(d_{\text{ff}}\)) was <strong>four times larger</strong> than \(d_{\text{model}}\) — that is, 2048 vs. 512 for the base model. However, other ratios may be used, such as doubling the size.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="encoderlayer">EncoderLayer</h2> <p>Most competitive sequence transformation models follow an <strong>encoder-decoder</strong> structure.</p> <p>The <strong>encoder</strong> receives the input and builds its representation (i.e., its features).</p> <p>The encoder is composed of a stack of <strong>identical layers</strong>. The original paper uses a stack of <strong>6 such layers</strong>, though the number can vary. Each encoder layer consists of <strong>two sub-layers</strong>:</p> <ol> <li>A <strong>Multi-Head Self-Attention</strong> mechanism</li> <li>A <strong>Feed-Forward Network (FFN)</strong> — which we discussed earlier</li> </ol> <p>Each of these sub-layers is wrapped in a <strong>residual connection</strong>, followed by a <strong>layer normalization</strong> step.</p> <p>Residual connections help ensure smooth gradient flow when training very deep models and preserve information from the original input sequence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Self attention
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="decoderlayer">DecoderLayer</h2> <p>The <strong>decoder</strong> uses the encoder’s embeddings along with other inputs to generate the <strong>target sequence</strong>.</p> <p>Like the encoder, the decoder is composed of a <strong>stack of identical layers</strong>, typically matching the encoder in depth.</p> <p>In addition to the two sub-layers found in the encoder (Multi-Head Self-Attention and Feed-Forward Network), each decoder layer includes a <strong>third sub-layer</strong>: <strong>Encoder-Decoder Attention</strong>. This allows the decoder to focus on relevant parts of the input sequence — that is, the encoder’s output.</p> <p>The self-attention sub-layer in the decoder is <strong>modified</strong> to prevent attending to <strong>future positions</strong>. This is implemented by <strong>masking</strong> — setting the scores corresponding to illegal connections in the Softmax input to \(-\infty\). This ensures that predictions for position \(i\) depend only on known outputs at positions less than \(i\).</p> <p>As in the encoder, <strong>residual connections</strong> and <strong>layer normalization</strong> are applied around each sub-layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="c1"># Self attention (маскированное)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Cross attention (с выходом энкодера)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="transformer">Transformer</h2> <p>Once all the components of the Transformer — the encoder, decoder, attention mechanisms, and positional encodings — are implemented, the final step is to combine them into a single model that can be trained on sequence pairs (e.g., source text and its translation).</p> <p>The <strong>encoder</strong> and <strong>decoder</strong> are each constructed as a <strong>stack of <code class="language-plaintext highlighter-rouge">num_layers</code> layers</strong>. Each <code class="language-plaintext highlighter-rouge">EncoderLayer</code> in the encoder sequentially refines the input representations: self-attention captures global dependencies, the feed-forward network introduces non-linearity, and residual connections with layer normalization ensure stability.</p> <p>Similarly, each <code class="language-plaintext highlighter-rouge">DecoderLayer</code> applies masked self-attention, cross-attention to the encoder output, and a feed-forward network. Repeating these layers multiple times allows the model to iteratively refine representations — as if it is “re-reading” the data at different levels of abstraction.</p> <p>The <strong>final output layer</strong> <code class="language-plaintext highlighter-rouge">fc_out</code> projects from the model dimension \(d_{\text{model}}\) to the size of the target language vocabulary. This projection interprets the decoder’s output vectors as <strong>logits</strong> — unnormalized scores for each token in the vocabulary:</p> \[\text{output} = W_{\text{out}} \cdot \text{dec\_output} + b_{\text{out}}\] <p>A Softmax (not explicitly shown in code, but implied in the loss function) is applied to these logits to produce a probability distribution over the vocabulary, from which the next word is selected.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">src_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">encoder_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">src_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">decoder_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">tgt_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="testing-transformer-just-run-it">Testing Transformer (just run it)</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_transformer</span><span class="p">():</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">src_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Generate synthetic data
</span>    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    
    <span class="c1"># Generate masks (example)
</span>    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># No masking
</span>    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Causal mask
</span>
    <span class="c1"># Initialize the model
</span>    <span class="n">transformer</span> <span class="o">=</span> <span class="nc">Transformer</span><span class="p">(</span>
        <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">src_vocab_size</span><span class="p">,</span>
        <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">tgt_vocab_size</span><span class="p">,</span>
        <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span>
    <span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">1. Positional Encoding Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Before PE: mean=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x_pe</span> <span class="o">=</span> <span class="nf">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">After PE: mean=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PE Shape: </span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should be [1, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s">])</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">2. Multi-Head Attention Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">mha</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">k</span> <span class="o">=</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="nf">mha</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Attention output shape: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Encoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">encoder_layer</span> <span class="o">=</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">enc_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">enc_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder output shape: </span><span class="si">{</span><span class="n">enc_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">enc_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Data changed: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span><span class="si">}</span><span class="s"> (should be False)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">4. Decoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">decoder_layer</span> <span class="o">=</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder output shape: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">dec_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Output norm: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="nf">norm</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">5. Full Transformer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input data:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">src: </span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tgt: </span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nf">transformer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Output shape check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Expected shape: (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">tgt_vocab_size</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Actual shape:   </span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Gradient check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">dummy_loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="n">dummy_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">has_gradients</span> <span class="o">=</span> <span class="nf">any</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Gradients computed: </span><span class="si">{</span><span class="n">has_gradients</span><span class="si">}</span><span class="s"> (should be True)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">6. Model Parameters Check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">encoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">decoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Test completed!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="machine-learning,"/><category term="transformers,"/><category term="multihead-attention,"/><category term="positional-encoding,"/><category term="nlp"/><summary type="html"><![CDATA[Learn and implement the most iconic architecture in modern deep learning.]]></summary></entry><entry><title type="html">Understanding Byte-Pair Encoding Algorithm</title><link href="https://xmarva.github.io/blog/2025/tokenization/" rel="alternate" type="text/html" title="Understanding Byte-Pair Encoding Algorithm"/><published>2025-04-01T15:00:00+00:00</published><updated>2025-04-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/tokenization</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/tokenization/"><![CDATA[<h2 id="tokenization">Tokenization</h2> <p><a href="https://www.kaggle.com/code/qmarva/1-bpe-tokenization-algorithm-eng?scriptVersionId=231677033"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=plastic&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"/></a> <a href="https://colab.research.google.com/drive/1lmfuMdC8v-lXL_MuyC0uBewdLLCTQzCO?usp=sharing"><img src="https://img.shields.io/badge/Colab-F9AB00?style=plastic&amp;logo=google-colab&amp;logoColor=white" alt="Colab"/></a></p> <p>Tokenization is a fundamental stage in natural language processing, the task of which is to split text into meaningful units (tokens).</p> <p>These units can be words, parts of words, or even characters. Historically, simple methods were used: splitting by spaces, regular expressions for extracting words and punctuation, manual rules for handling abbreviations. However, such approaches scaled poorly for languages with agglutinative morphology (e.g., Russian or Finnish) and complex word combinations.</p> <p>Traditional tokenization methods like space splitting or manual rules often prove ineffective in real-world scenarios: they struggle with typos, rare words, and multilingual texts. For example, words like “gooood” or mixed languages in a single sentence can break a classical tokenizer.</p> <p>In modern NLP, subword tokenization algorithms like <a href="https://arxiv.org/pdf/1508.07909">BPE (Byte Pair Encoding)</a> dominate, balancing the semantic integrity of tokens with efficient vocabulary usage. In this notebook, we will examine the BPE algorithm in detail and learn to work with tokenizers from the Hugging Face library.</p> <p>First, we will import all libraries and functions needed for this notebook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>
</code></pre></div></div> <hr/> <h2 id="loading-data">Loading Data</h2> <p>For demonstration, we will load the parallel English-Russian <a href="https://arxiv.org/abs/1812.10464">Tatoeba</a> corpus from Artetxe et al. (2019) via the <a href="http://huggingface.co/docs/datasets/loading">Hugging Face Datasets</a> library.</p> <p><a href="https://tatoeba.org/en/sentences/index">Tatoeba</a> is a free collection of translated example sentences for language learners, available in over 400 languages. Its name comes from the Japanese phrase «tatoeba» (例えば), meaning “for example.” It is written and maintained by a community of volunteers through open collaboration. Individual contributors are known as Tatoebans.</p> <p>We will use only the English and Russian subsets. All examples in this dataset are short everyday phrases: “Let’s try something.” → “Давайте что-нибудь попробуем!”.</p> <p>This format is convenient for training transformers, which work with sequences of limited length. In this notebook, we will not delve into transformer architecture but focus on text data preprocessing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_translation_dataset</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading Tatoeba en-ru...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Helsinki-NLP/tatoeba</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="sh">"</span><span class="s">en</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="sh">"</span><span class="s">ru</span><span class="sh">"</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error while loading dataset: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Dataset structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Data sample:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EN: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">RU: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
    
    <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <hr/> <h2 id="data-analysis">Data Analysis</h2> <p>Let’s take a quick look at the dataset to understand what we’re dealing with. We won’t dive deep into data analysis methods but will examine basic statistics.</p> <p>The <code class="language-plaintext highlighter-rouge">analyze_dataset</code> function shows that the average length of English sentences is 7.2 words, Russian — 6.2. The maximum lengths (30 and 28 words) indicate the presence of outliers that may require truncation.</p> <p>The histograms show right-skewed distributions: most sentences are shorter than 15 words. These observations influence model hyperparameter choices, e.g., <code class="language-plaintext highlighter-rouge">max_length=64</code> provides padding headroom even if actual sequences are shorter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>

    <span class="n">en_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">ru_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Analysis based on first </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s"> samples:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">English sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">English Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Russian Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="simple-tokenizer">Simple Tokenizer</h2> <p>Now we will write a <code class="language-plaintext highlighter-rouge">BaseTokenizer</code> class for text preprocessing, building a token vocabulary, and collecting token frequency statistics. This class will serve as the foundation for more complex tokenizers and provide a common structure for processing text data.</p> <p>We declare the class using the <a href="https://docs.python.org/3/library/dataclasses.html">@dataclass</a> decorator to auto-generate the constructor. Parameters we need: <code class="language-plaintext highlighter-rouge">language</code> (text language), <code class="language-plaintext highlighter-rouge">vocab_size</code> (max vocabulary size), <code class="language-plaintext highlighter-rouge">min_freq</code> (minimum frequency for including a token in the vocabulary), and <code class="language-plaintext highlighter-rouge">special_tokens</code> (list of special tokens).</p> <p>If <code class="language-plaintext highlighter-rouge">special_tokens</code> are not specified, default values are used: <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called immediately after object initialization. Here, we initialize the <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">id2token</code> dictionaries that map tokens to their numeric IDs. Special tokens must be added to the vocabulary first. For example, <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> gets ID 0, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> — 1, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">preprocess_text</code> method is used to preprocess text. We will convert text to lowercase and split it into tokens using a regular expression.</p> <p>The pattern <code class="language-plaintext highlighter-rouge">r"\w+[\w']*|['’][a-z]+|[^\w\s]"</code> captures:</p> <ul> <li>Words with apostrophes (e.g., <code class="language-plaintext highlighter-rouge">don't</code> → <code class="language-plaintext highlighter-rouge">["don't"]</code>).</li> <li>Contractions starting with an apostrophe (e.g., <code class="language-plaintext highlighter-rouge">'s</code> → <code class="language-plaintext highlighter-rouge">["'s"]</code>).</li> <li>Individual punctuation marks (e.g., <code class="language-plaintext highlighter-rouge">"!"</code> → <code class="language-plaintext highlighter-rouge">["!"]</code>).</li> </ul> <p>Note that the regex may not cover all edge cases (e.g., emojis or compound symbols), requiring modification for specific tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">get_stats</code> method collects token frequency statistics. For each text in the <code class="language-plaintext highlighter-rouge">examples</code> list, the <code class="language-plaintext highlighter-rouge">preprocess_text</code> function is called, then the <code class="language-plaintext highlighter-rouge">Counter</code> is updated.</p> <p>For example, the text <code class="language-plaintext highlighter-rouge">"Hello, world!"</code> returns a counter with keys <code class="language-plaintext highlighter-rouge">["hello", ",", "world", "!"]</code> and their frequencies. This method is used during tokenizer training to select tokens for the vocabulary based on <code class="language-plaintext highlighter-rouge">min_freq</code> and <code class="language-plaintext highlighter-rouge">vocab_size</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <p>Below, we consolidate all code into a single class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
</code></pre></div></div> <p>In reality, this basic approach has many drawbacks. For example, converting text to lowercase may lose case information. Additionally, this tokenization ignores word morphology, leading to issues with rare words or homonyms.</p> <p>Let’s write an <code class="language-plaintext highlighter-rouge">analyze_token_statistics</code> function to count unique tokens and their frequencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_stats</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Token statistics for </span><span class="si">{</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total unique tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Top 10 most frequent tokens:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">en_tokenizer</span><span class="p">)</span>
<span class="n">ru_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ru_tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>The difference in token counts for English (1337) and Russian (2065) stems from language features: Russian has richer morphology (endings, prefixes) and more word forms. The dominance of punctuation (. and , in the top) suggests the need for their pre-filtering or separate handling.</p> <p>Interestingly, the <code class="language-plaintext highlighter-rouge">"</code> token appears more frequently in English (146 times) — likely due to translation specifics in Tatoeba.</p> <p>Critically, this approach does not split words into subword units, leaving rare words intact and inflating vocabulary size. For comparison, we will explore the BPE tokenizer in subsequent experiments.</p> <hr/> <h2 id="bpe-tokenization-algorithm">BPE Tokenization Algorithm</h2> <p>Now let’s examine how the <strong>BPE (Byte Pair Encoding)</strong> tokenizer works. The core idea is to iteratively merge the most frequent character or token pairs, gradually forming a subword vocabulary. This efficiently handles rare and complex words by splitting them into known components.</p> <h3 id="bpetokenizer-class">BPETokenizer Class</h3> <p>First, declare the class with the <code class="language-plaintext highlighter-rouge">@dataclass</code> decorator. Since it inherits from <code class="language-plaintext highlighter-rouge">BaseTokenizer</code>, it already includes parameters <code class="language-plaintext highlighter-rouge">language</code>, <code class="language-plaintext highlighter-rouge">vocab_size</code>, <code class="language-plaintext highlighter-rouge">min_freq</code>, and <code class="language-plaintext highlighter-rouge">special_tokens</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
</code></pre></div></div> <h3 id="initialization">Initialization</h3> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called after object creation. Here we:</p> <ol> <li>Call the parent <code class="language-plaintext highlighter-rouge">__post_init__</code> to initialize base structures like <code class="language-plaintext highlighter-rouge">token2id</code>.</li> <li>Add a <code class="language-plaintext highlighter-rouge">merges</code> dictionary to store character pairs and their merged versions (e.g., <code class="language-plaintext highlighter-rouge">('h', 'e')</code> → <code class="language-plaintext highlighter-rouge">'he'</code>).</li> <li>Initialize <code class="language-plaintext highlighter-rouge">vocab</code> with special tokens.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>
</code></pre></div></div> <h3 id="generating-character-pairs">Generating Character Pairs</h3> <p>The <code class="language-plaintext highlighter-rouge">get_pairs</code> method splits a word into consecutive character pairs. For example, the word <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code> returns pairs <code class="language-plaintext highlighter-rouge">[('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')]</code>. These pairs are analyzed during training to find the most frequent combinations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div> <hr/> <h2 id="training-the-tokenizer">Training the Tokenizer</h2> <p>The <code class="language-plaintext highlighter-rouge">train</code> method is the core of BPE. It has several stages:</p> <p><strong>Collect Initial Statistics:</strong></p> <ul> <li>Split each token into characters and count character sequence frequencies. For example, token <code class="language-plaintext highlighter-rouge">"hello"</code> becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>, and its frequency increments the counter for <code class="language-plaintext highlighter-rouge">'h e l l o'</code>.</li> <li>Collect all unique characters from the text.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Adding Characters to Vocabulary:</strong></p> <ul> <li>Each unique character (e.g., <code class="language-plaintext highlighter-rouge">'h'</code>, <code class="language-plaintext highlighter-rouge">'e'</code>) is added to <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">vocab</code> if not already present. This ensures even individual characters have IDs.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Main Merge Loop:</strong></p> <ul> <li>Each iteration counts the frequency of all possible character pairs in the current word representations. For example, the word <code class="language-plaintext highlighter-rouge">'h e l l o'</code> has pairs <code class="language-plaintext highlighter-rouge">('h', 'e')</code>, <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, etc.</li> <li>Select the most frequent pair (e.g., <code class="language-plaintext highlighter-rouge">('l', 'l')</code> for <code class="language-plaintext highlighter-rouge">hello</code>) and create a new token <code class="language-plaintext highlighter-rouge">'ll'</code>.</li> <li>Update <code class="language-plaintext highlighter-rouge">merges</code>, <code class="language-plaintext highlighter-rouge">vocab</code>, <code class="language-plaintext highlighter-rouge">token2id</code>, and <code class="language-plaintext highlighter-rouge">id2token</code>.</li> <li>Recalculate word frequencies by replacing the selected pair with the new token. For example, <code class="language-plaintext highlighter-rouge">'h e l l o'</code> becomes <code class="language-plaintext highlighter-rouge">'h e ll o'</code> after merging <code class="language-plaintext highlighter-rouge">('l', 'l')</code>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
    <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="c1"># Count pair frequencies
</span>    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
        <span class="k">break</span>  <span class="c1"># No pairs left → stop
</span>
    <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

    <span class="c1"># Update vocabulary
</span>    <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

    <span class="c1"># Recalculate frequencies with new token
</span>    <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
        <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>
</code></pre></div></div> <p>If <code class="language-plaintext highlighter-rouge">num_merges</code> is too high and pairs are exhausted early, training stops. Progress is printed every 1000 iterations to track vocabulary growth.</p> <hr/> <h3 id="text-tokenization">Text Tokenization</h3> <p>The <code class="language-plaintext highlighter-rouge">tokenize</code> method converts text to token IDs:</p> <ol> <li>Text is split into tokens via <code class="language-plaintext highlighter-rouge">preprocess_text</code>.</li> <li>The <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> special token is prepended.</li> <li>For each token (e.g., <code class="language-plaintext highlighter-rouge">"hello"</code>): <ul> <li>Characters are split into <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>.</li> <li>Merges from <code class="language-plaintext highlighter-rouge">merges</code> are applied iteratively. For example, if <code class="language-plaintext highlighter-rouge">('l', 'l')</code> is in <code class="language-plaintext highlighter-rouge">merges</code>, the character list becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'll', 'o']</code>, then remaining pairs are checked.</li> <li>Unknown characters (e.g., <code class="language-plaintext highlighter-rouge">'#'</code>) are replaced with <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</li> </ul> </li> <li>The <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is appended.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
            <span class="c1"># Find first available merge pair
</span>            <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                    <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Replace pair with new token
</span>            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

        <span class="c1"># Add final symbols to result
</span>        <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>During tokenization, merges are applied left-to-right, and the <strong>first</strong> available pair from <code class="language-plaintext highlighter-rouge">merges</code> is chosen. This can yield different results depending on the merge order. For example, if <code class="language-plaintext highlighter-rouge">merges</code> contains <code class="language-plaintext highlighter-rouge">('h', 'e')</code> and <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, the first encountered pair is merged.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
                <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>

        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training BPE tokenizer for </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
            <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

            <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
                <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>

            <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Merges completed: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_merges</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

            <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                        <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

            <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>When applying the tokenizer to convert text to tokens, the algorithm first splits text into base characters, then iteratively merges character pairs using the built merge dictionary. Each word in the text is represented as a sequence of subwords (or tokens) created during training.</p> <p>The number of merges (<code class="language-plaintext highlighter-rouge">num_merges</code> parameter) determines how many times the algorithm will merge characters into new tokens. More merges create larger, more informative tokens. However, excessive merges can lead to loss of fine-grained details.</p> <p>This algorithm performs well with large text corpora and helps models handle rare or unseen words by replacing them with subwords from more frequent character combinations. Additionally, BPE works with any language, even those with unusual or complex alphabets, as it starts from base characters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">80000</span>
<span class="n">train_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
<span class="n">en_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>
<span class="n">ru_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>

<span class="n">en_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">en_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">ru_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">ru_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">English vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Russian vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ru_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span>
<span class="n">ru_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">en_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">en_sample</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ru_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">ru_sample</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">)</span>
</code></pre></div></div> <p>Overall, BPE effectively addresses rare and complex words, improving tokenization quality and NLP model performance.</p> <p>However, even after training, artifacts remain. For example, “useless” splits into [“us”, “el”, “ess”], and “бесполезно” into [“бес”, “пол”, “ез”, “но”]. This stems from the limited number of merges and the lack of explicit morpheme boundary consideration in our educational implementation.</p> <p>In production tokenizers (e.g., Hugging Face’s), such issues are mitigated by pretraining on massive corpora and tens of thousands of merges.</p> <hr/> <h2 id="batch-preparation">Batch Preparation</h2> <p>The <code class="language-plaintext highlighter-rouge">prepare_batch</code> function converts tokenized sequences into tensors suitable for training. Each sentence is padded to a fixed length (<code class="language-plaintext highlighter-rouge">max_length=64</code>) with the <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token, and attention masks tell the model to ignore these “empty” positions.</p> <p>For example, a sentence with 24 tokens becomes a vector of length 64, where the last 40 elements are zeros (ID <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>). Masking is critical for transformers, as the attention mechanism would otherwise account for meaningless padding tokens, distorting weights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
                 <span class="n">src_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">tgt_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="n">src_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">src_texts</span><span class="p">]</span>
    <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">tgt_texts</span><span class="p">]</span>

    <span class="n">src_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">src_masks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_masks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>

        <span class="n">src_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_pad</span><span class="p">)</span>
        <span class="n">tgt_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_pad</span><span class="p">)</span>
        <span class="n">src_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_mask</span><span class="p">)</span>
        <span class="n">tgt_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_masks</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_masks</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="tokenizer-verification">Tokenizer verification</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Prepared batch shapes:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Example source tokens:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Corresponding mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">base_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Base tokenization: </span><span class="si">{</span><span class="n">base_tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Number of merges learned: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample merges (first 5):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">merged</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">merged</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample vocabulary items (first 10):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final tokenization:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoded tokens: </span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing English tokenizer:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr/> <h2 id="hugging-face-tokenizers">Hugging Face Tokenizers</h2> <p>All this seems quite complex. Our current tokenizer works imperfectly and is slow. Fortunately, programmers avoid reinventing the wheel. In practice, it’s much easier to use a ready-made tokenizer via <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> from the <code class="language-plaintext highlighter-rouge">transformers</code> library.</p> <p>The <code class="language-plaintext highlighter-rouge">opus-mt-en-ru</code> model already has a pretrained BPE vocabulary optimized for the language pair. The tokenizer automatically adds special tokens, handles case, and rare symbols. When processing the dataset, the <code class="language-plaintext highlighter-rouge">map</code> function applies tokenization in parallel to all examples, speeding up work via batching.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Helsinki-NLP/opus-mt-en-ru</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
<span class="p">):</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="n">source_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>
        <span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>

        <span class="n">source_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">source_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="n">target_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">target_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">decoder_attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span>
        <span class="n">preprocess_function</span><span class="p">,</span>
        <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">column_names</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="comparing-tokenizers">Comparing Tokenizers</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Custom BPE Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of prepared batches:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">array</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (dtype: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sample data from first batch:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Source tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Target tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Source mask (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hugging Face Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset features: </span><span class="si">{</span><span class="n">processed_dataset</span><span class="p">.</span><span class="n">features</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of examples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">first_example</span> <span class="o">=</span> <span class="n">processed_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">First example details:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input IDs shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded input:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Labels shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded labels:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention mask sample:</span><span class="sh">"</span><span class="p">,</span> <span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>


<span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="tokenization,"/><category term="bpe,"/><category term="algorithms,"/><category term="nlp"/><summary type="html"><![CDATA[Implement one of the most popular tokenization algorithms and learn how to use ready-made solutions.]]></summary></entry><entry><title type="html">Can AI Achieve True Creativity?</title><link href="https://xmarva.github.io/blog/2025/creative-ai/" rel="alternate" type="text/html" title="Can AI Achieve True Creativity?"/><published>2025-02-13T15:00:00+00:00</published><updated>2025-02-13T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/creative-ai</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/creative-ai/"><![CDATA[<p>The criticism that AI cannot create something fundamentally new often overlooks a key fact: human creativity itself is built on combining existing knowledge, concepts, and imagery.</p> <p>Modern research reveals that when humans invent something novel, the brain doesn’t conjure elements “from nothing” but recombines fragments of prior experiences. Neural networks operate similarly.</p> <p>When GPT-4 generates text, it relies on statistical patterns learned from existing data rather than conscious intent.</p> <h3 id="the-brains-creative-networks-vs-gans">The Brain’s Creative Networks vs. GANs</h3> <p>In one experiment, participants were asked to devise unconventional uses for everyday objects (e.g., a coffee cup). fMRI scans showed two activated networks during creativity:</p> <ul> <li>The default mode network (associated with daydreaming and associations)</li> <li>The frontoparietal network (linked to focus and control)</li> </ul> <p>The dorsolateral prefrontal cortex also activates during musical improvisation. This interaction mirrors generative adversarial networks (GANs), where one component generates ideas and another evaluates their plausibility. Both biological and artificial systems balance freedom and constraints to produce novelty.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/0-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/0-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="fMRI activation during creative tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fMRI activation patterns during creative tasks (Source: Beaty et al., 2016) </div> <h3 id="the-limits-of-perception-and-data">The Limits of Perception and Data</h3> <p>Human perception constrains creativity. We can’t imagine colors beyond the visible spectrum—any new shade is a recombination of known hues. Similarly, Stable Diffusion can’t generate images of objects absent from its training data. It blends learned features, much like the brain combines memories.</p> <p>Blind individuals describe colors through analogies to sounds or textures (e.g., red as “loud noise,” blue as “smooth surface”). This suggests creativity is rooted in sensory experience—a dimension AI lacks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/1-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/1-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stable Diffusion-generated floral carpet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Floral carpet generated by Stable Diffusion </div> <h3 id="embodied-vs-abstract-learning">Embodied vs. Abstract Learning</h3> <p>A key difference lies in how humans and AI acquire knowledge:</p> <ul> <li>Humans learn through physical interaction (e.g., toddlers touching objects to link tactile and visual data)</li> <li>AI processes abstract tokens/pixels</li> </ul> <p>While LLMs develop semantic relationships (e.g., “apple” vectors near “fruit” and “tree”), they lack embodied experiences stored in the brain’s sensorimotor cortex. When we think “run,” motor neurons activate—a connection AI can’t replicate.</p> <h3 id="the-myth-of-intentional-creativity">The Myth of Intentional Creativity</h3> <p>Critics argue humans possess creative “intent,” but jazz improvisation studies show decreased prefrontal cortex activity during spontaneous creation. Ideas emerge automatically from learned patterns—a process strikingly similar to how neural networks operate.</p> <h3 id="the-originality-debate">The Originality Debate</h3> <p>If human innovation is recombination, demanding absolute novelty from AI is flawed. Both are constrained by their “training data”:</p> <ul> <li>Human brains: Biological experiences</li> <li>AI models: Digital datasets</li> </ul> <p>The distinction lies in complexity and emotional embodiment. While AI manipulates mathematical structures, the brain ties patterns to emotions and bodily states—for now.</p> <p><strong>References</strong><br/> [1] Beaty, R. E., Benedek, M., Silvia, P. J., &amp; Schacter, D. L. (2016). Creative Cognition and Brain Network Dynamics. <em>Trends in Cognitive Sciences</em><br/> [2] De Borst, A. W., &amp; de Gelder, B. (2018). Mental Imagery Follows Similar Cortical Reorganization as Perception: Intra-Modal and Cross-Modal Plasticity in Congenitally Blind. <em>Cerebral Cortex</em><br/> [3] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dea, J. (2013). Distributed Representations of Words and Phrases and their Compositionality<br/> [4] Limb, C. J., &amp; Braun, A. R. (2008). Neural Substrates of Spontaneous Musical Performance: An fMRI Study of Jazz Improvisation</p>]]></content><author><name></name></author><category term="featured-posts"/><category term="ai,"/><category term="neuroscience,"/><category term="creativity"/><summary type="html"><![CDATA[Exploring the parallels between human creativity and neural networks]]></summary></entry><entry><title type="html">Algebraic Foundations of Low-Rank Adaptation</title><link href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/" rel="alternate" type="text/html" title="Algebraic Foundations of Low-Rank Adaptation"/><published>2024-12-30T15:09:00+00:00</published><updated>2024-12-30T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/lora-algorithm-for-llms</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><![CDATA[<h2 id="the-paradox-of-scale">The Paradox of Scale</h2> <p>The evolution of language models presents us with an intriguing paradox: while increasing model size enhances general capability, it simultaneously complicates practical deployment through prohibitive computational demands. This tension between capacity and practicality forms the crucible where Low-Rank Adaptation (LoRA) emerges as an elegant solution. To understand its mechanisms, we must first establish fundamental mathematical constructs.</p> <h2 id="matrix-theory-foundations">Matrix Theory Foundations</h2> <h3 id="the-algebraic-scaffolding">The Algebraic Scaffolding</h3> <p>A matrix \(A \in \mathbb{R}^{m \times n}\) represents a linear transformation between vector spaces \(\mathbb{R}^n \to \mathbb{R}^m\). Each element \(a_{ij}\) encodes the transformation coefficient between basis vectors \(e_j\) and \(e_i\). In neural networks, these matrices become learned representations of feature interactions.</p> <p>The <strong>rank</strong> of a matrix, denoted \(\rho(A)\), measures its column space dimensionality through the maximal number of linearly independent columns. Formally:</p> \[\rho(A) = \dim(\text{col}(A)) = \dim(\text{row}(A))\] <p>This duality between row and column space dimensionalities (proven via the Fundamental Theorem of Linear Algebra) becomes crucial for understanding parameter efficiency.</p> <h3 id="rank-constrained-transformations">Rank-Constrained Transformations</h3> <p>Consider two matrices \(B \in \mathbb{R}^{m \times r}\) and \(A \in \mathbb{R}^{r \times n}\). Their product \(BA\) inherently satisfies:</p> \[\rho(BA) \leq \min(\rho(B), \rho(A)) \leq r\] <p>This rank upper bound enables dramatic parameter reduction when \(r \ll \min(m,n)\). For a neural layer with \(m \times n\) weights, replacing full updates with low-rank factors reduces trainable parameters from \(mn\) to \(r(m+n)\) – an efficiency gain of \(\frac{mn}{r(m+n)}\). For typical layers (\(m,n \sim 10^3\), \(r \sim 10^1\)), this yields ~100x parameter reduction.</p> <h2 id="the-low-rank-adaptation-hypothesis">The Low-Rank Adaptation Hypothesis</h2> <h3 id="intrinsic-dimensionality-of-task-adaptation">Intrinsic Dimensionality of Task Adaptation</h3> <p>Modern language models exhibit an intriguing property: while pretrained on broad corpora, task-specific adaptation appears to operate in low-dimensional subspaces. This phenomenon aligns with the <strong>manifold hypothesis</strong>, suggesting high-dimensional data actually resides on lower-dimensional manifolds.</p> <p>Let \(\Delta W \in \mathbb{R}^{m \times n}\) represent weight updates during fine-tuning. The LoRA conjecture posits:</p> \[\rho(\Delta W) \leq r \ll \min(m,n)\] <p>Experimental validation shows task adaptation often requires surprisingly low ranks (\(r=8\) achieves strong performance). This implies that while the original parameter space is vast, task-specific adjustments occupy a small subspace.</p> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Visualize the weight matrix as a point in \(\mathbb{R}^{mn}\). Full fine-tuning moves this point through the high-dimensional space. LoRA constrains movement to a low-dimensional <strong>adaptation manifold</strong> spanned by \(B\) and \(A\):</p> \[\mathcal{M}_r = \{ W + BA \mid B \in \mathbb{R}^{m \times r}, A \in \mathbb{R}^{r \times n} \}\] <p>The approximation error is bounded by the Eckart–Young theorem:</p> \[\min_{\rho(BA)\leq r} \| \Delta W - BA \|_F = \sum_{i=r+1}^{\min(m,n)} \sigma_i(\Delta W)\] <p>where \(\sigma_i\) denotes singular values. Rapidly decaying singular values in \(\Delta W\) (as observed empirically) enable accurate low-rank approximation.</p> <h2 id="algorithmic-implementation">Algorithmic Implementation</h2> <h3 id="parameterization-and-initialization">Parameterization and Initialization</h3> <p>For a pretrained weight matrix \(W_0\), LoRA constructs:</p> \[W = W_0 + \frac{\alpha}{r}BA\] <p>Where:</p> <ul> <li>\(B\) initialized with \(\mathcal{N}(0, \sigma^2)\)</li> <li>\(A\) initialized to zero</li> <li>\(\alpha\): learning rate scaling factor</li> </ul> <p>The initialization strategy ensures \(\Delta W = 0\) at training onset, preserving original model behavior. The \(\alpha/r\) scaling normalizes parameter updates across different ranks, maintaining stable learning dynamics.</p> <h3 id="gradient-dynamics">Gradient Dynamics</h3> <p>Let \(\mathcal{L}\) be the loss function. The gradient through the LoRA parameters becomes:</p> \[\nabla_B \mathcal{L} = \frac{\alpha}{r} (\nabla_{W} \mathcal{L}) A^T \\ \nabla_A \mathcal{L} = \frac{\alpha}{r} B^T (\nabla_{W} \mathcal{L})\] <p>This reveals an important property: gradient signals flow through both low-rank factors, with the scaling term modulating update magnitudes. The rank \(r\) therefore acts as a gradient multiplier – higher ranks enable stronger gradient signals but increase parameter count.</p> <h2 id="practical-considerations-and-variations">Practical Considerations and Variations</h2> <h3 id="rank-selection-tradeoffs">Rank Selection Tradeoffs</h3> <p>The choice of \(r\) balances expressivity vs efficiency:</p> <ul> <li><strong>Lower ranks (r=1-4):</strong> Maximize parameter efficiency, suitable for similar source/target tasks</li> <li><strong>Medium ranks (r=8-16):</strong> General-purpose setting for domain adaptation</li> <li><strong>Higher ranks (r=32+):</strong> Needed for complex task transfers or low-data scenarios</li> </ul> <p>Empirical studies show performance follows logarithmic scaling:</p> \[\text{Performance}(r) \approx \text{Performance}(\text{full}) - c/\log r\] <p>Where \(c\) is task-dependent. This suggests diminishing returns beyond certain ranks.</p> <h3 id="architectural-variants">Architectural Variants</h3> <ol> <li><strong>Bottleneck Adaptation:</strong> Stack multiple low-rank layers (\(W_0 + B_1A_1 + B_2A_2\)) for hierarchical adaptation</li> <li><strong>Sparse LoRA:</strong> Combine with magnitude pruning on \(BA\) product</li> <li><strong>Dynamic Rank Allocation:</strong> Use singular value thresholds to automatically select per-layer ranks</li> <li><strong>LoRA++:</strong> Introduce learned scaling factors per layer instead of fixed \(\alpha/r\)</li> </ol> <h3 id="compositional-adaptation">Compositional Adaptation</h3> <p>For multi-task learning, LoRA enables parameter composition:</p> \[W = W_0 + \sum_{k=1}^K B_kA_k\] <p>Where each \(B_kA_k\) captures task-specific adaptations. During inference, select subsets of adapters via:</p> \[W = W_0 + \sum_{k \in S} B_kA_k\] <p>This facilitates efficient multi-task serving with \(\mathcal{O}(Kr)\) storage instead of \(\mathcal{O}(K)\) full models.</p> <h2 id="theoretical-implications">Theoretical Implications</h2> <h3 id="implicit-regularization">Implicit Regularization</h3> <p>The low-rank constraint acts as a strong regularizer, preventing overfitting to small datasets. Consider the Rademacher complexity for a LoRA-adapted layer:</p> \[\mathcal{R}_n(\mathcal{H}_{\text{LoRA}}) \leq \frac{\alpha \sqrt{2r\log(2mn)}}{n}\] <p>Compared to full fine-tuning’s \(\mathcal{O}(\sqrt{mn/n})\) complexity, LoRA’s bound is significantly tighter, explaining its improved generalization in low-data regimes.</p> <h3 id="information-bottleneck-perspective">Information Bottleneck Perspective</h3> <p>Interpreting through the information bottleneck lens, LoRA enforces:</p> \[\min_{B,A} I(W; BA) \quad \text{s.t.} \quad I(BA; \mathcal{T}) \geq I_c\] <p>Where \(\mathcal{T}\) is the target task and \(I_c\) the required information. The low-rank structure naturally minimizes irrelevant information from \(W\) while preserving task-relevant features.</p> <h2 id="epilogue">Epilogue</h2> <p>LoRA epitomizes the principle that profound solutions often arise from deep mathematical insight rather than brute-force computation. By reconceptualizing adaptation as a low-rank update process, it achieves an elegant synthesis of efficiency and effectiveness – a reminder that in machine learning as in mathematics, constraints often breed creativity.</p> <p>The road ahead suggests intriguing possibilities: could other matrix properties (e.g., sparsity patterns, eigenvalue distributions) inspire new adaptation paradigms? As language models continue evolving, such algebraic perspectives will likely remain essential tools for harnessing their potential.</p>]]></content><author><name></name></author><category term="nlp,"/><category term="llm,"/><category term="lora"/><summary type="html"><![CDATA[Mathematical exploration of parameter-efficient fine-tuning through matrix rank theory]]></summary></entry><entry><title type="html">LLMs for Those Who Missed Out</title><link href="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/" rel="alternate" type="text/html" title="LLMs for Those Who Missed Out"/><published>2024-04-24T15:09:00+00:00</published><updated>2024-04-24T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/"><![CDATA[<p>An LLM (large language model) consists of just two files:</p> <ul> <li>Large file with billions of parameters (weights)</li> <li>Small file with about 500 lines of code to run the model</li> </ul> <p>LLM doesn’t take up much space and doesn’t require extensive computing power.</p> <p>For example, the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama-2–70b model</a> from Meta, which is open source and has 70 billion parameters, is just 140 GB and can be run locally on a MacBook without internet access. You can <a href="https://www.llama.com/llama-downloads/">download this model</a> from Meta’s website and use it for free.</p> <p>However, you need significant computing resources to get these parameters (train the model).</p> <h2 id="how-to-train-a-model">How to Train a Model</h2> <h3 id="stage-1-pretraining-training-the-base-model">Stage 1: Pretraining (Training the base model)</h3> <p>Think of this process as compressing the Internet into a neural network, similar to how a ZIP file compresses documents into an archive.</p> <p>Here’s what you need to get a model like Llama-2:</p> <ul> <li>Take a “slice of the internet,” approximately ten terabytes of text in size</li> <li>Use a cluster of ~6,000 GPUs (specialized graphics processors used for heavy computations)</li> <li>Compress the text into the neural network. This takes about 12 days and costs around $2M</li> <li>Acquire the base model (the file with parameters)</li> </ul> <p>The primary function of the base model is to predict the next word. You input a sequence of words, and it outputs the next most likely word based on the texts it was trained on.</p> <p>For example, for the phrase “cat sat on a,” the LLM might predict the word “mat” with 97% probability.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/0-480.webp 480w,/assets/img/posts/2024-04-24-llms/0-800.webp 800w,/assets/img/posts/2024-04-24-llms/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image from the presentation illustrating how the neural network works" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image from the presentation illustrating how the neural network works </div> <p>That’s basically how compression works: if you can predict the next word accurately, you can use this information to compress the original data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/1-480.webp 480w,/assets/img/posts/2024-04-24-llms/1-800.webp 800w,/assets/img/posts/2024-04-24-llms/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="I checked, and ChatGPT indeed responds with “mat”" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> I checked, and ChatGPT indeed responds with “mat” </div> <p>However, unlike a ZIP file, where compression is lossless, LLM “compresses” the internet with losses. This is why models can hallucinate: they make up information that doesn’t actually exist but appears plausible (similar to what they have seen in the training data).</p> <p>Moreover, models can sometimes produce completely unexpected things. For example, ChatGPT knows the answer to the question, “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer), but it doesn’t know the answer to the question, “Who is Mary Lee Pfeiffer’s son?”</p> <p>This isn’t a typical database that simply stores information; it’s a different format we don’t fully understand.</p> <p>This clearly illustrates that we don’t quite grasp how this whole thing works and can only see the results it produces.</p> <h3 id="stage-2-finetuning-additional-training">Stage 2: Finetuning (Additional training)</h3> <p>The base model isn’t very practical for everyday use. We don’t just want to receive continuous word sequences; we want to ask questions and get answers.</p> <p>This requires finetuning — a process in which we develop an assistant model that answers questions.</p> <p>The training process for the assistant model is similar to that of the base model, but now we train the model not on internet texts but on data we manually collect. Companies hire people who write questions and answer them.</p> <p>If the training of the base model occurs on huge volumes of often low-quality text, the training of the assistant model involves comparatively smaller volumes (say, 100,000 documents), but they are all of very high quality.</p> <p>After such finetuning, the model understands that when asked a question, it should respond in the style of a helpful assistant.</p> <p>Unlike the training of the base model, which is conducted at most 2–3 times a year, finetuning can be done regularly, even weekly, as it is much less costly.</p> <h3 id="stage-3-optional-comparisons">Stage 3 (optional): Comparisons</h3> <p>In many cases it’s easier not to write a response from scratch but to compare several different answer options and choose the best one.</p> <p>The data from these comparisons is used to train the model further.</p> <p>At OpenAI, this is called RLHF (Reinforcement Learning from Human Feedback).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/2-480.webp 480w,/assets/img/posts/2024-04-24-llms/2-800.webp 800w,/assets/img/posts/2024-04-24-llms/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Difference between Open and Closed Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Difference between Open and Closed Models. Source: Chatbot leaderboards </div> <p>Models with open weights, such as Llama-2, are still inferior compared to proprietary models like GPT-4 and Claude.</p> <p>However, proprietary models cannot be downloaded for personal use or further training; they can only be operated through a web interface or an API (though the introduction of Custom GPTs at OpenAI is a first step towards customization)</p> <h2 id="capabilities-of-llms-using-chatgpt-as-an-example">Capabilities of LLMs (using ChatGPT as an example)</h2> <p>Prompted by an LLM, it can understand what the user needs and use external tools for this:</p> <ul> <li>Search the internet if the user needs up-to-date information (goes to Bing, retrieves a list of the latest links for the query, copies the full text of each link into its context window, and summarizes the information)</li> <li>Use a calculator to do calculations</li> <li>Use a Python library to draw graphs</li> <li>Draw images using DALL-E</li> <li>Write code</li> </ul> <p>Moreover, LLMs can be multimodal. They can recognize text, voice, images, or video and respond with voice, images, or video.</p> <h2 id="can-llms-make-decisions">Can LLMs Make Decisions?</h2> <p>There’s a well-known book by Kahneman, “Thinking, Fast and Slow.”</p> <p>The main idea of the book is that there are two systems in the brain: System 1, which has fast, automatic reactions, and System 2, which is slower, rational, and conscious and makes complex decisions.</p> <p>For example, the answer to question 2+2 comes from System 1 because this knowledge is automatic for us. But calculating 17×24 requires System 2.</p> <p>If we apply these terms, current LLMs only possess System 1. They can only provide the most likely next word in real time.</p> <p>It would be great if we could come to an LLM and say: here’s my question; you can think for 30 minutes, but I need a very accurate and high-quality answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/3-480.webp 480w,/assets/img/posts/2024-04-24-llms/3-800.webp 800w,/assets/img/posts/2024-04-24-llms/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Thought tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Thought tree </div> <p>No models can do that yet. But it would be desirable for a model to have something like a “thought tree,” through which it could navigate, analyze the results, go back and try again until it achieves a result it is most confident about.</p> <h2 id="can-llms-train-themselves">Can LLMs Train Themselves?</h2> <p>There’s a famous case where AlphaGo (a program that plays Go) was trained in two stages:</p> <p>First, it was trained on human games and learned to play very well. Then it began to train itself-playing against itself, trying to maximize the likelihood of winning-and significantly improved its quality.</p> <p>But in LLMs, we’re only at stage 1-training only occurs on materials created by humans.</p> <p>Why is this?</p> <p>In the game of Go, there is a very clear criterion for success-a won game- and you can train the model to maximize the likelihood of winning.</p> <p>In LLMs, the criteria are not so obvious. It’s not clear how to assess whether the result is good.</p> <p>Such criteria can be found in some narrow topics, but in general, it’s still hard to imagine.</p> <h2 id="what-llms-will-be-able-to-do-in-a-few-years">What LLMs Will Be Able to Do in a Few Years</h2> <p>Finally, a small forecast.</p> <p>In a few years, LLMs:</p> <ul> <li>Will have more knowledge than any human on all subjects</li> <li>Can operate on the internet</li> <li>Can use existing software infrastructure (calculator, Python, mouse/keyboard)</li> <li>Can see and generate images and videos</li> <li>Can hear and speak, and also generate music</li> <li>Can think for a long period using System 2</li> <li>Can train themselves in areas where there are clear criteria</li> <li>Can be customized and refined for specific tasks. Many versions will exist in app stores</li> <li>Can interact with other LLMs</li> </ul>]]></content><author><name></name></author><category term="old-posts"/><category term="nlp,"/><category term="llm"/><summary type="html"><![CDATA[Let's talk about large language models. Once again.]]></summary></entry><entry><title type="html">How to Write Good Python Code</title><link href="https://xmarva.github.io/blog/2023/python-code/" rel="alternate" type="text/html" title="How to Write Good Python Code"/><published>2023-02-07T15:09:00+00:00</published><updated>2023-02-07T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2023/python-code</id><content type="html" xml:base="https://xmarva.github.io/blog/2023/python-code/"><![CDATA[<p>Python is a fantastic programming language!</p> <p>It can be used for many things, like building websites, exploring data, and teaching machines to learn.</p> <p>If you already know Python or are just beginning, writing code that is strong, easy to read, and easy to keep up with is important.</p> <p>In this bogpost, we’ll look at the basic rules for writing great Python code and share some tips to help you make your programs even better.</p> <h2 id="-use-meaningful-naming-conventions">📚 Use Meaningful Naming Conventions</h2> <p>One of the most important aspects of good Python code is meaningful naming conventions.</p> <p>Choosing descriptive and concise names for variables, functions, and classes can help make your code more readable and understandable.</p> <p>Using proper naming conventions can also help you avoid naming conflicts, reduce the risk of errors, and simplify maintenance.</p> <p>For example, these are bad variable names:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>And these are better ones:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_number</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">second_number</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sum_of_numbers</span> <span class="o">=</span> <span class="n">first_number</span> <span class="o">+</span> <span class="n">second_number</span>
<span class="n">double_sum</span> <span class="o">=</span> <span class="n">sum_of_numbers</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>This includes using proper indentation, white space, line breaks, and following a code style guide like the PEP 8 style guide.</p> <p>Clear, organized code makes it easier to understand and modify and reduces the risk of errors.</p> <p>Here’s an example of lousy code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sum: </span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Difference: </span><span class="sh">"</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Product: </span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Quotient: </span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
</code></pre></div></div> <p>And here’s an example of good code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Sum</span><span class="sh">"</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Difference</span><span class="sh">"</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Product</span><span class="sh">"</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Quotient</span><span class="sh">"</span><span class="p">:</span> <span class="n">q</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h2 id="-write-comments">💬 Write Comments</h2> <p>Adding comments to your code is a great way to explain what it does and provide context for other developers.</p> <p>Comments should be used to explain complex code, provide additional information about the purpose of the code, and describe your thought process.</p> <p>Writing comments can also help you better understand your code when you return to it later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># function to calculate sum
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># this function calculates sum of two numbers
</span><span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments are not very descriptive or helpful in understanding the purpose of the functions.</p> <p>The first comment is trivial and adds no additional information. The second comment repeats what the function name already tells us.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the sum of two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns their sum.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the sum of `a` and `b`
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the difference between two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns the difference of `a` and `b`.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the difference between `a` and `b`
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments provide a clear and concise explanation of the purpose and behaviour of each function.</p> <p>The use of docstrings makes it easy to understand what the functions do and what arguments they take in. This makes the code more readable and maintainable.</p> <h2 id="-use-modules-and-packages">🧰 Use Modules and Packages</h2> <p>Modules and packages are a great way to organize your code into reusable blocks.</p> <p>They allow you to group related code together and make it easier to manage, understand, and maintain.</p> <p>The Python Standard Library is an good resource for finding pre-existing modules and packages. You can import it into your programs to save time and effort.</p> <p>Consider a project to build a simple weather application that provides a given city’s current temperature and conditions. We can structure the project as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weather_app/
    __init__.py
    weather.py
    utils/
        __init__.py
        api.py
        data_processing.py
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">weather.py</code> is the main module that the user interacts with, which provides a single function to get the current weather information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_current_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Gets the current weather information for the given city.

    Args:
        city (str): The city for which to get the weather information.

    Returns:
        dict: The weather information for the given city.
    </span><span class="sh">"""</span>
    <span class="n">weather_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">api</span><span class="p">.</span><span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>
    <span class="n">processed_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">data_processing</span><span class="p">.</span><span class="nf">process_weather_data</span><span class="p">(</span><span class="n">weather_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <p>The utils package contains two modules, <code class="language-plaintext highlighter-rouge">api.py</code> and <code class="language-plaintext highlighter-rouge">data_processing.py</code>, which contain helper functions to retrieve the raw weather data from an API and to process the raw data into a more readable format, respectively.</p> <p>These modules can be reused across different projects, so it makes sense to organize them into a separate package.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># api.py
</span><span class="k">def</span> <span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Retrieves the raw weather data for the given city.

    Args:
        city (str): The city for which to retrieve the weather data.

    Returns:
        dict: The raw weather data for the given city.
    </span><span class="sh">"""</span>
    <span class="c1"># code to retrieve data from API
</span>    <span class="k">return</span> <span class="n">raw_data</span>

<span class="c1"># data_processing.py
</span><span class="k">def</span> <span class="nf">process_weather_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Processes the raw weather data into a more readable format.

    Args:
        raw_data (dict): The raw weather data.

    Returns:
        dict: The processed weather data.
    </span><span class="sh">"""</span>
    <span class="c1"># code to process data
</span>    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <h2 id="-test-your-code">🧪 Test Your Code</h2> <p>Testing your code helps you catch bugs and ensure that your code works as expected.</p> <p>Writing test cases is also an good way to document your code and help others understand it. Try all possible scenarios when testing your code, including edge cases and error conditions.</p> <p>Consider a module <code class="language-plaintext highlighter-rouge">calculator.py</code> that implements a simple calculator with basic arithmetic operations. We can write test cases for each operation using a testing framework such as <code class="language-plaintext highlighter-rouge">unittest</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">unittest</span>
<span class="kn">import</span> <span class="n">calculator</span>

<span class="k">class</span> <span class="nc">TestCalculator</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_addition</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_subtraction</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_multiplication</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_division</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">unittest</span><span class="p">.</span><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <p>Each test case tests a single operation in the calculator module and uses the <code class="language-plaintext highlighter-rouge">assertEqual</code> method to verify that the result of the operation is as expected.</p> <p>If any test fails, an error will be raised, and the test result will be reported as failed.</p> <p>For debugging we can use the <code class="language-plaintext highlighter-rouge">print</code> statement to print the intermediate results or the values of variables in the code, or use a debugger such as <code class="language-plaintext highlighter-rouge">pdb</code> to step through the code and inspect the values of variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">calculator</span>
<span class="kn">import</span> <span class="n">pdb</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span> <span class="c1"># Set a breakpoint
</span><span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-document-your-code">📜 Document Your Code</h2> <p>Documenting your code with docstrings can help others understand what it does and how it works.</p> <p>Docstrings should provide a high-level overview of the code, including its purpose, usage, and limitations.</p> <p>They should also be written in a clear and natural language style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Circle</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Class to represent a circle with a given radius.

    Attributes:
        radius (float): The radius of the circle.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the Circle class with a given radius.

        Args:
            radius (float): The radius of the circle.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">=</span> <span class="n">radius</span>

    <span class="k">def</span> <span class="nf">area</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the area of the circle.

        Returns:
            float: The area of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">circumference</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the circumference of the circle.

        Returns:
            float: The circumference of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">radius</span>
</code></pre></div></div> <p>The class has a docstring explaining its purpose and the attributes it has.</p> <p>Each method has its docstring explaining what it does and what arguments it takes and returns.</p> <p>This makes the code easier to understand and maintain and more accessible for others to use and build upon.</p> <h2 id="-handle-exceptions-gracefully">💥 Handle Exceptions Gracefully</h2> <p>Handling exceptions in your code is essential for ensuring that it continues to run even when unexpected events occur.</p> <p>Use <code class="language-plaintext highlighter-rouge">try</code> and <code class="language-plaintext highlighter-rouge">except</code> statements to handle exceptions and provide helpful error messages that explain what went wrong and how to fix it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="n">result</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="mi">0</span>
<span class="k">except</span> <span class="nb">ZeroDivisionError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a non-zero value for division</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The code inside the <code class="language-plaintext highlighter-rouge">try</code> block may raise a <code class="language-plaintext highlighter-rouge">ZeroDivisionError</code> exception.</p> <p>The <code class="language-plaintext highlighter-rouge">except</code> block handles the exception and prints a helpful error message to the user.</p> <p>This way, the program can continue running even when an unexpected error occurs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">file.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">FileNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a valid file path</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle any other exceptions
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An unexpected error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, the code inside the try block may raise a <code class="language-plaintext highlighter-rouge">FileNotFoundError</code> or any other exception.</p> <p>The first <code class="language-plaintext highlighter-rouge">except</code> block handles the FileNotFoundError and provides a helpful error message for the user.</p> <p>The second <code class="language-plaintext highlighter-rouge">except</code> block handles any other exceptions that may occur and provides a generic error message.</p> <p>This way the program can continue running even when unexpected errors occur and provide helpful error messages to the user.</p> <h2 id="-use-keyword-arguments">🔑 Use Keyword Arguments</h2> <p>Keyword arguments are a powerful feature of Python that allows you to specify default values for function arguments and make your code more readable and flexible.</p> <p>Using keyword arguments can also help you reduce the number of lines of code in your programs and make them easier to understand.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">!</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">John</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hello, John!
</span><span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">Jane</span><span class="sh">"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hi, Jane!
</span></code></pre></div></div> <p>In this example, the greet function takes in two arguments: name and message. The message argument has a default value of “Hello”.</p> <p>When we call <code class="language-plaintext highlighter-rouge">greet("John")</code>, the default value of <code class="language-plaintext highlighter-rouge">"Hello"</code> is used for the message argument. But when we call <code class="language-plaintext highlighter-rouge">greet("Jane", message="Hi")</code>, the keyword argument is used instead, and the output is <code class="language-plaintext highlighter-rouge">"Hi, Jane!"</code>.</p> <h2 id="️-follow-the-zen-of-python">🧘‍♀️ Follow the Zen of Python</h2> <p>The Zen of Python is a collection of principles and guidelines for writing good Python code.</p> <p>It includes tips on writing simple, clear, and maintainable code and advice on choosing between different solutions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">this</span>

<span class="k">def</span> <span class="nf">sort_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Simple is better than complex
</span>    <span class="n">data</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Readability counts
</span><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Explicit is better than implicit
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Flat is better than nested
</span><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">lists</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Use meaningful names
</span><span class="k">def</span> <span class="nf">calculate_average_score</span><span class="p">(</span><span class="n">scores</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">score</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># One obvious way to do it
</span>    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="n">count</span>
</code></pre></div></div> <p>We follow the Zen of Python by:</p> <ul> <li>Writing straightforward code (e.g. the <code class="language-plaintext highlighter-rouge">sort_data</code> function)</li> <li>Choosing meaningful names for variables and functions (e.g. <code class="language-plaintext highlighter-rouge">calculate_average_score</code>)</li> <li>Keeping the code flat and avoiding nested structures where possible (e.g. the flatten function)</li> <li>Being explicit and transparent in our code (e.g. using return statements)</li> </ul> <h2 id="-refactor-your-code-regularly">🛠 Refactor Your Code Regularly</h2> <p>Refactoring is improving the structure and quality of your code without changing its external behaviour.</p> <p>It can help you identify areas that need improvement and make your code more maintainable over time. This can be especially important in projects with a long lifespan or requiring continuous updates.</p> <p>So you can simplify complex sections, make your code more efficient, and eliminate any redundant or unnecessary parts. You can also take advantage of new features or libraries that have become available since you wrote the original code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">number</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">number</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Refactored code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span>
</code></pre></div></div> <p>We have refactored the <code class="language-plaintext highlighter-rouge">calculate_sum</code> function to use the built-in sum function instead of manually iterating over the numbers and adding them up. This code is more efficient and readable and takes advantage of a built-in feature of Python that can perform the same calculation.</p>]]></content><author><name></name></author><category term="old-posts"/><category term="python,"/><category term="coding"/><summary type="html"><![CDATA[Let's look at the basic rules for writing great Python code]]></summary></entry></feed>