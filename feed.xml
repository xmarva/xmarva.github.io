<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://xmarva.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xmarva.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-17T15:31:56+00:00</updated><id>https://xmarva.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Building a Transformer</title><link href="https://xmarva.github.io/blog/2025/building-a-transformer/" rel="alternate" type="text/html" title="Building a Transformer"/><published>2025-04-16T15:00:00+00:00</published><updated>2025-04-16T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/building-a-transformer</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/building-a-transformer/"><![CDATA[<h2 id="building-a-transformer">Building a Transformer</h2> <p>The Transformer architecture was a groundbreaking development in the field of sequence processing. Unlike traditional models like RNNs and LSTMs, which process data sequentially, the Transformer uses an attention mechanism that allows it to process the entire sequence in parallel. This dramatically speeds up training and improves performance.</p> <p>The key innovation of the Transformer is the use of self-attention, which enables the model to effectively take into account the context of each word or token, regardless of its position in the sequence. This architecture has become the foundation for many modern models, including BERT, GPT, and others, significantly improving the performance of natural language processing tasks.</p> <p>We’ll need to install a few libraries (and import even more), and as we go, the list will only grow. You can safely ignore these setup cells and just run them. Focus on the main code instead.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">torchdata</span><span class="o">==</span><span class="mf">0.3</span><span class="p">.</span><span class="mi">0</span> <span class="n">torchtext</span><span class="o">==</span><span class="mf">0.12</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2</span> <span class="n">altair</span><span class="o">==</span><span class="mf">5.5</span><span class="p">.</span><span class="mi">0</span> <span class="n">GPUtil</span><span class="o">==</span><span class="mf">1.4</span><span class="p">.</span><span class="mi">0</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">de_core_news_sm</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_sm</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">GPUtil</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">import</span> <span class="n">altair</span> <span class="k">as</span> <span class="n">alt</span>
<span class="kn">from</span> <span class="n">altair</span> <span class="kn">import</span> <span class="n">Chart</span>

<span class="n">alt</span><span class="p">.</span><span class="n">data_transformers</span><span class="p">.</span><span class="nf">disable_max_rows</span><span class="p">()</span>
</code></pre></div></div> <h2 id="positional-encoding">Positional Encoding</h2> <p>Before the Transformer architecture came along, sequence models like RNNs and LSTMs processed data step by step, inherently taking element order into account. But their inefficiency (due to sequential computation and difficulty with parallelization) drove the search for alternatives.</p> <p>The Transformer eliminated these limitations by introducing a fully parallel approach. But that parallelism introduced a new problem: if all tokens are processed simultaneously, how can the model know their order?</p> <p>To address this, researchers proposed <strong>Positional Encoding</strong> — a mechanism that encodes positional information into each element.</p> <p>Positional Encoding adds special signals to the token embeddings that depend on their position in the sequence. This helps the model distinguish between, say, the word “cat” at position 1 and “cat” at position 5 — even if their semantic embeddings are identical. The encoding formula uses a mix of sine and cosine functions with different frequencies:</p> \[PE_{(pos,\, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right), \quad PE_{(pos,\, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)\] <p>where \(pos\) is the position in the sequence, \(d_{\text{model}}\) is the embedding dimension, and \(i\) is the index of the vector component.</p> <p>The core idea is that these sinusoidal functions allow the model to pay attention to <strong>relative positions</strong>.</p> <h3 id="why-does-this-formula-encode-relative-positions">Why does this formula encode relative positions?</h3> <p>Imagine every position in the sequence as a point on a number line. If we generate sine and cosine signals for position \(pos\), then for position \(pos + k\) those signals can be expressed using combinations of the originals. For example, using the angle addition formula:</p> \[\sin(pos + k) = \sin(pos)\cos(k) + \cos(pos)\sin(k),\] <p>This means a shift by \(k\) positions becomes a weighted sum of the original sine and cosine values. That lets the model naturally pick up on things like “a word three positions away” being related to the current one — even if it’s never seen sequences that long during training.</p> <p>The logarithmic frequency decay in the denominator, \(10000^{2i/d_{\text{model}}}\), ensures that different components of the position vector focus on different scales. For small \(i\) (i.e., early dimensions of the vector), the denominator is large, so the sine and cosine functions grow slowly with \(pos\). These low-frequency oscillations help distinguish broad regions of the sequence — like the beginning (positions 1–100) from the middle (positions 101–200). For large \(i\), the denominator shrinks, so the functions grow faster and produce high-frequency oscillations that capture fine-grained position differences — like 101 vs. 102.</p> <p>Alternating between sine and cosine for even and odd indices ensures unique positional encodings. If we used only sine, some positions could accidentally overlap because of its periodic nature (e.g., \(\sin(pos)\) and \(\sin(pos + 2\pi)\)). Including cosine for neighboring vector components breaks that symmetry: the combination of \(\sin(f(pos))\) and \(\cos(f(pos))\) across various frequencies \(f\) guarantees that every \(pos\) has a unique vector. Since sine and cosine are nearly orthogonal (their dot product is close to zero), their signals don’t interfere with the word embeddings, letting the model process semantics and position independently.</p> <p>The sum \(\text{Embedding} + PE\) works because both word embeddings and positional encodings have the same dimensionality, \(d_{\text{model}}\). This addition requires no learnable parameters — the model receives a unified signal where the meaning of a word is adjusted based on its position. Gradients flow cleanly through this operation since the derivative of a sum is just the sum of derivatives. As a result, during training, the model naturally learns to refine both the semantic embeddings and the use of positional signals (via the attention mechanism), without signal conflict.</p> <p>Researchers explored other options too, like learnable positional embeddings. But the sinusoidal approach proved more effective at generalizing to sequences longer than those seen during training. So, Positional Encoding became a well-balanced solution — expressive, efficient, and free of extra learnable parameters — making it a perfect fit for the Transformer’s parallel architecture.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span>
                             <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h2 id="multiheadattention">MultiHeadAttention</h2> <p><strong>MultiHeadAttention</strong> is the core of the Transformer architecture. Attention in Transformers emerged as a response to the limitations of earlier attention mechanisms used in seq2seq models. Initially, self-attention allowed each element in a sequence to interact with others by computing weighted sums of their features. But there was a problem: a single-head attention mechanism could only focus on <strong>one type of dependency</strong> — like syntactic relationships or semantic similarity. For complex tasks like translation, we need to capture <strong>multiple types of interactions</strong> at once: subject-verb agreement, anaphora, contextual synonyms, and more.</p> <p><strong>The solution</strong>: instead of one attention mechanism, use several parallel “heads,” each learning to capture its own type of dependency. Formally, for input vectors (embeddings) \(X \in \mathbb{R}^{n \times d_{\text{model}}}\) — where \(n\) is the sequence length and \(d_{\text{model}}\) is the embedding size — each head \(h\) projects \(X\) into three separate spaces: queries (\(Q_h\)), keys (\(K_h\)), and values (\(V_h\)), using learned weight matrices:</p> \[Q_h = X W_h^Q, \quad K_h = X W_h^K, \quad V_h = X W_h^V,\] <p>where \(W_h^Q, W_h^K \in \mathbb{R}^{d_{\text{model}} \times d_k}\), \(W_h^V \in \mathbb{R}^{d_{\text{model}} \times d_v}\), and \(d_k\), \(d_v\) are the dimensionalities of the key/query and value subspaces, respectively. This triplet (\(Q, K, V\)) mirrors concepts from information retrieval:</p> <ul> <li><strong>Queries</strong> (\(Q\)) — what we’re looking for,</li> <li><strong>Keys</strong> (\(K\)) — where we’re looking,</li> <li><strong>Values</strong> (\(V\)) — what we retrieve.</li> </ul> <p>If we only had \(Q\) and \(K\), the model could measure similarity, but not transform or reweight information based on context. The \(V\) matrix introduces that flexibility — it allows the model to adapt the retrieved features.</p> <p>For each head, we compute <strong>scaled dot-product attention</strong>:</p> \[\text{Attention}(Q_h, K_h, V_h) = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k}}\right) V_h.\] <p><strong>Why softmax?</strong> Softmax turns unbounded similarity scores (logits) into a probability distribution where the attention weights sum to 1. This keeps outputs within a stable range and ensures the model focuses on the most relevant tokens.</p> <p><strong>Why scale by \(\sqrt{d_k}\)?</strong> Without scaling, when \(d_k\) is large, the dot product \(Q_h K_h^T\) can have high variance. This leads to extremely sharp softmax distributions, causing gradients to vanish and slowing training. Scaling by \(\sqrt{d_k}\) keeps gradients in a healthy range.</p> <p><strong>Combining heads</strong>: the outputs of all heads are concatenated and projected back to \(d_{\text{model}}\):</p> \[\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W^O,\] <p>where \(W^O \in \mathbb{R}^{H d_v \times d_{\text{model}}}\) is a learned projection matrix. Typically, we set \(d_k = d_v = d_{\text{model}} / H\) to keep the total computation manageable. For example, with \(d_{\text{model}} = 512\) and \(H = 8\), we get \(d_k = d_v = 64\).</p> <p><strong>Why this choice of dimensions?</strong></p> <ul> <li>If \(d_k\) and \(d_v\) stayed constant as \(H\) increased, the computation cost would grow quadratically: \(O(H n^2 d_k)\).</li> <li>By reducing them to \(d_{\text{model}} / H\), we keep the overall complexity at \(O(n^2 d_{\text{model}})\) — the same as single-head attention.</li> <li>The output projection \(W^O\) restores the dimensionality to \(d_{\text{model}}\), keeping it compatible with the rest of the Transformer stack.</li> </ul> <p><strong>Why this structure works:</strong></p> <ol> <li><strong>Subspace separation</strong>: Each head operates in its own \(d_k\)-dimensional subspace, letting the model learn <strong>independent types of interactions</strong>. One head might track noun-adjective agreement, another might attend to pronoun references. The projection matrices \(W_h^Q, W_h^K, W_h^V\) effectively decompose the original embeddings into interpretable components.</li> <li><strong>Parallelism</strong>: Independent heads allow for efficient parallel computation on GPUs.</li> <li><strong>Interpretability</strong>: After training, analyzing attention weights per head reveals the kinds of patterns each one has learned.</li> </ol> <p><strong>Example computation for a single head:</strong><br/> Let \(X\) be an embedding matrix of shape \(n \times d_{\text{model}}\). For head \(h\):</p> <ul> <li>\(Q_h = X W_h^Q\) → shape \(n \times d_k\)</li> <li>\(K_h = X W_h^K\) → shape \(n \times d_k\)</li> <li>\(V_h = X W_h^V\) → shape \(n \times d_v\)</li> </ul> <p>Then the attention matrix \(A_h = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k}}\right)\) (shape \(n \times n\)) is multiplied by \(V_h\), giving an output of shape \(n \times d_v\). Concatenating the outputs from all heads gives a \(n \times (H d_v)\) matrix, which is projected back to \(n \times d_{\text{model}}\) via \(W^O\).</p> <p>Preserving the <strong>dimensionality</strong> is critical: MultiHeadAttention outputs have the same shape \(d_{\text{model}}\) as the input, allowing seamless integration with other Transformer components (like normalization and feed-forward layers) without extra transformation. This consistency also helps stabilize gradients in deep models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">d_model</span><span class="p">,</span> <span class="sh">"</span><span class="s">d_model must be divisible by num_heads</span><span class="sh">"</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Q: [batch_size, num_heads, seq_len, head_dim]
</span>        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_k</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_v</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="feedforward">FeedForward</h2> <p>We need the position-wise Feed Forward layer in the Transformer to provide <strong>non-linear feature transformation</strong> after the attention step. While MultiHeadAttention effectively captures global dependencies between tokens, that alone isn’t enough for complex tasks like translation — the model also needs to combine the extracted patterns and transform them into new semantic representations.</p> <p>Each token in the sequence is processed <strong>independently</strong> through two linear layers. The first layer expands the dimensionality from \(d_{\text{model}}\) (e.g., 512) to \(d_{\text{ff}}\) (typically 2048), followed by a ReLU activation function:</p> \[\text{hidden} = \text{ReLU}(x W_1 + b_1),\] <p>where \(W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}\).</p> <p>Expanding the dimensionality by a factor of 4 (\(d_{\text{ff}} = 4d_{\text{model}}\)) gives the model enough capacity to learn non-obvious combinations of features. The second linear layer projects the representation back down to the original dimensionality:</p> \[\text{output} = \text{hidden} W_2 + b_2,\] <p>where \(W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}\). Dropout (typically with a rate like 0.1) is applied between the layers for regularization.</p> <p><strong>Why this design?</strong></p> <ul> <li><strong>Non-linearity</strong>: ReLU breaks linearity, enabling the model to approximate complex functions. Without it, the two linear layers would collapse into a single matrix multiplication.</li> <li><strong>Expansion and compression</strong>: Increasing the dimensionality creates a kind of “bottleneck” that forces the model to filter out noise and extract more abstract features. This is similar to how an autoencoder works — but without information loss, since the output returns to the original size.</li> <li><strong>Position-wise independence</strong>: Processing each token separately helps compensate for any local information that might be diluted by the global attention mechanism. For example, in the phrase “blue ball”, attention may link the adjective to the noun, but the FFN refines their joint representation into a vector encoding both color and shape.</li> </ul> <p>The input and output of the FeedForward layer both have the same dimensionality, \(d_{\text{model}}\), which allows for stacking multiple Encoder/Decoder blocks. Dropout and residual connections (implemented outside this layer) help stabilize training in deep networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="encoderlayer">EncoderLayer</h2> <p>Before the introduction of the <strong>EncoderLayer</strong> in the Transformer, researchers faced a dilemma: how to combine global contextual understanding with local feature transformation, while maintaining stable training in deep networks. Earlier approaches like RNNs suffered from vanishing gradients, and convolutional networks required many layers to capture long-range dependencies. Self-attention solved the problem of modeling global context — but on its own, it couldn’t provide deep, hierarchical feature transformation. This raised the question: how can we structure sequential transformations so the model first identifies relationships between tokens, then “rethinks” them, all while staying robust as the network grows deeper?</p> <p>The <strong>EncoderLayer</strong> was the answer — a module that combines two essential stages. First, the input embeddings \(x\), already enriched with positional information (via Positional Encoding), are passed through <strong>MultiHeadAttention</strong>. Here, each token “asks” the rest of the sequence:</p> \[\text{attn\_output} = \text{MultiHeadAttention}(x, x, x, mask),\] <p>where <code class="language-plaintext highlighter-rouge">mask</code> is used to ignore future tokens (in the decoder) or padding tokens. This lets the model, for instance, link the pronoun “he” to the correct noun, even if they’re separated by dozens of words. But attention is a <strong>linear</strong> operation in feature space. To introduce <strong>non-linearity and depth</strong>, we follow up with a <strong>Feed Forward Network (FFN)</strong> — two linear layers with an intermediate dimensionality expansion:</p> \[\text{ffn\_output} = \text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2.\] <p>The FFN acts like the model’s “thought process”: it transforms the global dependencies identified by attention into new semantic representations. For example, if attention links “apple” and “green,” the FFN can encode that as a combined vector representing both fruit and color.</p> <p>But simply chaining these operations wasn’t enough. Deep networks often “forgot” the original inputs — gradients vanished, and features got distorted. This is where <strong>residual connections</strong> and <strong>layer normalization</strong> came in. After each sub-step (attention or FFN), the layer adds the original input \(x\) to the output and applies normalization:</p> \[x = \text{LayerNorm}(x + \text{Dropout}(\text{sublayer}(x))).\] <p>Residuals act like bridges, allowing gradients and raw input information to flow freely through even dozens of layers. LayerNorm stabilizes activation distributions by computing the mean and variance across the \(d_{\text{model}}\) dimensions — preventing exploding or vanishing values.</p> <p><strong>Why this order?</strong> If the FFN came before attention, the ReLU non-linearity could “break” the positional information that’s critical for self-attention. And the use of <strong>post-layer normalization</strong> (after the residual connection) instead of pre-normalization (before the sub-step) wasn’t arbitrary: in the original Transformer, this design helped gradients flow through both the transformed path and the original input path, balancing parameter updates.</p> <p><strong>Example</strong>: An embedding for the word “bank” may, after attention, be linked to “river” (bank as a shore) or “money” (bank as a financial institution). The FFN then transforms these associations into a context-specific representation. Residuals and normalization ensure that the signal remains stable. As this process repeats across multiple EncoderLayers, the model refines the meaning iteratively — like rereading a sentence and noticing new details each time.</p> <p>Historically, the EncoderLayer became the blueprint for scalability. It could be stacked N times (e.g., 6 or 12 layers), enabling deep models without collapsing gradients. The combination of self-attention and FFN turned out to be so effective that even today’s large language models — like GPT-4 — retain this core structure, merely enhancing it with new mechanisms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Self attention
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="decoderlayer">DecoderLayer</h2> <p>If the EncoderLayer in a Transformer learns to <strong>understand</strong> the input text by compressing its context into dense vectors, then the <strong>DecoderLayer</strong> was designed to <strong>generate output</strong> — word by word — while taking into account both previous predictions and the encoder’s information. Early approaches like seq2seq with attention already connected encoder and decoder, but their recurrent nature limited parallelism and made it harder to model complex dependencies. In the Transformer, the decoder had to be <strong>autoregressive</strong>, yet still <strong>parallelizable</strong> — and this is where <strong>masked self-attention</strong> combined with <strong>cross-attention</strong> became crucial.</p> <p>A <strong>DecoderLayer</strong> starts with a partially generated output sequence (e.g., the translation generated up to the current word). To ensure that the model doesn’t “peek” at future tokens, it uses <strong>masked self-attention</strong>:</p> \[\text{attn\_output} = \text{MultiHeadAttention}(x, x, x, tgt\_mask),\] <p>where \(tgt\_mask\) is an upper-triangular matrix with \(-\infty\) in positions corresponding to future tokens. When passed through softmax, these become zeros — effectively blocking attention to future positions. For instance, while generating the third word, the mask hides all tokens beyond the third, forcing the model to rely only on previously generated context.</p> <p>But self-attention isn’t enough — the decoder also needs to <strong>relate the output to the input</strong>. This is where <strong>cross-attention</strong> comes in: queries (\(Q\)) come from the decoder, while keys (\(K\)) and values (\(V\)) are taken from the encoder output:</p> \[\text{cross\_attn\_output} = \text{MultiHeadAttention}(x, enc\_output, enc\_output, src\_mask).\] <p>Here, \(src\_mask\) hides padding tokens from the source sequence. This step acts like an “interrogation” of the encoder: the decoder asks which parts of the input are relevant at this step in the output. For example, when translating the word “apple,” the decoder can use cross-attention to link it to either “яблоко” or “компания,” depending on context.</p> <p>After cross-attention, as in the encoder, comes a <strong>Feed Forward Network</strong> to inject non-linearity:</p> \[\text{ffn\_output} = \text{FFN}(x).\] <p>Each sublayer is wrapped with <strong>residual connections</strong> and <strong>layer normalization</strong>:</p> \[x = \text{LayerNorm}(x + \text{Dropout}(\text{sublayer}(x))),\] <p>ensuring gradient stability even in very deep networks.</p> <p><strong>Why three stages?</strong></p> <ol> <li><strong>Masked self-attention</strong> isolates the already-generated portion of the sequence, simulating RNN-like autoregression.</li> <li><strong>Cross-attention</strong> synchronizes encoder and decoder, letting the decoder “look into” the input — akin to alignment in statistical machine translation.</li> <li><strong>FFN</strong> transforms the combined context into a decision — a final refinement before predicting the next token.</li> </ol> <p><strong>Example</strong>: When translating “I hit the bank” into Russian:</p> <ol> <li>Masked self-attention links “I hit” to “the,” while blocking future tokens.</li> <li>Cross-attention identifies whether “bank” aligns to “берег” (if the context is a river) or “банк” (if financial).</li> <li>The FFN processes this and outputs either “по берегу” or “в банк,” preserving grammar and meaning.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="c1"># Self attention (маскированное)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Cross attention (с выходом энкодера)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="transformer">Transformer</h2> <p>Once all the components of the Transformer — encoder, decoder, attention mechanisms, and positional encodings — were developed, the final task was to <strong>assemble them into a complete model</strong> capable of learning from sequence pairs (e.g., source text and translation). Early approaches like Seq2Seq already used an encoder-decoder separation, but their recurrent nature limited both parallelism and depth. The Transformer architecture, as implemented in code, emerged as a balance between expressiveness and computational efficiency.</p> <p><strong>Model assembly</strong> starts with turning tokens into vectors. The embeddings (<code class="language-plaintext highlighter-rouge">encoder_embedding</code> and <code class="language-plaintext highlighter-rouge">decoder_embedding</code>) map words into a \(d_{\text{model}}\)-dimensional space, while <code class="language-plaintext highlighter-rouge">positional_encoding</code> injects positional information:</p> <p>\(X_{\text{enc}} = \text{Embedding}(src) + \text{PositionalEncoding}(src),\)<br/> \(X_{\text{dec}} = \text{Embedding}(tgt) + \text{PositionalEncoding}(tgt).\)</p> <p>Without positional encoding, the model wouldn’t be able to distinguish between permutations of words, since self-attention alone is order-invariant.</p> <p>Next, the encoder and decoder are built as <strong>stacks of layers</strong> (<code class="language-plaintext highlighter-rouge">num_layers</code>). Each layer in the encoder (<code class="language-plaintext highlighter-rouge">EncoderLayer</code>) progressively refines the input representations: self-attention extracts global dependencies, the FFN adds non-linearity, and residual connections with layer normalization ensure stability. Similarly, the decoder (<code class="language-plaintext highlighter-rouge">DecoderLayer</code>) applies masked self-attention, cross-attention to the encoder output, and the FFN — in that order. Repeating these layers allows the model to iteratively refine its understanding, as if “rereading” the data at different levels of abstraction.</p> <p>The <strong>final layer</strong> (<code class="language-plaintext highlighter-rouge">fc_out</code>) projects from \(d_{\text{model}}\) to the size of the target vocabulary. This projection interprets decoder vectors as logits — scores for each token in the vocabulary:</p> \[\text{output} = W_{\text{out}} \cdot \text{dec\_output} + b_{\text{out}}.\] <p>A softmax (not explicitly shown in code but implied in the loss function) converts these logits into a probability distribution, from which the next word is sampled or selected.</p> <p><strong>Why this particular structure?</strong></p> <ul> <li><strong>Depth (<code class="language-plaintext highlighter-rouge">num_layers</code>)</strong>: Each layer captures different facets of the data. Early encoder layers may pick up syntax, while later ones capture semantics. In the decoder, lower layers focus on alignment with the encoder, while upper ones refine output grammar and fluency.</li> <li><strong>Separate embeddings</strong>: Using different embedding matrices for source and target languages allows the model to work effectively in multilingual settings.</li> <li><strong>Dimensional consistency</strong>: All components maintain the same dimensionality \(d_{\text{model}}\), which simplifies training — gradients flow freely through residual paths, and parameters update coherently.</li> <li></li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">src_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">encoder_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">src_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">decoder_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">tgt_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="testing">Testing</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_transformer</span><span class="p">():</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">src_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Generate synthetic data
</span>    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    
    <span class="c1"># Generate masks (example)
</span>    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># No masking
</span>    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Causal mask
</span>
    <span class="c1"># Initialize the model
</span>    <span class="n">transformer</span> <span class="o">=</span> <span class="nc">Transformer</span><span class="p">(</span>
        <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">src_vocab_size</span><span class="p">,</span>
        <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">tgt_vocab_size</span><span class="p">,</span>
        <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span>
    <span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">1. Positional Encoding Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Before PE: mean=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x_pe</span> <span class="o">=</span> <span class="nf">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">After PE: mean=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PE Shape: </span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should be [1, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s">])</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">2. Multi-Head Attention Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">mha</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">k</span> <span class="o">=</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="nf">mha</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Attention output shape: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Encoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">encoder_layer</span> <span class="o">=</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">enc_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">enc_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder output shape: </span><span class="si">{</span><span class="n">enc_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">enc_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Data changed: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span><span class="si">}</span><span class="s"> (should be False)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">4. Decoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">decoder_layer</span> <span class="o">=</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder output shape: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">dec_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Output norm: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="nf">norm</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">5. Full Transformer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input data:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">src: </span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tgt: </span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nf">transformer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Output shape check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Expected shape: (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">tgt_vocab_size</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Actual shape:   </span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Gradient check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">dummy_loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="n">dummy_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">has_gradients</span> <span class="o">=</span> <span class="nf">any</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Gradients computed: </span><span class="si">{</span><span class="n">has_gradients</span><span class="si">}</span><span class="s"> (should be True)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">6. Model Parameters Check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">encoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">decoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Test completed!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="machine-learning,"/><category term="transformers,"/><category term="multihead-attention,"/><category term="positional-encoding,"/><category term="nlp"/><summary type="html"><![CDATA[Learn and implement the most iconic architecture in modern deep learning.]]></summary></entry><entry><title type="html">Understanding Byte-Pair Encoding Algorithm</title><link href="https://xmarva.github.io/blog/2025/tokenization/" rel="alternate" type="text/html" title="Understanding Byte-Pair Encoding Algorithm"/><published>2025-04-01T15:00:00+00:00</published><updated>2025-04-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/tokenization</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/tokenization/"><![CDATA[<h1 id="tokenization">Tokenization</h1> <p><a href="https://www.kaggle.com/code/qmarva/1-bpe-tokenization-algorithm-eng?scriptVersionId=231677033"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat-square&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"/></a> <a href="https://colab.research.google.com/drive/1lmfuMdC8v-lXL_MuyC0uBewdLLCTQzCO?usp=sharing"><img src="https://img.shields.io/badge/Colab-F9AB00?style=flat-square&amp;logo=google-colab&amp;logoColor=white" alt="Colab"/></a></p> <p>Tokenization is a fundamental stage in natural language processing, the task of which is to split text into meaningful units (tokens).</p> <p>These units can be words, parts of words, or even characters. Historically, simple methods were used: splitting by spaces, regular expressions for extracting words and punctuation, manual rules for handling abbreviations. However, such approaches scaled poorly for languages with agglutinative morphology (e.g., Russian or Finnish) and complex word combinations.</p> <p>Traditional tokenization methods like space splitting or manual rules often prove ineffective in real-world scenarios: they struggle with typos, rare words, and multilingual texts. For example, words like “gooood” or mixed languages in a single sentence can break a classical tokenizer.</p> <p>In modern NLP, subword tokenization algorithms like <a href="https://arxiv.org/pdf/1508.07909">BPE (Byte Pair Encoding)</a> dominate, balancing the semantic integrity of tokens with efficient vocabulary usage. In this notebook, we will examine the BPE algorithm in detail and learn to work with tokenizers from the Hugging Face library.</p> <p>First, we will import all libraries and functions needed for this notebook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>
</code></pre></div></div> <hr/> <h2 id="loading-data">Loading Data</h2> <p>For demonstration, we will load the parallel English-Russian <a href="https://arxiv.org/abs/1812.10464">Tatoeba</a> corpus from Artetxe et al. (2019) via the <a href="http://huggingface.co/docs/datasets/loading">Hugging Face Datasets</a> library.</p> <p><a href="https://tatoeba.org/en/sentences/index">Tatoeba</a> is a free collection of translated example sentences for language learners, available in over 400 languages. Its name comes from the Japanese phrase «tatoeba» (例えば), meaning “for example.” It is written and maintained by a community of volunteers through open collaboration. Individual contributors are known as Tatoebans.</p> <p>We will use only the English and Russian subsets. All examples in this dataset are short everyday phrases: “Let’s try something.” → “Давайте что-нибудь попробуем!”.</p> <p>This format is convenient for training transformers, which work with sequences of limited length. In this notebook, we will not delve into transformer architecture but focus on text data preprocessing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_translation_dataset</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading Tatoeba en-ru...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Helsinki-NLP/tatoeba</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="sh">"</span><span class="s">en</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="sh">"</span><span class="s">ru</span><span class="sh">"</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error while loading dataset: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Dataset structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Data sample:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EN: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">RU: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
    
    <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <hr/> <h2 id="data-analysis">Data Analysis</h2> <p>Let’s take a quick look at the dataset to understand what we’re dealing with. We won’t dive deep into data analysis methods but will examine basic statistics.</p> <p>The <code class="language-plaintext highlighter-rouge">analyze_dataset</code> function shows that the average length of English sentences is 7.2 words, Russian — 6.2. The maximum lengths (30 and 28 words) indicate the presence of outliers that may require truncation.</p> <p>The histograms show right-skewed distributions: most sentences are shorter than 15 words. These observations influence model hyperparameter choices, e.g., <code class="language-plaintext highlighter-rouge">max_length=64</code> provides padding headroom even if actual sequences are shorter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>

    <span class="n">en_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">ru_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Analysis based on first </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s"> samples:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">English sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">English Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Russian Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="simple-tokenizer">Simple Tokenizer</h2> <p>Now we will write a <code class="language-plaintext highlighter-rouge">BaseTokenizer</code> class for text preprocessing, building a token vocabulary, and collecting token frequency statistics. This class will serve as the foundation for more complex tokenizers and provide a common structure for processing text data.</p> <p>We declare the class using the <a href="https://docs.python.org/3/library/dataclasses.html">@dataclass</a> decorator to auto-generate the constructor. Parameters we need: <code class="language-plaintext highlighter-rouge">language</code> (text language), <code class="language-plaintext highlighter-rouge">vocab_size</code> (max vocabulary size), <code class="language-plaintext highlighter-rouge">min_freq</code> (minimum frequency for including a token in the vocabulary), and <code class="language-plaintext highlighter-rouge">special_tokens</code> (list of special tokens).</p> <p>If <code class="language-plaintext highlighter-rouge">special_tokens</code> are not specified, default values are used: <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called immediately after object initialization. Here, we initialize the <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">id2token</code> dictionaries that map tokens to their numeric IDs. Special tokens must be added to the vocabulary first. For example, <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> gets ID 0, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> — 1, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">preprocess_text</code> method is used to preprocess text. We will convert text to lowercase and split it into tokens using a regular expression.</p> <p>The pattern <code class="language-plaintext highlighter-rouge">r"\w+[\w']*|['’][a-z]+|[^\w\s]"</code> captures:</p> <ul> <li>Words with apostrophes (e.g., <code class="language-plaintext highlighter-rouge">don't</code> → <code class="language-plaintext highlighter-rouge">["don't"]</code>).</li> <li>Contractions starting with an apostrophe (e.g., <code class="language-plaintext highlighter-rouge">'s</code> → <code class="language-plaintext highlighter-rouge">["'s"]</code>).</li> <li>Individual punctuation marks (e.g., <code class="language-plaintext highlighter-rouge">"!"</code> → <code class="language-plaintext highlighter-rouge">["!"]</code>).</li> </ul> <p>Note that the regex may not cover all edge cases (e.g., emojis or compound symbols), requiring modification for specific tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">get_stats</code> method collects token frequency statistics. For each text in the <code class="language-plaintext highlighter-rouge">examples</code> list, the <code class="language-plaintext highlighter-rouge">preprocess_text</code> function is called, then the <code class="language-plaintext highlighter-rouge">Counter</code> is updated.</p> <p>For example, the text <code class="language-plaintext highlighter-rouge">"Hello, world!"</code> returns a counter with keys <code class="language-plaintext highlighter-rouge">["hello", ",", "world", "!"]</code> and their frequencies. This method is used during tokenizer training to select tokens for the vocabulary based on <code class="language-plaintext highlighter-rouge">min_freq</code> and <code class="language-plaintext highlighter-rouge">vocab_size</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <p>Below, we consolidate all code into a single class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
</code></pre></div></div> <p>In reality, this basic approach has many drawbacks. For example, converting text to lowercase may lose case information. Additionally, this tokenization ignores word morphology, leading to issues with rare words or homonyms.</p> <p>Let’s write an <code class="language-plaintext highlighter-rouge">analyze_token_statistics</code> function to count unique tokens and their frequencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_stats</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Token statistics for </span><span class="si">{</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total unique tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Top 10 most frequent tokens:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">en_tokenizer</span><span class="p">)</span>
<span class="n">ru_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ru_tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>The difference in token counts for English (1337) and Russian (2065) stems from language features: Russian has richer morphology (endings, prefixes) and more word forms. The dominance of punctuation (. and , in the top) suggests the need for their pre-filtering or separate handling.</p> <p>Interestingly, the <code class="language-plaintext highlighter-rouge">"</code> token appears more frequently in English (146 times) — likely due to translation specifics in Tatoeba.</p> <p>Critically, this approach does not split words into subword units, leaving rare words intact and inflating vocabulary size. For comparison, we will explore the BPE tokenizer in subsequent experiments.</p> <hr/> <h2 id="bpe-tokenization-algorithm">BPE Tokenization Algorithm</h2> <p>Now let’s examine how the <strong>BPE (Byte Pair Encoding)</strong> tokenizer works. The core idea is to iteratively merge the most frequent character or token pairs, gradually forming a subword vocabulary. This efficiently handles rare and complex words by splitting them into known components.</p> <h3 id="bpetokenizer-class">BPETokenizer Class</h3> <p>First, declare the class with the <code class="language-plaintext highlighter-rouge">@dataclass</code> decorator. Since it inherits from <code class="language-plaintext highlighter-rouge">BaseTokenizer</code>, it already includes parameters <code class="language-plaintext highlighter-rouge">language</code>, <code class="language-plaintext highlighter-rouge">vocab_size</code>, <code class="language-plaintext highlighter-rouge">min_freq</code>, and <code class="language-plaintext highlighter-rouge">special_tokens</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
</code></pre></div></div> <h3 id="initialization">Initialization</h3> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called after object creation. Here we:</p> <ol> <li>Call the parent <code class="language-plaintext highlighter-rouge">__post_init__</code> to initialize base structures like <code class="language-plaintext highlighter-rouge">token2id</code>.</li> <li>Add a <code class="language-plaintext highlighter-rouge">merges</code> dictionary to store character pairs and their merged versions (e.g., <code class="language-plaintext highlighter-rouge">('h', 'e')</code> → <code class="language-plaintext highlighter-rouge">'he'</code>).</li> <li>Initialize <code class="language-plaintext highlighter-rouge">vocab</code> with special tokens.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>
</code></pre></div></div> <h3 id="generating-character-pairs">Generating Character Pairs</h3> <p>The <code class="language-plaintext highlighter-rouge">get_pairs</code> method splits a word into consecutive character pairs. For example, the word <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code> returns pairs <code class="language-plaintext highlighter-rouge">[('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')]</code>. These pairs are analyzed during training to find the most frequent combinations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div> <hr/> <h2 id="training-the-tokenizer">Training the Tokenizer</h2> <p>The <code class="language-plaintext highlighter-rouge">train</code> method is the core of BPE. It has several stages:</p> <p><strong>Collect Initial Statistics:</strong></p> <ul> <li>Split each token into characters and count character sequence frequencies. For example, token <code class="language-plaintext highlighter-rouge">"hello"</code> becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>, and its frequency increments the counter for <code class="language-plaintext highlighter-rouge">'h e l l o'</code>.</li> <li>Collect all unique characters from the text.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Adding Characters to Vocabulary:</strong></p> <ul> <li>Each unique character (e.g., <code class="language-plaintext highlighter-rouge">'h'</code>, <code class="language-plaintext highlighter-rouge">'e'</code>) is added to <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">vocab</code> if not already present. This ensures even individual characters have IDs.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Main Merge Loop:</strong></p> <ul> <li>Each iteration counts the frequency of all possible character pairs in the current word representations. For example, the word <code class="language-plaintext highlighter-rouge">'h e l l o'</code> has pairs <code class="language-plaintext highlighter-rouge">('h', 'e')</code>, <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, etc.</li> <li>Select the most frequent pair (e.g., <code class="language-plaintext highlighter-rouge">('l', 'l')</code> for <code class="language-plaintext highlighter-rouge">hello</code>) and create a new token <code class="language-plaintext highlighter-rouge">'ll'</code>.</li> <li>Update <code class="language-plaintext highlighter-rouge">merges</code>, <code class="language-plaintext highlighter-rouge">vocab</code>, <code class="language-plaintext highlighter-rouge">token2id</code>, and <code class="language-plaintext highlighter-rouge">id2token</code>.</li> <li>Recalculate word frequencies by replacing the selected pair with the new token. For example, <code class="language-plaintext highlighter-rouge">'h e l l o'</code> becomes <code class="language-plaintext highlighter-rouge">'h e ll o'</code> after merging <code class="language-plaintext highlighter-rouge">('l', 'l')</code>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
    <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="c1"># Count pair frequencies
</span>    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
        <span class="k">break</span>  <span class="c1"># No pairs left → stop
</span>
    <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

    <span class="c1"># Update vocabulary
</span>    <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

    <span class="c1"># Recalculate frequencies with new token
</span>    <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
        <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>
</code></pre></div></div> <p>If <code class="language-plaintext highlighter-rouge">num_merges</code> is too high and pairs are exhausted early, training stops. Progress is printed every 1000 iterations to track vocabulary growth.</p> <hr/> <h3 id="text-tokenization">Text Tokenization</h3> <p>The <code class="language-plaintext highlighter-rouge">tokenize</code> method converts text to token IDs:</p> <ol> <li>Text is split into tokens via <code class="language-plaintext highlighter-rouge">preprocess_text</code>.</li> <li>The <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> special token is prepended.</li> <li>For each token (e.g., <code class="language-plaintext highlighter-rouge">"hello"</code>): <ul> <li>Characters are split into <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>.</li> <li>Merges from <code class="language-plaintext highlighter-rouge">merges</code> are applied iteratively. For example, if <code class="language-plaintext highlighter-rouge">('l', 'l')</code> is in <code class="language-plaintext highlighter-rouge">merges</code>, the character list becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'll', 'o']</code>, then remaining pairs are checked.</li> <li>Unknown characters (e.g., <code class="language-plaintext highlighter-rouge">'#'</code>) are replaced with <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</li> </ul> </li> <li>The <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is appended.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
            <span class="c1"># Find first available merge pair
</span>            <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                    <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Replace pair with new token
</span>            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

        <span class="c1"># Add final symbols to result
</span>        <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>During tokenization, merges are applied left-to-right, and the <strong>first</strong> available pair from <code class="language-plaintext highlighter-rouge">merges</code> is chosen. This can yield different results depending on the merge order. For example, if <code class="language-plaintext highlighter-rouge">merges</code> contains <code class="language-plaintext highlighter-rouge">('h', 'e')</code> and <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, the first encountered pair is merged.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
                <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>

        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training BPE tokenizer for </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
            <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

            <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
                <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>

            <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Merges completed: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_merges</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

            <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                        <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

            <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>When applying the tokenizer to convert text to tokens, the algorithm first splits text into base characters, then iteratively merges character pairs using the built merge dictionary. Each word in the text is represented as a sequence of subwords (or tokens) created during training.</p> <p>The number of merges (<code class="language-plaintext highlighter-rouge">num_merges</code> parameter) determines how many times the algorithm will merge characters into new tokens. More merges create larger, more informative tokens. However, excessive merges can lead to loss of fine-grained details.</p> <p>This algorithm performs well with large text corpora and helps models handle rare or unseen words by replacing them with subwords from more frequent character combinations. Additionally, BPE works with any language, even those with unusual or complex alphabets, as it starts from base characters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">80000</span>
<span class="n">train_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
<span class="n">en_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>
<span class="n">ru_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>

<span class="n">en_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">en_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">ru_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">ru_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">English vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Russian vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ru_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span>
<span class="n">ru_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">en_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">en_sample</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ru_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">ru_sample</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">)</span>
</code></pre></div></div> <p>Overall, BPE effectively addresses rare and complex words, improving tokenization quality and NLP model performance.</p> <p>However, even after training, artifacts remain. For example, “useless” splits into [“us”, “el”, “ess”], and “бесполезно” into [“бес”, “пол”, “ез”, “но”]. This stems from the limited number of merges and the lack of explicit morpheme boundary consideration in our educational implementation.</p> <p>In production tokenizers (e.g., Hugging Face’s), such issues are mitigated by pretraining on massive corpora and tens of thousands of merges.</p> <hr/> <h2 id="batch-preparation">Batch Preparation</h2> <p>The <code class="language-plaintext highlighter-rouge">prepare_batch</code> function converts tokenized sequences into tensors suitable for training. Each sentence is padded to a fixed length (<code class="language-plaintext highlighter-rouge">max_length=64</code>) with the <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token, and attention masks tell the model to ignore these “empty” positions.</p> <p>For example, a sentence with 24 tokens becomes a vector of length 64, where the last 40 elements are zeros (ID <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>). Masking is critical for transformers, as the attention mechanism would otherwise account for meaningless padding tokens, distorting weights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
                 <span class="n">src_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">tgt_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="n">src_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">src_texts</span><span class="p">]</span>
    <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">tgt_texts</span><span class="p">]</span>

    <span class="n">src_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">src_masks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_masks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>

        <span class="n">src_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_pad</span><span class="p">)</span>
        <span class="n">tgt_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_pad</span><span class="p">)</span>
        <span class="n">src_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_mask</span><span class="p">)</span>
        <span class="n">tgt_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_masks</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_masks</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="tokenizer-verification">Tokenizer verification</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Prepared batch shapes:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Example source tokens:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Corresponding mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">base_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Base tokenization: </span><span class="si">{</span><span class="n">base_tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Number of merges learned: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample merges (first 5):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">merged</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">merged</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample vocabulary items (first 10):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final tokenization:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoded tokens: </span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing English tokenizer:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr/> <h2 id="hugging-face-tokenizers">Hugging Face Tokenizers</h2> <p>All this seems quite complex. Our current tokenizer works imperfectly and is slow. Fortunately, programmers avoid reinventing the wheel. In practice, it’s much easier to use a ready-made tokenizer via <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> from the <code class="language-plaintext highlighter-rouge">transformers</code> library.</p> <p>The <code class="language-plaintext highlighter-rouge">opus-mt-en-ru</code> model already has a pretrained BPE vocabulary optimized for the language pair. The tokenizer automatically adds special tokens, handles case, and rare symbols. When processing the dataset, the <code class="language-plaintext highlighter-rouge">map</code> function applies tokenization in parallel to all examples, speeding up work via batching.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Helsinki-NLP/opus-mt-en-ru</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
<span class="p">):</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="n">source_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>
        <span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>

        <span class="n">source_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">source_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="n">target_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">target_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">decoder_attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span>
        <span class="n">preprocess_function</span><span class="p">,</span>
        <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">column_names</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="comparing-tokenizers">Comparing Tokenizers</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Custom BPE Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of prepared batches:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">array</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (dtype: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sample data from first batch:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Source tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Target tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Source mask (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hugging Face Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset features: </span><span class="si">{</span><span class="n">processed_dataset</span><span class="p">.</span><span class="n">features</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of examples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">first_example</span> <span class="o">=</span> <span class="n">processed_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">First example details:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input IDs shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded input:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Labels shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded labels:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention mask sample:</span><span class="sh">"</span><span class="p">,</span> <span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>


<span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="tokenization,"/><category term="bpe,"/><category term="algorithms,"/><category term="nlp"/><summary type="html"><![CDATA[Implement one of the most popular tokenization algorithms and learn how to use ready-made solutions.]]></summary></entry><entry><title type="html">Can AI Achieve True Creativity?</title><link href="https://xmarva.github.io/blog/2025/creative-ai/" rel="alternate" type="text/html" title="Can AI Achieve True Creativity?"/><published>2025-02-13T15:00:00+00:00</published><updated>2025-02-13T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/creative-ai</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/creative-ai/"><![CDATA[<p>The criticism that AI cannot create something fundamentally new often overlooks a key fact: human creativity itself is built on combining existing knowledge, concepts, and imagery.</p> <p>Modern research reveals that when humans invent something novel, the brain doesn’t conjure elements “from nothing” but recombines fragments of prior experiences. Neural networks operate similarly.</p> <p>When GPT-4 generates text, it relies on statistical patterns learned from existing data rather than conscious intent.</p> <h3 id="the-brains-creative-networks-vs-gans">The Brain’s Creative Networks vs. GANs</h3> <p>In one experiment, participants were asked to devise unconventional uses for everyday objects (e.g., a coffee cup). fMRI scans showed two activated networks during creativity:</p> <ul> <li>The default mode network (associated with daydreaming and associations)</li> <li>The frontoparietal network (linked to focus and control)</li> </ul> <p>The dorsolateral prefrontal cortex also activates during musical improvisation. This interaction mirrors generative adversarial networks (GANs), where one component generates ideas and another evaluates their plausibility. Both biological and artificial systems balance freedom and constraints to produce novelty.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/0-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/0-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="fMRI activation during creative tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fMRI activation patterns during creative tasks (Source: Beaty et al., 2016) </div> <h3 id="the-limits-of-perception-and-data">The Limits of Perception and Data</h3> <p>Human perception constrains creativity. We can’t imagine colors beyond the visible spectrum—any new shade is a recombination of known hues. Similarly, Stable Diffusion can’t generate images of objects absent from its training data. It blends learned features, much like the brain combines memories.</p> <p>Blind individuals describe colors through analogies to sounds or textures (e.g., red as “loud noise,” blue as “smooth surface”). This suggests creativity is rooted in sensory experience—a dimension AI lacks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/1-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/1-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stable Diffusion-generated floral carpet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Floral carpet generated by Stable Diffusion </div> <h3 id="embodied-vs-abstract-learning">Embodied vs. Abstract Learning</h3> <p>A key difference lies in how humans and AI acquire knowledge:</p> <ul> <li>Humans learn through physical interaction (e.g., toddlers touching objects to link tactile and visual data)</li> <li>AI processes abstract tokens/pixels</li> </ul> <p>While LLMs develop semantic relationships (e.g., “apple” vectors near “fruit” and “tree”), they lack embodied experiences stored in the brain’s sensorimotor cortex. When we think “run,” motor neurons activate—a connection AI can’t replicate.</p> <h3 id="the-myth-of-intentional-creativity">The Myth of Intentional Creativity</h3> <p>Critics argue humans possess creative “intent,” but jazz improvisation studies show decreased prefrontal cortex activity during spontaneous creation. Ideas emerge automatically from learned patterns—a process strikingly similar to how neural networks operate.</p> <h3 id="the-originality-debate">The Originality Debate</h3> <p>If human innovation is recombination, demanding absolute novelty from AI is flawed. Both are constrained by their “training data”:</p> <ul> <li>Human brains: Biological experiences</li> <li>AI models: Digital datasets</li> </ul> <p>The distinction lies in complexity and emotional embodiment. While AI manipulates mathematical structures, the brain ties patterns to emotions and bodily states—for now.</p> <p><strong>References</strong><br/> [1] Beaty, R. E., Benedek, M., Silvia, P. J., &amp; Schacter, D. L. (2016). Creative Cognition and Brain Network Dynamics. <em>Trends in Cognitive Sciences</em><br/> [2] De Borst, A. W., &amp; de Gelder, B. (2018). Mental Imagery Follows Similar Cortical Reorganization as Perception: Intra-Modal and Cross-Modal Plasticity in Congenitally Blind. <em>Cerebral Cortex</em><br/> [3] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dea, J. (2013). Distributed Representations of Words and Phrases and their Compositionality<br/> [4] Limb, C. J., &amp; Braun, A. R. (2008). Neural Substrates of Spontaneous Musical Performance: An fMRI Study of Jazz Improvisation</p>]]></content><author><name></name></author><category term="featured-posts"/><category term="ai,"/><category term="neuroscience,"/><category term="creativity"/><summary type="html"><![CDATA[Exploring the parallels between human creativity and neural networks]]></summary></entry><entry><title type="html">Algebraic Foundations of Low-Rank Adaptation</title><link href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/" rel="alternate" type="text/html" title="Algebraic Foundations of Low-Rank Adaptation"/><published>2024-12-30T15:09:00+00:00</published><updated>2024-12-30T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/lora-algorithm-for-llms</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><![CDATA[<h2 id="the-paradox-of-scale">The Paradox of Scale</h2> <p>The evolution of language models presents us with an intriguing paradox: while increasing model size enhances general capability, it simultaneously complicates practical deployment through prohibitive computational demands. This tension between capacity and practicality forms the crucible where Low-Rank Adaptation (LoRA) emerges as an elegant solution. To understand its mechanisms, we must first establish fundamental mathematical constructs.</p> <h2 id="matrix-theory-foundations">Matrix Theory Foundations</h2> <h3 id="the-algebraic-scaffolding">The Algebraic Scaffolding</h3> <p>A matrix \(A \in \mathbb{R}^{m \times n}\) represents a linear transformation between vector spaces \(\mathbb{R}^n \to \mathbb{R}^m\). Each element \(a_{ij}\) encodes the transformation coefficient between basis vectors \(e_j\) and \(e_i\). In neural networks, these matrices become learned representations of feature interactions.</p> <p>The <strong>rank</strong> of a matrix, denoted \(\rho(A)\), measures its column space dimensionality through the maximal number of linearly independent columns. Formally:</p> \[\rho(A) = \dim(\text{col}(A)) = \dim(\text{row}(A))\] <p>This duality between row and column space dimensionalities (proven via the Fundamental Theorem of Linear Algebra) becomes crucial for understanding parameter efficiency.</p> <h3 id="rank-constrained-transformations">Rank-Constrained Transformations</h3> <p>Consider two matrices \(B \in \mathbb{R}^{m \times r}\) and \(A \in \mathbb{R}^{r \times n}\). Their product \(BA\) inherently satisfies:</p> \[\rho(BA) \leq \min(\rho(B), \rho(A)) \leq r\] <p>This rank upper bound enables dramatic parameter reduction when \(r \ll \min(m,n)\). For a neural layer with \(m \times n\) weights, replacing full updates with low-rank factors reduces trainable parameters from \(mn\) to \(r(m+n)\) – an efficiency gain of \(\frac{mn}{r(m+n)}\). For typical layers (\(m,n \sim 10^3\), \(r \sim 10^1\)), this yields ~100x parameter reduction.</p> <h2 id="the-low-rank-adaptation-hypothesis">The Low-Rank Adaptation Hypothesis</h2> <h3 id="intrinsic-dimensionality-of-task-adaptation">Intrinsic Dimensionality of Task Adaptation</h3> <p>Modern language models exhibit an intriguing property: while pretrained on broad corpora, task-specific adaptation appears to operate in low-dimensional subspaces. This phenomenon aligns with the <strong>manifold hypothesis</strong>, suggesting high-dimensional data actually resides on lower-dimensional manifolds.</p> <p>Let \(\Delta W \in \mathbb{R}^{m \times n}\) represent weight updates during fine-tuning. The LoRA conjecture posits:</p> \[\rho(\Delta W) \leq r \ll \min(m,n)\] <p>Experimental validation shows task adaptation often requires surprisingly low ranks (\(r=8\) achieves strong performance). This implies that while the original parameter space is vast, task-specific adjustments occupy a small subspace.</p> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Visualize the weight matrix as a point in \(\mathbb{R}^{mn}\). Full fine-tuning moves this point through the high-dimensional space. LoRA constrains movement to a low-dimensional <strong>adaptation manifold</strong> spanned by \(B\) and \(A\):</p> \[\mathcal{M}_r = \{ W + BA \mid B \in \mathbb{R}^{m \times r}, A \in \mathbb{R}^{r \times n} \}\] <p>The approximation error is bounded by the Eckart–Young theorem:</p> \[\min_{\rho(BA)\leq r} \| \Delta W - BA \|_F = \sum_{i=r+1}^{\min(m,n)} \sigma_i(\Delta W)\] <p>where \(\sigma_i\) denotes singular values. Rapidly decaying singular values in \(\Delta W\) (as observed empirically) enable accurate low-rank approximation.</p> <h2 id="algorithmic-implementation">Algorithmic Implementation</h2> <h3 id="parameterization-and-initialization">Parameterization and Initialization</h3> <p>For a pretrained weight matrix \(W_0\), LoRA constructs:</p> \[W = W_0 + \frac{\alpha}{r}BA\] <p>Where:</p> <ul> <li>\(B\) initialized with \(\mathcal{N}(0, \sigma^2)\)</li> <li>\(A\) initialized to zero</li> <li>\(\alpha\): learning rate scaling factor</li> </ul> <p>The initialization strategy ensures \(\Delta W = 0\) at training onset, preserving original model behavior. The \(\alpha/r\) scaling normalizes parameter updates across different ranks, maintaining stable learning dynamics.</p> <h3 id="gradient-dynamics">Gradient Dynamics</h3> <p>Let \(\mathcal{L}\) be the loss function. The gradient through the LoRA parameters becomes:</p> \[\nabla_B \mathcal{L} = \frac{\alpha}{r} (\nabla_{W} \mathcal{L}) A^T \\ \nabla_A \mathcal{L} = \frac{\alpha}{r} B^T (\nabla_{W} \mathcal{L})\] <p>This reveals an important property: gradient signals flow through both low-rank factors, with the scaling term modulating update magnitudes. The rank \(r\) therefore acts as a gradient multiplier – higher ranks enable stronger gradient signals but increase parameter count.</p> <h2 id="practical-considerations-and-variations">Practical Considerations and Variations</h2> <h3 id="rank-selection-tradeoffs">Rank Selection Tradeoffs</h3> <p>The choice of \(r\) balances expressivity vs efficiency:</p> <ul> <li><strong>Lower ranks (r=1-4):</strong> Maximize parameter efficiency, suitable for similar source/target tasks</li> <li><strong>Medium ranks (r=8-16):</strong> General-purpose setting for domain adaptation</li> <li><strong>Higher ranks (r=32+):</strong> Needed for complex task transfers or low-data scenarios</li> </ul> <p>Empirical studies show performance follows logarithmic scaling:</p> \[\text{Performance}(r) \approx \text{Performance}(\text{full}) - c/\log r\] <p>Where \(c\) is task-dependent. This suggests diminishing returns beyond certain ranks.</p> <h3 id="architectural-variants">Architectural Variants</h3> <ol> <li><strong>Bottleneck Adaptation:</strong> Stack multiple low-rank layers (\(W_0 + B_1A_1 + B_2A_2\)) for hierarchical adaptation</li> <li><strong>Sparse LoRA:</strong> Combine with magnitude pruning on \(BA\) product</li> <li><strong>Dynamic Rank Allocation:</strong> Use singular value thresholds to automatically select per-layer ranks</li> <li><strong>LoRA++:</strong> Introduce learned scaling factors per layer instead of fixed \(\alpha/r\)</li> </ol> <h3 id="compositional-adaptation">Compositional Adaptation</h3> <p>For multi-task learning, LoRA enables parameter composition:</p> \[W = W_0 + \sum_{k=1}^K B_kA_k\] <p>Where each \(B_kA_k\) captures task-specific adaptations. During inference, select subsets of adapters via:</p> \[W = W_0 + \sum_{k \in S} B_kA_k\] <p>This facilitates efficient multi-task serving with \(\mathcal{O}(Kr)\) storage instead of \(\mathcal{O}(K)\) full models.</p> <h2 id="theoretical-implications">Theoretical Implications</h2> <h3 id="implicit-regularization">Implicit Regularization</h3> <p>The low-rank constraint acts as a strong regularizer, preventing overfitting to small datasets. Consider the Rademacher complexity for a LoRA-adapted layer:</p> \[\mathcal{R}_n(\mathcal{H}_{\text{LoRA}}) \leq \frac{\alpha \sqrt{2r\log(2mn)}}{n}\] <p>Compared to full fine-tuning’s \(\mathcal{O}(\sqrt{mn/n})\) complexity, LoRA’s bound is significantly tighter, explaining its improved generalization in low-data regimes.</p> <h3 id="information-bottleneck-perspective">Information Bottleneck Perspective</h3> <p>Interpreting through the information bottleneck lens, LoRA enforces:</p> \[\min_{B,A} I(W; BA) \quad \text{s.t.} \quad I(BA; \mathcal{T}) \geq I_c\] <p>Where \(\mathcal{T}\) is the target task and \(I_c\) the required information. The low-rank structure naturally minimizes irrelevant information from \(W\) while preserving task-relevant features.</p> <h2 id="epilogue">Epilogue</h2> <p>LoRA epitomizes the principle that profound solutions often arise from deep mathematical insight rather than brute-force computation. By reconceptualizing adaptation as a low-rank update process, it achieves an elegant synthesis of efficiency and effectiveness – a reminder that in machine learning as in mathematics, constraints often breed creativity.</p> <p>The road ahead suggests intriguing possibilities: could other matrix properties (e.g., sparsity patterns, eigenvalue distributions) inspire new adaptation paradigms? As language models continue evolving, such algebraic perspectives will likely remain essential tools for harnessing their potential.</p>]]></content><author><name></name></author><category term="nlp,"/><category term="llm,"/><category term="lora"/><summary type="html"><![CDATA[Mathematical exploration of parameter-efficient fine-tuning through matrix rank theory]]></summary></entry><entry><title type="html">LLMs for Those Who Missed Out</title><link href="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/" rel="alternate" type="text/html" title="LLMs for Those Who Missed Out"/><published>2024-04-24T15:09:00+00:00</published><updated>2024-04-24T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/"><![CDATA[<p>An LLM (large language model) consists of just two files:</p> <ul> <li>Large file with billions of parameters (weights)</li> <li>Small file with about 500 lines of code to run the model</li> </ul> <p>LLM doesn’t take up much space and doesn’t require extensive computing power.</p> <p>For example, the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama-2–70b model</a> from Meta, which is open source and has 70 billion parameters, is just 140 GB and can be run locally on a MacBook without internet access. You can <a href="https://www.llama.com/llama-downloads/">download this model</a> from Meta’s website and use it for free.</p> <p>However, you need significant computing resources to get these parameters (train the model).</p> <h2 id="how-to-train-a-model">How to Train a Model</h2> <h3 id="stage-1-pretraining-training-the-base-model">Stage 1: Pretraining (Training the base model)</h3> <p>Think of this process as compressing the Internet into a neural network, similar to how a ZIP file compresses documents into an archive.</p> <p>Here’s what you need to get a model like Llama-2:</p> <ul> <li>Take a “slice of the internet,” approximately ten terabytes of text in size</li> <li>Use a cluster of ~6,000 GPUs (specialized graphics processors used for heavy computations)</li> <li>Compress the text into the neural network. This takes about 12 days and costs around $2M</li> <li>Acquire the base model (the file with parameters)</li> </ul> <p>The primary function of the base model is to predict the next word. You input a sequence of words, and it outputs the next most likely word based on the texts it was trained on.</p> <p>For example, for the phrase “cat sat on a,” the LLM might predict the word “mat” with 97% probability.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/0-480.webp 480w,/assets/img/posts/2024-04-24-llms/0-800.webp 800w,/assets/img/posts/2024-04-24-llms/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image from the presentation illustrating how the neural network works" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image from the presentation illustrating how the neural network works </div> <p>That’s basically how compression works: if you can predict the next word accurately, you can use this information to compress the original data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/1-480.webp 480w,/assets/img/posts/2024-04-24-llms/1-800.webp 800w,/assets/img/posts/2024-04-24-llms/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="I checked, and ChatGPT indeed responds with “mat”" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> I checked, and ChatGPT indeed responds with “mat” </div> <p>However, unlike a ZIP file, where compression is lossless, LLM “compresses” the internet with losses. This is why models can hallucinate: they make up information that doesn’t actually exist but appears plausible (similar to what they have seen in the training data).</p> <p>Moreover, models can sometimes produce completely unexpected things. For example, ChatGPT knows the answer to the question, “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer), but it doesn’t know the answer to the question, “Who is Mary Lee Pfeiffer’s son?”</p> <p>This isn’t a typical database that simply stores information; it’s a different format we don’t fully understand.</p> <p>This clearly illustrates that we don’t quite grasp how this whole thing works and can only see the results it produces.</p> <h3 id="stage-2-finetuning-additional-training">Stage 2: Finetuning (Additional training)</h3> <p>The base model isn’t very practical for everyday use. We don’t just want to receive continuous word sequences; we want to ask questions and get answers.</p> <p>This requires finetuning — a process in which we develop an assistant model that answers questions.</p> <p>The training process for the assistant model is similar to that of the base model, but now we train the model not on internet texts but on data we manually collect. Companies hire people who write questions and answer them.</p> <p>If the training of the base model occurs on huge volumes of often low-quality text, the training of the assistant model involves comparatively smaller volumes (say, 100,000 documents), but they are all of very high quality.</p> <p>After such finetuning, the model understands that when asked a question, it should respond in the style of a helpful assistant.</p> <p>Unlike the training of the base model, which is conducted at most 2–3 times a year, finetuning can be done regularly, even weekly, as it is much less costly.</p> <h3 id="stage-3-optional-comparisons">Stage 3 (optional): Comparisons</h3> <p>In many cases it’s easier not to write a response from scratch but to compare several different answer options and choose the best one.</p> <p>The data from these comparisons is used to train the model further.</p> <p>At OpenAI, this is called RLHF (Reinforcement Learning from Human Feedback).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/2-480.webp 480w,/assets/img/posts/2024-04-24-llms/2-800.webp 800w,/assets/img/posts/2024-04-24-llms/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Difference between Open and Closed Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Difference between Open and Closed Models. Source: Chatbot leaderboards </div> <p>Models with open weights, such as Llama-2, are still inferior compared to proprietary models like GPT-4 and Claude.</p> <p>However, proprietary models cannot be downloaded for personal use or further training; they can only be operated through a web interface or an API (though the introduction of Custom GPTs at OpenAI is a first step towards customization)</p> <h2 id="capabilities-of-llms-using-chatgpt-as-an-example">Capabilities of LLMs (using ChatGPT as an example)</h2> <p>Prompted by an LLM, it can understand what the user needs and use external tools for this:</p> <ul> <li>Search the internet if the user needs up-to-date information (goes to Bing, retrieves a list of the latest links for the query, copies the full text of each link into its context window, and summarizes the information)</li> <li>Use a calculator to do calculations</li> <li>Use a Python library to draw graphs</li> <li>Draw images using DALL-E</li> <li>Write code</li> </ul> <p>Moreover, LLMs can be multimodal. They can recognize text, voice, images, or video and respond with voice, images, or video.</p> <h2 id="can-llms-make-decisions">Can LLMs Make Decisions?</h2> <p>There’s a well-known book by Kahneman, “Thinking, Fast and Slow.”</p> <p>The main idea of the book is that there are two systems in the brain: System 1, which has fast, automatic reactions, and System 2, which is slower, rational, and conscious and makes complex decisions.</p> <p>For example, the answer to question 2+2 comes from System 1 because this knowledge is automatic for us. But calculating 17×24 requires System 2.</p> <p>If we apply these terms, current LLMs only possess System 1. They can only provide the most likely next word in real time.</p> <p>It would be great if we could come to an LLM and say: here’s my question; you can think for 30 minutes, but I need a very accurate and high-quality answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/3-480.webp 480w,/assets/img/posts/2024-04-24-llms/3-800.webp 800w,/assets/img/posts/2024-04-24-llms/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Thought tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Thought tree </div> <p>No models can do that yet. But it would be desirable for a model to have something like a “thought tree,” through which it could navigate, analyze the results, go back and try again until it achieves a result it is most confident about.</p> <h2 id="can-llms-train-themselves">Can LLMs Train Themselves?</h2> <p>There’s a famous case where AlphaGo (a program that plays Go) was trained in two stages:</p> <p>First, it was trained on human games and learned to play very well. Then it began to train itself-playing against itself, trying to maximize the likelihood of winning-and significantly improved its quality.</p> <p>But in LLMs, we’re only at stage 1-training only occurs on materials created by humans.</p> <p>Why is this?</p> <p>In the game of Go, there is a very clear criterion for success-a won game- and you can train the model to maximize the likelihood of winning.</p> <p>In LLMs, the criteria are not so obvious. It’s not clear how to assess whether the result is good.</p> <p>Such criteria can be found in some narrow topics, but in general, it’s still hard to imagine.</p> <h2 id="what-llms-will-be-able-to-do-in-a-few-years">What LLMs Will Be Able to Do in a Few Years</h2> <p>Finally, a small forecast.</p> <p>In a few years, LLMs:</p> <ul> <li>Will have more knowledge than any human on all subjects</li> <li>Can operate on the internet</li> <li>Can use existing software infrastructure (calculator, Python, mouse/keyboard)</li> <li>Can see and generate images and videos</li> <li>Can hear and speak, and also generate music</li> <li>Can think for a long period using System 2</li> <li>Can train themselves in areas where there are clear criteria</li> <li>Can be customized and refined for specific tasks. Many versions will exist in app stores</li> <li>Can interact with other LLMs</li> </ul>]]></content><author><name></name></author><category term="old-posts"/><category term="nlp,"/><category term="llm"/><summary type="html"><![CDATA[Let's talk about large language models. Once again.]]></summary></entry><entry><title type="html">How to Write Good Python Code</title><link href="https://xmarva.github.io/blog/2023/python-code/" rel="alternate" type="text/html" title="How to Write Good Python Code"/><published>2023-02-07T15:09:00+00:00</published><updated>2023-02-07T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2023/python-code</id><content type="html" xml:base="https://xmarva.github.io/blog/2023/python-code/"><![CDATA[<p>Python is a fantastic programming language!</p> <p>It can be used for many things, like building websites, exploring data, and teaching machines to learn.</p> <p>If you already know Python or are just beginning, writing code that is strong, easy to read, and easy to keep up with is important.</p> <p>In this bogpost, we’ll look at the basic rules for writing great Python code and share some tips to help you make your programs even better.</p> <h2 id="-use-meaningful-naming-conventions">📚 Use Meaningful Naming Conventions</h2> <p>One of the most important aspects of good Python code is meaningful naming conventions.</p> <p>Choosing descriptive and concise names for variables, functions, and classes can help make your code more readable and understandable.</p> <p>Using proper naming conventions can also help you avoid naming conflicts, reduce the risk of errors, and simplify maintenance.</p> <p>For example, these are bad variable names:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>And these are better ones:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_number</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">second_number</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sum_of_numbers</span> <span class="o">=</span> <span class="n">first_number</span> <span class="o">+</span> <span class="n">second_number</span>
<span class="n">double_sum</span> <span class="o">=</span> <span class="n">sum_of_numbers</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>This includes using proper indentation, white space, line breaks, and following a code style guide like the PEP 8 style guide.</p> <p>Clear, organized code makes it easier to understand and modify and reduces the risk of errors.</p> <p>Here’s an example of lousy code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sum: </span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Difference: </span><span class="sh">"</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Product: </span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Quotient: </span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
</code></pre></div></div> <p>And here’s an example of good code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Sum</span><span class="sh">"</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Difference</span><span class="sh">"</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Product</span><span class="sh">"</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Quotient</span><span class="sh">"</span><span class="p">:</span> <span class="n">q</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h2 id="-write-comments">💬 Write Comments</h2> <p>Adding comments to your code is a great way to explain what it does and provide context for other developers.</p> <p>Comments should be used to explain complex code, provide additional information about the purpose of the code, and describe your thought process.</p> <p>Writing comments can also help you better understand your code when you return to it later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># function to calculate sum
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># this function calculates sum of two numbers
</span><span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments are not very descriptive or helpful in understanding the purpose of the functions.</p> <p>The first comment is trivial and adds no additional information. The second comment repeats what the function name already tells us.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the sum of two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns their sum.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the sum of `a` and `b`
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the difference between two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns the difference of `a` and `b`.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the difference between `a` and `b`
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments provide a clear and concise explanation of the purpose and behaviour of each function.</p> <p>The use of docstrings makes it easy to understand what the functions do and what arguments they take in. This makes the code more readable and maintainable.</p> <h2 id="-use-modules-and-packages">🧰 Use Modules and Packages</h2> <p>Modules and packages are a great way to organize your code into reusable blocks.</p> <p>They allow you to group related code together and make it easier to manage, understand, and maintain.</p> <p>The Python Standard Library is an good resource for finding pre-existing modules and packages. You can import it into your programs to save time and effort.</p> <p>Consider a project to build a simple weather application that provides a given city’s current temperature and conditions. We can structure the project as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weather_app/
    __init__.py
    weather.py
    utils/
        __init__.py
        api.py
        data_processing.py
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">weather.py</code> is the main module that the user interacts with, which provides a single function to get the current weather information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_current_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Gets the current weather information for the given city.

    Args:
        city (str): The city for which to get the weather information.

    Returns:
        dict: The weather information for the given city.
    </span><span class="sh">"""</span>
    <span class="n">weather_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">api</span><span class="p">.</span><span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>
    <span class="n">processed_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">data_processing</span><span class="p">.</span><span class="nf">process_weather_data</span><span class="p">(</span><span class="n">weather_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <p>The utils package contains two modules, <code class="language-plaintext highlighter-rouge">api.py</code> and <code class="language-plaintext highlighter-rouge">data_processing.py</code>, which contain helper functions to retrieve the raw weather data from an API and to process the raw data into a more readable format, respectively.</p> <p>These modules can be reused across different projects, so it makes sense to organize them into a separate package.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># api.py
</span><span class="k">def</span> <span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Retrieves the raw weather data for the given city.

    Args:
        city (str): The city for which to retrieve the weather data.

    Returns:
        dict: The raw weather data for the given city.
    </span><span class="sh">"""</span>
    <span class="c1"># code to retrieve data from API
</span>    <span class="k">return</span> <span class="n">raw_data</span>

<span class="c1"># data_processing.py
</span><span class="k">def</span> <span class="nf">process_weather_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Processes the raw weather data into a more readable format.

    Args:
        raw_data (dict): The raw weather data.

    Returns:
        dict: The processed weather data.
    </span><span class="sh">"""</span>
    <span class="c1"># code to process data
</span>    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <h2 id="-test-your-code">🧪 Test Your Code</h2> <p>Testing your code helps you catch bugs and ensure that your code works as expected.</p> <p>Writing test cases is also an good way to document your code and help others understand it. Try all possible scenarios when testing your code, including edge cases and error conditions.</p> <p>Consider a module <code class="language-plaintext highlighter-rouge">calculator.py</code> that implements a simple calculator with basic arithmetic operations. We can write test cases for each operation using a testing framework such as <code class="language-plaintext highlighter-rouge">unittest</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">unittest</span>
<span class="kn">import</span> <span class="n">calculator</span>

<span class="k">class</span> <span class="nc">TestCalculator</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_addition</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_subtraction</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_multiplication</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_division</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">unittest</span><span class="p">.</span><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <p>Each test case tests a single operation in the calculator module and uses the <code class="language-plaintext highlighter-rouge">assertEqual</code> method to verify that the result of the operation is as expected.</p> <p>If any test fails, an error will be raised, and the test result will be reported as failed.</p> <p>For debugging we can use the <code class="language-plaintext highlighter-rouge">print</code> statement to print the intermediate results or the values of variables in the code, or use a debugger such as <code class="language-plaintext highlighter-rouge">pdb</code> to step through the code and inspect the values of variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">calculator</span>
<span class="kn">import</span> <span class="n">pdb</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span> <span class="c1"># Set a breakpoint
</span><span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-document-your-code">📜 Document Your Code</h2> <p>Documenting your code with docstrings can help others understand what it does and how it works.</p> <p>Docstrings should provide a high-level overview of the code, including its purpose, usage, and limitations.</p> <p>They should also be written in a clear and natural language style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Circle</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Class to represent a circle with a given radius.

    Attributes:
        radius (float): The radius of the circle.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the Circle class with a given radius.

        Args:
            radius (float): The radius of the circle.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">=</span> <span class="n">radius</span>

    <span class="k">def</span> <span class="nf">area</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the area of the circle.

        Returns:
            float: The area of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">circumference</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the circumference of the circle.

        Returns:
            float: The circumference of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">radius</span>
</code></pre></div></div> <p>The class has a docstring explaining its purpose and the attributes it has.</p> <p>Each method has its docstring explaining what it does and what arguments it takes and returns.</p> <p>This makes the code easier to understand and maintain and more accessible for others to use and build upon.</p> <h2 id="-handle-exceptions-gracefully">💥 Handle Exceptions Gracefully</h2> <p>Handling exceptions in your code is essential for ensuring that it continues to run even when unexpected events occur.</p> <p>Use <code class="language-plaintext highlighter-rouge">try</code> and <code class="language-plaintext highlighter-rouge">except</code> statements to handle exceptions and provide helpful error messages that explain what went wrong and how to fix it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="n">result</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="mi">0</span>
<span class="k">except</span> <span class="nb">ZeroDivisionError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a non-zero value for division</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The code inside the <code class="language-plaintext highlighter-rouge">try</code> block may raise a <code class="language-plaintext highlighter-rouge">ZeroDivisionError</code> exception.</p> <p>The <code class="language-plaintext highlighter-rouge">except</code> block handles the exception and prints a helpful error message to the user.</p> <p>This way, the program can continue running even when an unexpected error occurs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">file.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">FileNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a valid file path</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle any other exceptions
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An unexpected error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, the code inside the try block may raise a <code class="language-plaintext highlighter-rouge">FileNotFoundError</code> or any other exception.</p> <p>The first <code class="language-plaintext highlighter-rouge">except</code> block handles the FileNotFoundError and provides a helpful error message for the user.</p> <p>The second <code class="language-plaintext highlighter-rouge">except</code> block handles any other exceptions that may occur and provides a generic error message.</p> <p>This way the program can continue running even when unexpected errors occur and provide helpful error messages to the user.</p> <h2 id="-use-keyword-arguments">🔑 Use Keyword Arguments</h2> <p>Keyword arguments are a powerful feature of Python that allows you to specify default values for function arguments and make your code more readable and flexible.</p> <p>Using keyword arguments can also help you reduce the number of lines of code in your programs and make them easier to understand.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">!</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">John</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hello, John!
</span><span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">Jane</span><span class="sh">"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hi, Jane!
</span></code></pre></div></div> <p>In this example, the greet function takes in two arguments: name and message. The message argument has a default value of “Hello”.</p> <p>When we call <code class="language-plaintext highlighter-rouge">greet("John")</code>, the default value of <code class="language-plaintext highlighter-rouge">"Hello"</code> is used for the message argument. But when we call <code class="language-plaintext highlighter-rouge">greet("Jane", message="Hi")</code>, the keyword argument is used instead, and the output is <code class="language-plaintext highlighter-rouge">"Hi, Jane!"</code>.</p> <h2 id="️-follow-the-zen-of-python">🧘‍♀️ Follow the Zen of Python</h2> <p>The Zen of Python is a collection of principles and guidelines for writing good Python code.</p> <p>It includes tips on writing simple, clear, and maintainable code and advice on choosing between different solutions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">this</span>

<span class="k">def</span> <span class="nf">sort_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Simple is better than complex
</span>    <span class="n">data</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Readability counts
</span><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Explicit is better than implicit
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Flat is better than nested
</span><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">lists</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Use meaningful names
</span><span class="k">def</span> <span class="nf">calculate_average_score</span><span class="p">(</span><span class="n">scores</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">score</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># One obvious way to do it
</span>    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="n">count</span>
</code></pre></div></div> <p>We follow the Zen of Python by:</p> <ul> <li>Writing straightforward code (e.g. the <code class="language-plaintext highlighter-rouge">sort_data</code> function)</li> <li>Choosing meaningful names for variables and functions (e.g. <code class="language-plaintext highlighter-rouge">calculate_average_score</code>)</li> <li>Keeping the code flat and avoiding nested structures where possible (e.g. the flatten function)</li> <li>Being explicit and transparent in our code (e.g. using return statements)</li> </ul> <h2 id="-refactor-your-code-regularly">🛠 Refactor Your Code Regularly</h2> <p>Refactoring is improving the structure and quality of your code without changing its external behaviour.</p> <p>It can help you identify areas that need improvement and make your code more maintainable over time. This can be especially important in projects with a long lifespan or requiring continuous updates.</p> <p>So you can simplify complex sections, make your code more efficient, and eliminate any redundant or unnecessary parts. You can also take advantage of new features or libraries that have become available since you wrote the original code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">number</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">number</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Refactored code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span>
</code></pre></div></div> <p>We have refactored the <code class="language-plaintext highlighter-rouge">calculate_sum</code> function to use the built-in sum function instead of manually iterating over the numbers and adding them up. This code is more efficient and readable and takes advantage of a built-in feature of Python that can perform the same calculation.</p>]]></content><author><name></name></author><category term="old-posts"/><category term="python,"/><category term="coding"/><summary type="html"><![CDATA[Let's look at the basic rules for writing great Python code]]></summary></entry></feed>