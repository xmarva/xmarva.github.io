<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://xmarva.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xmarva.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-01T23:51:43+00:00</updated><id>https://xmarva.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Attention Mechanism from RNN to FlashAttention</title><link href="https://xmarva.github.io/blog/2025/attention/" rel="alternate" type="text/html" title="Attention Mechanism from RNN to FlashAttention"/><published>2025-04-01T15:00:00+00:00</published><updated>2025-04-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/attention</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/attention/"><![CDATA[<p>In recurrent neural networks, what we typically do is take our sequence and pass in a single timestep at a time and produce an output. This means when we pass in \(x_1\) we create a hidden state \(h_1\) that captures all the relevant information in the input, and this hidden state then is used to produce the output \(y_1\). Now what makes it an RNN is when we pass in the second timestep \(x_2\) to produce the hidden state \(h_2\), the hidden state already contains information about the past \(h_1\)! Therefore our output of \(y_2\) is informed both by information from \(x_2\) and \(x_1\) encoded through the hidden states. If we keep this going, when we want to make a prediction at \(y_{100}\), we will be using a hidden state that has encoded information of all the inputs \(x_1\) to \(x_{100}\). Everything explained so far is a causal RNN, basically to make a prediction of sometime timestep \(t\), we can use all the input timesteps \(&lt;=t\). We can easily expand this though to make a bidirectional RNN, where to make a prediction at time \(t\), we can look at the entire sequence as well. In this case we will really have two hidden states, one that looks backwards and another that looks forward! Whether you use causal or bidirectional depends a lot on what you want to do. If you want to do Name Entity Recognition (i.e. determine if each word in a sentence is an entity), you can look at the entire sentence to do this. On the other hand if you want to forecast the future, like a stock price, then you have to use causal as you can only look at the past to predict the future.</p> <p>All this sounds well and good, but there was one glaring problem: Memory. The hidden states we use to encode the history can only contain so much information, i.e. as the sequence length becomes longer the model will start to forget. This matters a lot for things like Natural Language Processing, as there may be imporant relations between parts of a book that are pages, or even chapters, apart. To solve this issue, Attention Augmented RNNs were introduced in the paper <a href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation By Jointly Learning To Align and Translate</a>.</p> <h2 id="attention-augmented-rnn">Attention Augmented RNN</h2> <p>If I had to use two words to define attention it would be: <strong>Weighted Average</strong>. In the paper, the call the hidden states <em>annotations</em>, but they are the same thing! So lets go back to our RNN again, before we do our prediction for \(y_t\), we have a sequence of hidden states \(h_t\) that contain the information about the sequence \(x_t\) itself produced from the RNN mechanism. The problem is again, \(h_t\) for large values of \(t\) will have forgotten imporant information about early \(x_t\) values with small values of \(t\). So what if we got everyone to know each other again? We can produce a context vector \(c_i\) that is a weighted average of all the hidden states in the case of a bidirectional architecture, or just the previous hidden states in a causal architecture. This means at any time of the context vector \(c_t\), it will be a weighted average of all of the timesteps so it is reminded about more distant timesteps, solving our <em>memory</em> problem!</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/rnn_with_attention.png?raw=true" width="800"/> </div> <p>Now I keep saying weighted average, and this is because for one of the timesteps, the model has to learn the weights of what is the most important information to know at those times, and then weight them higher! As per the paper, the weights were learned through an alignment model, which was just a feedforward network, that scores how well hidden states as time \(t\) is related to those around it in the sequence. These scores were then passed through a softmax to ensure all the learned weights sum upto 1, and then the context vectors are computed based on them! This means every context vector is a customized weighted average that learned exactly what information to put empahsis on at every timestep of the context vectors.</p> <h3 id="problems">Problems</h3> <p>There were some issues with this though, some which were already known about RNNs:</p> <ul> <li><strong>Efficient but Slow</strong>: The RNN mechanism has a for-loop through the sequence making training very slow, but inference was efficient</li> <li><strong>Lack of Positional Information</strong>: Our context vectors are just weighted averages of hidden, there is no information about position or time, but obviously in most sequence tasks, the order in your data appears is very important</li> <li><strong>Redundancy</strong>: We are effectively learning the same thing twice here, the hidden states encode sequential information, but the attention mechanism also encodes sequential information</li> </ul> <h3 id="attention-is-all-you-need">Attention is All You Need!</h3> <p>The groundbreaking paper, <a href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a> solved all of the problems above, but added a new one: Computational Cost. Lets first look at what the proposed Attention mechanism is doing!</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/attention_mechanism_visual.png?raw=true" width="800"/> </div> <p>The input is a sequence of embedding vectors and the output is a sequence of context vectors. Lets quickly look at the formulation for this:</p> <p>\(\)\text{Attention}(Q,K,V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_e}})V\(\)</p> <p>We see some new notation show up now, \(Q\), \(K\), \(V\), so lets define them:</p> <ul> <li>\(Q\): Queries, they are the token we are interested in</li> <li>\(K\): Keys, they are the other tokens we want to compare our query against</li> <li>\(V\): Values, they are the values we will weight in our weighted average</li> </ul> <p>This is a little weird so lets step through it! First important note, the \(Q\), \(K\), and \(V\) are three projections of our original data input \(X\). This basically means we have three linear layers that all take the same input \(X\) to produce our \(Q\), \(K\), \(V\).</p> <h3 id="step-1-compute-the-attention-matrix-with-softmaxqkt">Step 1: Compute the Attention Matrix with \(Softmax(QK^T)\)</h3> <p>So the first step is the computing the \(Softmax(QK^T)\), where Q and K both have the shape (Sequence Length x Embedding Dimension). The output of this computation will be sequence length x sequence length. This is what it looks like!</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/computing_attention.png?raw=true" width="800"/> </div> <p>In the image above, I also applied the softmax (not shown for simplicity), so each row of the attention matrix adds up to 1 (like probabilities).</p> <p><strong>Recap: Dot Product</strong></p> <p>As a quick reminder, this whole mechanism depends on the dot product, and more specifically, its geometric interpretation</p> \[a\cdot b = \sum_{i=1}^n a_i*b_i = |a||b|cos(\theta)\] <p>What the dot product really signifies is the similarity between vectors. Remember the cosine of 0 is just 1, so the highest possible cosine value would be when the vectors \(a\) and \(b\) point in the exact same direction. This means vectors that are similar in direction have higher magnitude.</p> <p><strong>Recap: Matrix Multiplication</strong></p> <p>Also remember, matrix multiplication is basically just a bunch of dot products, repeating the multiply/add operation repeatedly. If we are multiplying matrix \(A\) with matrix \(B\), what we are really doing is doing the dot product of every row of \(A\) and every column of \(B\)!</p> <p>So with our quick recaps, lets go back to the image above, when we are multiplying \(Q\) by \(K^T\), we are multiplying each vector in the sequence \(Q\) by each vector in the sequence \(K\) and computing their dot product similarity. Again, \(Q\) and \(K\) are just projections of the original data \(X\), so really we are just computing the similarity between every possible combination of timesteps in \(X\). We also could have just done \(XX^T\), this would technically be the same thing, but by including the projections of \(X\) rather than using the the raw inputs themselves, we allow the model to have more learnable parameters so it can futher accentuate similarities and differences between different timesteps!</p> <p>The final result of this operation is the attention matrix, that computes the similarity between every possible pairs of tokens.</p> <p><strong>Note</strong> I didn’t inlude anything about the \(\frac{1}{\sqrt{d_e}}\) term in the formula. This is just a normalization constant that ensures our variance of the attention matrix isn’t too large after our matrix multiplication. This just leads to more stable training!</p> <h3 id="step-2-weighting-the-values-matrix">Step 2: Weighting the Values Matrix</h3> <p>Now that we have our similarities of how each timestep is related to all the other timesteps, we can now do our weighted average! After the weighted average computation, each vector for each timestep isn’t just the data of the timestep but rather a weighted average of all the vectors in the sequence and how they are related to that timestep of interest.</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/encoder_attention_vis.png?raw=true" width="800"/> </div> <p>The output of this operation gives us the sequence of context vectors!</p> <h3 id="enforcing-causality">Enforcing Causality</h3> <p>What we have seen so far is the equivalent to a Bidirectional RNN. The weighted average operation we are doing is between a timestep of interest and all timesteps before and after it. If we wanted a causal model, where a context vector only depends on the timesteps before it, then we need to apply a causal mask to our attention mechanism.</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/causal_masking.png?raw=true" width="800"/> </div> <p>As you can see, we apply a mask to all values of \(t\) where the index of the column values (our key index) is greater than the index of the row value (our value index). In practice, once we apply this mask to our attention matrix, we can then multiply by our values. You will see that the context vector at time \(t\) is only then dependent on previous timesteps, as we make sure future vectors of \(V\) are zeroed out!</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/decoder_attention_vis.png?raw=true" width="800"/> </div> <h3 id="lets-build-this">Lets Build This!</h3> <p>Now that we have everything we need, we can build it! We wont be trainig any models now, just defining and exploring the architecture here. To do so, we will define some data in the form of <code class="language-plaintext highlighter-rouge">Batch x Sequence Length x Embed Dim</code>. The Embedding dimension is basically, what dimension vector do we want to use to represent a single timestep, and the sequence length is how many timesteps there are in total.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of Input is:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="implement-attention-without-any-learnable-parameters-textsoftmaxfracxxtsqrtd_ex">Implement Attention Without Any Learnable Parameters \(\text{Softmax}(\frac{XX^T}{\sqrt{d_e}})X\)</h3> <p>This whole attention operation is again very flexible and there is technically no reason to have any learnable parameters in its formulation (other than the obvious for wanting better predictive performance). So lets quickly just implement the formula as is using raw inputs \(X\) rather than doing any learned projections of the data.</p> <p><strong>Step 1</strong></p> <p>Compute \(XX^T\) which will provide the similarity score between every pair of vectors in \(X\). This will be contained inside a <code class="language-plaintext highlighter-rouge">batch x sequence_length x sequence_length</code> matrix</p> <p><strong>Step 2</strong></p> <p>After computing the similarity score, we can check and see that the variance of the similarity matrix is extremely high, this is the main reason for the normalization of dividing by the square root of the embedding dimension. In the end, the similarity scores are passed through a softmax to compute a probability vector, dividing by a constant basically acts as a temperature parameter to cool the distribution and provide more stable training!</p> <p><strong>Step 3</strong></p> <p>Each row of our <code class="language-plaintext highlighter-rouge">sequence_length x sequence_length</code> matrix is the similarity of how one timestep is related to all other timesteps! What we want to do is, instead of raw similarity scores, we will convert them to probabilities, so when we do the weighted average on our values matrix, the weights add up to 1!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### First compute XX^T for similarity score between every pair of tokens ###
</span><span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="c1">### Normalize the Similarity Scores ###
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Prenormalization Variance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">similarity</span><span class="p">.</span><span class="nf">var</span><span class="p">())</span>
<span class="n">similarity_norm</span> <span class="o">=</span> <span class="n">similarity</span> <span class="o">/</span> <span class="p">(</span><span class="n">embed_dim</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Normed Similarity Variance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">similarity_norm</span><span class="p">.</span><span class="nf">var</span><span class="p">())</span>

<span class="c1">### Check the Shape of our Similarity Tensor ###
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of Normed Similarity:</span><span class="sh">"</span><span class="p">,</span> <span class="n">similarity_norm</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">### Compute similarity on every row of the attention matrix (i.e along the last dimension) ###
</span><span class="n">attention_mat</span> <span class="o">=</span> <span class="n">similarity_norm</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Verify each row adds up to 1 ###
</span><span class="n">summed_attention_mat</span> <span class="o">=</span> <span class="n">attention_mat</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Everything Equal to One:</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">summed_attention_mat</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">summed_attention_mat</span><span class="p">)))</span>

<span class="c1">### Multiply our Attention Matrix against its Values (X in our case) for our Weighted Average ###
</span><span class="n">context_vectors</span> <span class="o">=</span> <span class="n">attention_mat</span> <span class="o">@</span> <span class="n">x</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output Shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">context_vectors</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <p>Thats it! This is basically all the attention computation is doing mathematically, we will add in the learnable parameters in just a bit. Something important to bring to your attention is the input shape of \(x\) and our output shape of the context vectors are identical. Again, the input is the raw data, the output is weighted averaged of how every token is realted to all the other ones. But the shapes not changing is quite convenient, and allows us to stack together a bunch attention mechanisms on top of one another!</p> <h3 id="lets-add-learnable-parameters">Lets Add Learnable Parameters</h3> <p>This time, instead of using \(X\) as our input, we will create our three projections of \(X\) (Queries, Keys and Values), but then repeat the operation we just did here! Also for convenience, I will wrap it all in a PyTorch class so we can continue adding stuff onto it as we go on!</p> <p>Now what are these projections exactly? The are pointwise (or per timestep) projections! Remember, in our example here, each timestep is encoded by a vector of size 128. We will create three learnable weight matricies, incorporated inside the Linear modules in PyTorch, that take these 128 numbers per timestep and projects them to another 128 numbers (it is typical to keep the embedding dimension the same). This is a per timestep operation not across timesteps operation (across timesteps occurs within the attention computation). Obviously, PyTorch will accelerate this per timestep operation by doing it in parallel, but regardless, different timesteps dont get to see each other in the projection step.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embedding_dimension</span>

        <span class="c1">### Create Pointwise Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1">### Create Queries, Keys and Values from X ###
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1">### Do the same Computation from above, just with our QKV Matricies instead of X ###
</span>        <span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attention</span>  <span class="o">=</span> <span class="n">similarity</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">@</span> <span class="n">v</span>

        <span class="k">return</span> <span class="n">output</span>

<span class="n">attention</span> <span class="o">=</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="multiheaded-attention">MultiHeaded Attention</h3> <p>Now we have a small problem! Remember, the Attention matrix encodes the similarity between each pair of timesteps in your sequence. But in many cases, language being a prime example, there can be different types of relationships between different pairs of words, but our attention computation is restricted to only learn one of them. The solution to this is <strong>MultiHeaded Attention</strong>. Inside each attention computation, what if we have 2 attention matricies, or 8 or however many we want! The more we have the larger diversity of relationships we can learn!</p> <h4 id="single-headed-attention-recap">Single Headed Attention Recap</h4> <p>Lets summarize everything we have seen so far with a single visual, and we will call this a Single Head of attention. We will also have our embedding dimension for each word in the sequence be 9, and the sequence length is 8.</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/single_headed_attention_visual.png?raw=true" width="800"/> </div> <p>This is again called single headed attention because we only compute a single attention matrix following the logic above!</p> <h4 id="moving-to-multiheaded-attention">Moving to MultiHeaded Attention</h4> <p>For multiheaded attention there isn’t really a lot changing. Remember, to create our \(Q\), \(K\), \(V\) in single headed attention, we have 3 linear projection layers that take in the embedding dimension and output the same embedding dimension (in our case it takes in 9 and outputs 9). But in multiheaded attention, we can actually reduce our embedding dimension to a smaller value, do the attention computation on the tokens with this condensed embedding dimension, repeat it a bunch of times, and then concatenate together the outputs.</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/multiheaded_attention_visual.png?raw=true" width="800"/> </div> <p>It is general practice to have the number of heads you pick to be a divisor of the embedding dimension. For example, in our case, our original embedding dimension is 9, so we can pick 3 heads because 9 is divisible by 3. This also means our head dimension would now be 3, because 9/3 = 3. In typical transformers, the embedding dimension is 768, and they typically have 12 heads of attention. This means each head of attention will have a dimension of 64 because 768/12 = 64.</p> <p>The main reason we want it to evenly divide is because we have three heads, each takes in an embedding dimension of 9 and compresses to 3 before computing attention, and then outputs a tensor of embedding size 3. We can then take our 3 tensors, each having an embedding dimension of 3, concatenate them together, returning us back to the 9 that we began with! Again, this is just for convenience, so the embedding dimension of the input and output tensor dont change in any way. Last problem is, each head of attention is computed individually, so the final concatenated tensor has a bunch of heads of attention packed together, but we never got to share information between the different heads of attention. This is why we have the final head projection, that will take in the embedding dimension of 9 from our concatenated tensor, and output an embedding dimension of 9, therefore meshing information across the heads of our embedding dimension.</p> <p>Lets go ahead and build this module as shown in the figure above! Basically, each head will have 3 projection layers for Q,K,V, we will perform the attention computation, and then stick all the results back together at the end!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1">### Make sure Embedding Dimension is Divisible by Num Heads ###
</span>        <span class="k">assert</span> <span class="n">embedding_dimension</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Make sure your embed_dim </span><span class="si">{</span><span class="n">embedding_dimension</span><span class="si">}</span><span class="s"> is divisible by the number of heads </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="c1">### Compute Head Dimension ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span>


        <span class="c1">### Create a List of Lists which has all our Q,K,V projections for each head ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">multihead_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>

        <span class="c1">### For head Head create the QKV ###
</span>        <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">):</span>

            <span class="c1">### Create a dictionary of the 3 projection  layers we need ###
</span>            <span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">[</span><span class="sh">"</span><span class="s">Q</span><span class="sh">"</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)],</span>
                    <span class="p">[</span><span class="sh">"</span><span class="s">K</span><span class="sh">"</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)],</span>
                    <span class="p">[</span><span class="sh">"</span><span class="s">V</span><span class="sh">"</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)],</span>
                <span class="p">]</span>
            <span class="p">)</span>

            <span class="c1">### Store Dictionary in List ###
</span>            <span class="n">self</span><span class="p">.</span><span class="n">multihead_qkv</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">qkv_proj</span><span class="p">)</span>

        <span class="c1">### Create final Projection layer, it will be applied to the concatenated heads will have shape embed_dim again ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">head_mesh</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="c1">### Create a list ot store each heads output ###
</span>        <span class="n">head_outs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1">### Loop Through Each head of Attention ###
</span>        <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">multihead_qkv</span><span class="p">:</span>

            <span class="c1">### Access layers like a dictionary (ModuleDict) ###
</span>            <span class="c1">### q,k,v will be (Batch x Seq len x head_dim)
</span>            <span class="n">q</span> <span class="o">=</span> <span class="n">head</span><span class="p">[</span><span class="sh">"</span><span class="s">Q</span><span class="sh">"</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">head</span><span class="p">[</span><span class="sh">"</span><span class="s">K</span><span class="sh">"</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">head</span><span class="p">[</span><span class="sh">"</span><span class="s">V</span><span class="sh">"</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>

            <span class="c1">### Now do the same Attention computation as before! ###
</span>            <span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">attention</span>  <span class="o">=</span> <span class="n">similarity</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">attention</span> <span class="o">@</span> <span class="n">v</span>

            <span class="c1">### Store this output in the head_outs ###
</span>            <span class="n">head_outs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1">### head_outs has num_heads tensors, each with the compressed embedding dimension of head_dim ###
</span>        <span class="c1">### We can concatenate them all back together along the embedding dimension just like we did in the image above ###
</span>        <span class="n">head_outs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">head_outs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">### head_outs will have the same shape now as our input x! ###
</span>        <span class="k">if</span> <span class="n">head_outs</span><span class="p">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">Exception</span><span class="p">(</span><span class="sh">"</span><span class="s">Something has gone wrong in the attention computation</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1">### Now each head was computed independently, we need them to get to know each other, so pass our head_outs through final projection ###
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">head_mesh</span><span class="p">(</span><span class="n">head_outs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>


<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">mha</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###
</span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">mha</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>

</code></pre></div></div> <h3 id="increasing-efficiency">Increasing Efficiency</h3> <p>We now have a successful Multihead Attention layer!! This basically has all the same math and lodgic of attention, except for one small issue: efficiency. Typically we want to avoid for loops as much as possible in our PyTorch code, being able to vectorize and do things in parallel will make much better use of the GPUs we train on. To make this more efficient though, theres something we need to understand first: PyTorch Linear layers on multidimensional tensors!</p> <h4 id="linear-layers-on-multidimensional-tensors">Linear Layers on MultiDimensional Tensors</h4> <p>We have already seen <code class="language-plaintext highlighter-rouge">nn.Linear(input_dim, output_dim)</code> many times already, and this module expects a tensor of shape <code class="language-plaintext highlighter-rouge">[Batch x input_dim]</code> and it will output <code class="language-plaintext highlighter-rouge">[Batch x output_dim]</code>. But what if our input is <code class="language-plaintext highlighter-rouge">[Batch x Dim1 x Dim2 x input_dim]</code>, then what happens? Basically, PyTorch will automatically flatten all the dimensions other than the last one automagically, do the linear layer, and then return back to the expected shape, so we would get an output of <code class="language-plaintext highlighter-rouge">[Batch x Dim1 x Dim2 x output_dim]</code>. Another way of thinking about this is, PyTorch linear layers only are applied to the last dimension of your tensor. Lets do a quick example!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>

<span class="n">tensor_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tensor_1_out</span> <span class="o">=</span> <span class="nf">fc</span><span class="p">(</span><span class="n">tensor_1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_1</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">"</span><span class="s">Output Shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_1_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">tensor_2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tensor_2_out</span> <span class="o">=</span> <span class="nf">fc</span><span class="p">(</span><span class="n">tensor_2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input Shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_2</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">"</span><span class="s">Output Shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_2_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="packing-linear-layers">Packing Linear Layers</h3> <p>Another important idea is packing our linear layers together. Lets think about out example again, each projection for Q, K and V have a Linear layer that takes in 9 values and outputs 3 values, and we repeat this 3 times for each head. Lets just think about our Queries for now.</p> <ul> <li>Query for Head 1: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3</li> <li>Query for Head 2: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3</li> <li>Query for Head 3: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3</li> </ul> <p>Well what if we reframed this? What if we had a single linear layer that take input x with embedding dim 9 and outputs something with embedding dim 9. Afterwards, we can cut the matrix into our three heads of attention. Lets do a quick example!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span>

<span class="c1">### Pass tensor through layer to make Queries ###
</span><span class="n">q</span> <span class="o">=</span> <span class="nf">fc</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of all Queries:</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">### Cut Embedding dimension into 3 heads ###
</span><span class="n">q_head1</span><span class="p">,</span> <span class="n">q_head2</span><span class="p">,</span> <span class="n">q_head3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of each Head of Query:</span><span class="sh">"</span><span class="p">,</span> <span class="n">q_head1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="multidimensional-matrix-multiplication">MultiDimensional Matrix Multiplication</h3> <p>So, we have composed our 9 linear layers (3 heads have 3 projections for Q,k,V each) into just 3 linear layers, where we have packed all the heads into them. But after we chunk up our Q,K,V tensors each into three more tensors for each head we will still need to do the looping operation to go through the cooresponding q,k,v matricies. Can we parallelize this too? Of course! We just need to better understand higher dimensional matrix multiplication.</p> <h4 id="recap">Recap:</h4> <p>Matrix multiplication is typicall seen like this, multiplying an <code class="language-plaintext highlighter-rouge">[AxB]</code> matrix by a <code class="language-plaintext highlighter-rouge">[BxC]</code> which will produce a <code class="language-plaintext highlighter-rouge">[AxC]</code> matrix. But what if we have a <code class="language-plaintext highlighter-rouge">[Batch x dim1 x A x B]</code> multiplied by a <code class="language-plaintext highlighter-rouge">[Batch x dim1 x B x C]</code>. Matrix multiplication again only happens on the last two dimensions, so because our first tensor ends with an <code class="language-plaintext highlighter-rouge">[AxB]</code> and the second tensor ends with a <code class="language-plaintext highlighter-rouge">[BxC]</code>, the resulting matrix multiplication will be <code class="language-plaintext highlighter-rouge">[Batch x dim1 x A x C</code>]`. Lets see a quick example!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output Shape:</span><span class="sh">"</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="nd">@b</span><span class="p">).</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="the-trick-of-parallelizing-heads">The Trick of Parallelizing Heads</h3> <p>Now for the trick of parallelizing our heads by using everything we have just seen! All we need to do is split the embedding dimension up and move the heads out of the way so the computation can occur. Remember, we have our \(Q\), \(K\), and \(V\) matricies right now that each contain all the projected heads and are in the shape <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Embed_dim]</code> ([batch x 8 x 9] in our case).</p> <ul> <li>Step 1: Split the embedding dimension into the number of heads and head dim. We already know that our embedding dimension is divisible as thats how we set it, so we can do <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Embed_dim]</code> -&gt; <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Num_Heads x Embed_Dim]</code>. (This would be taking our [batch x 8 x 9] and converting to [batch x 8 x 3 x 3])</li> <li>The attention computation has to happen between two matricies of shape <code class="language-plaintext highlighter-rouge">[Seq_len x Embed_Dim]</code> for queries and <code class="language-plaintext highlighter-rouge">[Embed_Dim x Seq_len]</code> for our transposed keys. In the case of multihead attention, the matrix multiplication happens across the Head Dimension rather than embedding. If our Queries, Keys and Values are in the shape <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Num_Heads x Embed_Dim]</code>, we can just transpose the Seq_len and Num_heads dimensions and make a tensor of shape <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Seq_len x Embed_Dim]</code>. This way when I do Queries multiplied by the Transpose of Keys I will be doing <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Seq_len x Embed_Dim]</code> multiplied by <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Embed Dim x Seq Len]</code> creating the attention matrix <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Seq_len x Seq Len]</code>. Therefore we have effectively created for every sample in the batch, and for every head of attention, a unique attention matrix! Thus we have parallelized the Attention Matrix computation.</li> <li>Now that we have out all our attention matricies <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Seq_len x Seq Len]</code>, we can perform our scaling by our constant, and perform softmax across every row of each attention matrix (along the last dimension).</li> <li>The last step is to multiply out attention matrix <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Seq_len x Seq Len]</code> by the Values which is in the shape <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Num_Heads x Embed_Dim]</code>, which will get us to <code class="language-plaintext highlighter-rouge">[Batch x Num_Heads x Seq_len x Embed_Dim]</code>!</li> <li>Lastly, we need to put our Num Heads and Embedding dimensions back together, so we can permute the Num Heads and Seq Len dimensions again which gives us <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Num_Heads x Embed_Dim]</code> and flatten on the last two dimension finally giving us <code class="language-plaintext highlighter-rouge">[Batch x Seq_len x Num_Heads x Embed_Dim]</code>.</li> <li>This flattening operation is equivalent to concatenation of all the heads of attention, so we can pass this through our final projection layer so all the heads of attention gets to know each other!</li> </ul> <h3 id="more-efficient-attention-implementation">More Efficient Attention Implementation</h3> <p>We will also include some extra dropout layers typically added to attention computations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SelfAttentionEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
               <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
               <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">

        Args:
            embed_dim: Transformer Embedding Dimension
            num_heads: Number of heads of computation for Attention
            attn_p: Probability for Dropout2d on Attention cube
            proj_p: Probability for Dropout on final Projection
        </span><span class="sh">"""</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttentionEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###
</span>        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>

        <span class="c1">### Define all our Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="c1">### Perform Attention Computation ###
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>

        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">a</span> <span class="o">=</span> <span class="nc">SelfAttentionEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###
</span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">a</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h2 id="padding">Padding!</h2> <p>One aspect we haven’t talked about yet is padding. If you remember back to when we looked at <a href="https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20NLP/Recurrent%20Neural%20Networks/IMDB%20Classification/Sequence%20Classification.ipynb">classification with LSTMS</a> we had a pad token in our model to allow us to pad sequences to the longest in the batch. We have to train in batches, and unfortunately, if each sequence is of different lengths, we cannot put them together, so in this situation, we just take all the sequences that are shorter than the longest in the batch and then pad them.</p> <h3 id="sequence-padding-and-attention-masking">Sequence Padding and Attention Masking</h3> <p>One typical consideration for models like this is, when we pass in sentences of data, different sentences have different lengths. To create a matrix of tokens, we need to make sure all the sequences are the same, therefore we have to pad the shorter sequences to the longer ones. This is what the padding looks like!</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/sequence_padding.png?raw=true" width="600"/> </div> <p>Now the pad tokens are just fillers, they don’t add any information at all to the data. To deal with this, we need to make sure that when we compute attention, these pad tokens are ignored. This is very similar to the Causal Mask in <a href="https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT">GPT</a>, and we will be doing exactly what we did there, fill in the masked locations with \(-\infty\) before softmax is computed. Lets take a quick look at this for one of the sentences above.</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/padding_attention_mask.png?raw=true" width="600"/> </div> <p>You can see that we zero out the columns of our attention mask that has padding tokens, but not the row. <strong>The reason for this is, the model should learn the relation of how the padding tokens are related to the words (this is most likely just positional information that padding is at the end), but it should not learn how other words are related to padding</strong>, as the padding adds no semantic meaning. This is why, when multiplying by our values, we have zeroed out the attention weights of the 4th column, as when computing weighted averages of how one word is related to all others, the padding token is not included.</p> <h2 id="computing-the-reweighted-padded-attention-mask">Computing the Reweighted Padded Attention Mask</h2> <p>Lets create some numbers so we can get a better idea of how this works. Let the tokens be \(X = [10, 2, \text{&lt;pad&gt;}]\), so the third token is a padding token. Lets then also pretend, we pass this to our RoBERTa model, and when we go to compute our attention \(QK^T\), the raw output before the softmax is below.</p> <p>\begin{equation} \begin{bmatrix} 7 &amp; -8 &amp; 6 <br/> -3 &amp; 2 &amp; 4 <br/> 1 &amp; 6 &amp; -2 <br/> \end{bmatrix} \end{equation}</p> <p>Remember, the equation for softmax is:</p> \[\text{Softmax}(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^N{e^{x_j}}}\] <p>Then, if we ignore padding and everything right now, we can compute softmax for row of the matrix above:</p> <p>\begin{equation} \text{Softmax} \begin{bmatrix} 7 &amp; -8 &amp; 6 <br/> -3 &amp; 2 &amp; 4 <br/> 1 &amp; 6 &amp; -2 <br/> \end{bmatrix} = \begin{bmatrix} \frac{e^{7}}{e^{7}+e^{-8}+e^{6}} &amp; \frac{e^{-8}}{e^{7}+e^{-8}+e^{6}} &amp; \frac{e^{6}}{e^{7}+e^{-8}+e^{6}} <br/> \frac{e^{-3}}{e^{-3}+e^{2}+e^{4}} &amp; \frac{e^{2}}{e^{-3}+e^{2}+e^{4}} &amp; \frac{e^{4}}{e^{-3}+e^{2}+e^{4}} <br/> \frac{e^{1}}{e^{1}+e^{6}+e^{-2}} &amp; \frac{e^{6}}{e^{1}+e^{6}+e^{-2}} &amp; \frac{e^{-2}}{e^{1}+e^{6}+e^{-2}} <br/> \end{bmatrix} = \begin{bmatrix} 0.73 &amp; 0.0000002 &amp; 0.27 <br/> 0.0008 &amp; 0.12 &amp; 0.88 <br/> 0.007 &amp; 0.99 &amp; 0.003 <br/> \end{bmatrix} \end{equation}</p> <p>But what we need is to mask out all the tokens in this matrix related to padding. Just like we did in <a href="https://github.com/priyammaz/HAL-DL-From-Scratch/tree/main/PyTorch%20for%20NLP/GPT">GPT</a>, we will fill in the indexes of the that we want to mask with \(-\infty\). If only the last token was a padding token in our sequence, then the attention before the softmax should be written as:</p> <p>\begin{equation} \begin{bmatrix} 7 &amp; -8 &amp; -\infty <br/> -3 &amp; 2 &amp; -\infty <br/> 1 &amp; 6 &amp; -\infty <br/> \end{bmatrix} \end{equation}</p> <p>Taking the softmax of the rows of this matrix then gives:</p> <p>\begin{equation} \text{Softmax} \begin{bmatrix} 7 &amp; -8 &amp; -\infty <br/> -3 &amp; 2 &amp; -\infty <br/> 1 &amp; 6 &amp; -\infty <br/> \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 <br/> 0.0067 &amp; 0.9933 &amp; 0 <br/> 0.0067 &amp; 0.9933 &amp; 0 <br/> \end{bmatrix} \end{equation}</p> <p>Which is exactly what we want! Therefore, when computing attention, we need to know which tokens are padding tokens in our data, and then go ahead and perform this computation on our attention output before multiplying with values.</p> <h3 id="add-padding-to-our-attention-computation">Add Padding to our Attention Computation!</h3> <p>There are a few ways to indicate padding, but the most common is a simple True/False vector. Lets say we have the following 3 sequences:</p> <ul> <li>sequence_1: [a1, a2, a3]</li> <li>sequence_2: [b1, b2, b3, b4]</li> <li>sequence_3: [c1, c2, c3]</li> </ul> <p>In this situation we would pad sequence_1 and sequence_3 to the longest sequence, so after padding we would have something like this:</p> <ul> <li>sequence_1: [a1, a2, a3, <strong>&lt;PAD&gt;</strong>]</li> <li>sequence_2: [b1, b2, b3, b4]</li> <li>sequence_3: [c1, c2, c3, <strong>&lt;PAD&gt;</strong>]</li> </ul> <p>Now we will create boolean vectors, identifying True/False for tokens we want to compute attention on. Anything that is a <strong>&lt;PAD&gt;</strong> token would be omitted, so we would have the following attention masks for each sequence:</p> <ul> <li>sequence_1: [True, True, True, False]</li> <li>sequence_2: [True, True, True, True]</li> <li>sequence_3: [True, True, True, False]</li> </ul> <p>Then, as indicated above, for columns for tokens identified as False (because they are pad tokens) we will zero out the attention scores and reweight them to properly compute attention without learning how words are related to the padding (only how the padding is related to the words)</p> <h4 id="repeating-to-match-attention-matrix-shape">Repeating To Match Attention Matrix Shape</h4> <p>So we now have our attention mask and know we need to mask the columns in our attention matrix that coorespond to them! Lets also pretend for now we are doing single headed attention, we can deal with the multiheaded attention in a bit. In this case, lets take a look at our attention and mask shapes:</p> <p><code class="language-plaintext highlighter-rouge">attn.shape</code> - (Batch x seq_len x seq_len)</p> <p><code class="language-plaintext highlighter-rouge">mask.shape</code> - (Batch x seq_len)</p> <p>It is clear that our mask is missing a dimension, and we need to repeat it. Lets take sequence_1 for instance that has a mask of [True, True, True, False]. Because the sequence length here is 4, lets repeat this row 4 times:</p> <p>\begin{bmatrix} True &amp; True &amp; True &amp; False<br/> True &amp; True &amp; True &amp; False<br/> True &amp; True &amp; True &amp; False<br/> True &amp; True &amp; True &amp; False \end{bmatrix}</p> <p>By repeating our mask for sequence_1, we get exactly a seq_len x seq_len matrix where the column we don’t want to compute attention for is indicated by the False!</p> <p>Therefore our final mask will be of shape (Batch x seq_len x seq_len)</p> <p><strong>CAVEAT</strong> Technically we don’t need to do this actually, and this is why the padding can get confusing as different people/functions do it different ways. First of all, lets say we reshaped our attention mask from (Batch x seq_len) to (Batch x 1 x seq_len). That 1 is a dummy dimension, and when we use the <code class="language-plaintext highlighter-rouge">Tensor.masked_fill_</code> it will automatically broadcast our attention mask across that dummy 1 dimension. On the other hand, when we go to use <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">flash_attention</a> at the end, this method wants the tensor specifically in the shape (Batch x seq_len x seq_len)</p> <p>For testing, lets just use a (1 x 6 x 6) tensor as a fake 8 x 8 attention matrix for a single sample (not multihead attention yet). Lets also pretend the last two tokens are pad tokens, so we want the attention mask that will fill the last two columns with \(-\infty\), so when we take the softmax in the future it’ll become zero!</p> <p><code class="language-plaintext highlighter-rouge">Tensor.masked_fill_</code> will fill anywhere indicated as <code class="language-plaintext highlighter-rouge">True</code> with our fill value. In our case though, we have the tokens we don’t want to compute on (the ones we want to fill with \(-\infty\) as false! So we just need to flip the boolean in our attention mask.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Create an example attention matrix (b x n x n) ###
</span><span class="n">rand_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>

<span class="c1">### Create Attention Mask in the shape (b x n) ###
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Method 1:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">--------</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">### Add Extra Dimension for the (b x n x n) ###
### So unsqueeze mask to be (b x 1 x n) ###
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Unsqueezed with dummy broadcast dimension ###
</span><span class="nf">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-inf</span><span class="sh">"</span><span class="p">)))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Method 2:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">--------</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">### Repeat the Dummy Dimension so attention mask is (b x n x n) ###
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># repeat dummy middle dim 6 times (for the seq_len)
</span><span class="nf">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-inf</span><span class="sh">"</span><span class="p">)))</span>
</code></pre></div></div> <h4 id="matching-shapes-for-multihead-attention">Matching Shapes for MultiHead Attention</h4> <p>So we now have our attention mask and know how to mask the columns cooresponding to them! But we have one other problem: MultiHead Attention. This means we don’t have just a single attention matrix, but rather <code class="language-plaintext highlighter-rouge">num_heads</code> of them. This means we need to mask the cooresponding columns of <strong>ALL</strong> attention matricies across the heads. Lets take a quick look at our tensors in question again:</p> <p><code class="language-plaintext highlighter-rouge">attn.shape</code> - (Batch x num_heads x seq_len x seq_len)</p> <p><code class="language-plaintext highlighter-rouge">mask.shape</code> - (Batch x seq_len)</p> <p>This means we can do what we did earlier, just twice! We can take our mask and unsqueeze twice, and go from (Batch x seq_len) to (Batch x 1 x 1 x seq_len), and this would be totally fine! Except again, when we do our <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">flash_attention</a> later, it again expects a (seq_len x seq_len). Luckily though, flash attention will broadcast over the head dimension as long as that dimension is present in the mask, so we just have to do what we did earlier and repeat (Batch x 1 x 1 x seq_len) to (Batch x 1 x seq_len x seq_len)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Create an example attention matrix (b x h x n x n) ###
</span><span class="n">rand_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span> <span class="c1"># I have 2 heads here!
</span>
<span class="c1">### Create Attention Mask in the shape (b x n) ###
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Method 1:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">--------</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">### Add Two Extra Dimension for the (b x h x n x n) ###
### So unsqueeze mask to be (b x 1 x 1 x n) ###
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">### Unsqueezed with dummy broadcast dimension ###
</span><span class="nf">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-inf</span><span class="sh">"</span><span class="p">)))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Method 2:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">--------</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">### Repeat the Dummy Dimension for seq_len so attention mask is (b 1 x n x n) ###
</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># repeat dummy middle dim 6 times (for the seq_len)
</span><span class="nf">print</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rand_attn</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">-inf</span><span class="sh">"</span><span class="p">)))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                 <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">

        Args:
            embed_dim: Transformer Embedding Dimension
            num_heads: Number of heads of computation for Attention
            attn_p: Probability for Dropout2d on Attention cube
            proj_p: Probability for Dropout on final Projection
        </span><span class="sh">"""</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###
</span>        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>

        <span class="c1">### Define all our Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="c1">### Perform Attention Computation ###
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="c1">####################################################################################
</span>        <span class="c1">### FILL ATTENTION MASK WITH -Infinity ###
</span>
        <span class="c1">### NOTE:
</span>        <span class="c1">### attn.shape - (Batch x num_heads x seq_len x seq_len)
</span>        <span class="c1">### mask.shape - (Batch x seq_len)
</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>

            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>

        <span class="c1">####################################################################################
</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>

        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###
</span><span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###
### This will be a tensor upto the max(seq_lens) ###
</span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###
</span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">a</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="enforcing-causality-1">Enforcing Causality</h3> <p>Everything we have done so far is an encoding transformer, which is eqivalent to a bidirectional RNN where a single timestep can look forwards and backwards. To enforce causality, we can only look backwards, so we have to add in a causal mask! How the causal mask looks is the following:</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/causal_masking.png?raw=true" width="600"/> </div> <p>Basically, every Query vector can only attend to the Key vectors that are at the same timestep or before! Now how do we actually do this? Unfortunately its not as easy as just changing the numbers in our attention mask, because we still need every row of the attention mask to add to one (like a probability vector!). We want something like this though:</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/decoder_attention_vis.png?raw=true" width="600"/> </div> <p>As we can see, because we have zeroed out the attention weights on future keys, when we multiply by values, we multiply by 0. Therefore our weighted average context vector of a timestep only is computed at that timestep and previous, never the future!</p> <h3 id="computing-the-reweighted-causal-attention-mask">Computing the Reweighted Causal Attention Mask</h3> <p>Lets pretend the raw outputs of \(QK^T\), before the softmax, is below:</p> <p>\begin{equation} \begin{bmatrix} 7 &amp; -8 &amp; 6 <br/> -3 &amp; 2 &amp; 4 <br/> 1 &amp; 6 &amp; -2 <br/> \end{bmatrix} \end{equation}</p> <p>Remember, the equation for softmax is:</p> <p>\(\)\text{Softmax}(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^N{e^{x_j}}}\(\)</p> <p>Then, we can compute softmax for row of the matrix above:</p> <p>\begin{equation} \text{Softmax} \begin{bmatrix} 7 &amp; -8 &amp; 6 <br/> -3 &amp; 2 &amp; 4 <br/> 1 &amp; 6 &amp; -2 <br/> \end{bmatrix} = \begin{bmatrix} \frac{e^{7}}{e^{7}+e^{-8}+e^{6}} &amp; \frac{e^{-8}}{e^{7}+e^{-8}+e^{6}} &amp; \frac{e^{6}}{e^{7}+e^{-8}+e^{6}} <br/> \frac{e^{-3}}{e^{-3}+e^{2}+e^{4}} &amp; \frac{e^{2}}{e^{-3}+e^{2}+e^{4}} &amp; \frac{e^{4}}{e^{-3}+e^{2}+e^{4}} <br/> \frac{e^{1}}{e^{1}+e^{6}+e^{-2}} &amp; \frac{e^{6}}{e^{1}+e^{6}+e^{-2}} &amp; \frac{e^{-2}}{e^{1}+e^{6}+e^{-2}} <br/> \end{bmatrix} = \begin{bmatrix} 0.73 &amp; 0.0000002 &amp; 0.27 <br/> 0.0008 &amp; 0.12 &amp; 0.88 <br/> 0.007 &amp; 0.99 &amp; 0.003 <br/> \end{bmatrix} \end{equation}</p> <p>But, what we want, is the top triangle to have weights of 0, and the rest adding up to 1. So lets take the second vector in the matrix above to see how we can do that.</p> <p>\(\)x_2 = [-3, 2, 4]\(\)</p> <p>Because this is the second vector, we need to zero out the softmax output for everything after the second index (so in our case just the last value). Lets replace the value 4 by \(-\infty\). Then we can write it as:</p> <p>\(\)x_2 = [-3, 2, -\infty]\(\)</p> <p>Lets now take softmax of this vector!</p> <p>\(\)\text{Softmax}(x_2) = [\frac{e^{-3}}{e^{-3}+e^{2}+e^{-\infty}}, \frac{e^{2}}{e^{-3}+e^{2}+e^{-\infty}}, \frac{e^{-\infty}}{e^{-3}+e^{2}+e^{-\infty}}]\(\)</p> <p>Remember, \(e^{-\infty}\) is equal to 0, so we can solve solve this!</p> <p>\(\)\text{Softmax}(x_2) = [\frac{e^{-3}}{e^{-3}+e^{2}+0}, \frac{e^{2}}{e^{-3}+e^{2}+0}, \frac{0}{e^{-3}+e^{2}+0}] = [\frac{e^{-3}}{e^{-3}+e^{2}+0}, \frac{e^{2}}{e^{-3}+e^{2}+0}, \frac{0}{e^{-3}+e^{2}+0}] = [0.0067, 0.9933, 0.0000]\(\)</p> <p>So we have exactly what we want! The attention weight of the last value is set to 0, so when we are on the second vector \(x_2\), we cannot look forward to the future value vectors \(v_3\), and the remaining parts add up to 1 so its still a probability vector! To do this correctly for the entire matrix, we can just substitute in the top triangle of \(QK^T\) with \(-\infty\). This would look like:</p> <p>\begin{equation} \begin{bmatrix} 7 &amp; -\infty &amp; -\infty <br/> -3 &amp; 2 &amp; -\infty <br/> 1 &amp; 6 &amp; -2 <br/> \end{bmatrix} \end{equation}</p> <p>Taking the softmax of the rows of this matrix then gives:</p> <p>\begin{equation} \text{Softmax} \begin{bmatrix} 7 &amp; -\infty &amp; -\infty <br/> -3 &amp; 2 &amp; -\infty <br/> 1 &amp; 6 &amp; -2 <br/> \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 &amp; 0 <br/> 0.0067 &amp; 0.9933 &amp; 0 <br/> 0.007 &amp; 0.99 &amp; 0.003 <br/> \end{bmatrix} \end{equation}</p> <p>Therefore, the best way to apply out attention mask is by filling the top right triangle with \(-\inf\) and then take the softmax! So lets go ahead and add an option for causality for our attention function we wrote above!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Attention Mask
</span><span class="n">Again</span><span class="p">,</span> <span class="n">we</span> <span class="n">may</span> <span class="n">have</span> <span class="n">a</span> <span class="n">causal</span> <span class="n">mask</span><span class="p">,</span> <span class="n">but</span> <span class="n">that</span> <span class="n">doesn</span><span class="sh">'</span><span class="s">t mean we dont also have an attention mask! Regardless of causality, we want to ensure that tokens never attend to pad tokens. Lets say we have a sequence of 8, but the last 4 tokens are pad tokens, then this would look like this:

seq_len = 8

### Create Causal Mask ###
ones = torch.ones((seq_len, seq_len))
causal_mask = torch.tril(ones).bool()

### Create Padding Mask ###
padding_mask = torch.tensor([1,1,1,1,0,0,0,0]).bool()
padding_mask = padding_mask.unsqueeze(0).repeat(seq_len,1)

### Combine Masks (set positions we dont want in our causal mask to False based on the padding mask) ###
causal_mask = causal_mask.masked_fill(~padding_mask, 0)
causal_mask
</span></code></pre></div></div> <p>As you can see here, we have our causal attention mask as described earlier where the top right triangle is masked out, but for tokens that were pad tokens, the full column is masked out!</p> <h3 id="lets-incorporate-the-causal-mask-into-self-attention">Lets Incorporate the Causal Mask into Self-Attention!</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
               <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
               <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">causal</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">

        Args:
            embed_dim: Transformer Embedding Dimension
            num_heads: Number of heads of computation for Attention
            attn_p: Probability for Dropout2d on Attention cube
            proj_p: Probability for Dropout on final Projection
            causal: Do you want to apply a causal mask?
        </span><span class="sh">"""</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###
</span>        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>

        <span class="c1">### Define all our Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="c1">### Perform Attention Computation ###
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">causal</span><span class="p">:</span>
            <span class="c1">####################################################################################
</span>            <span class="c1">### Create the Causal Mask (On the correct device) ###
</span>
            <span class="c1">### Create a Seq_Len x Seq_Len tensor full of Ones
</span>            <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">attn</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1">### Fill Top right triangle with Zeros (as we dont want to attend to them) ###
</span>            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>

            <span class="c1">### Add extra dimensions for Batch size and Number of Heads ###
</span>            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">seq_len</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>

            <span class="c1">### If we have padding mask, then update our causal mask ###
</span>            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>

                <span class="c1">### Each sample could have a different number of pad tokens, so repeat causal mask for batch size ###
</span>                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1">### Expand and repeat the Padding Mask (b x s) -&gt; (b x 1 x s x s)###
</span>                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

                <span class="c1">### Fill causal mask where attention mask is False with False (to ensure all padding tokens are masked out) ###
</span>                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

            <span class="c1">### Fill attn with -inf wherever causal mask is False ###
</span>            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>

        <span class="c1">####################################################################################
</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>

        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###
</span><span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###
### This will be a tensor upto the max(seq_lens) ###
</span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###
</span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">a</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h2 id="cross-attention">Cross Attention</h2> <p>We have talked about Self-Attention so far, where a model looks at a sequence and tries to learn how every word in that sequence is related to itself! This is totally fine if we are just dealing with that sequence (BERT/GPT), but what if we are translating between two different sequences? For example, the original <a href="https://arxiv.org/pdf/1706.03762">Attention is All You Need</a> paper was about Neural Machine Translation. So if we are doing English to French generation, we don’t only care about how english tokens are related english, and how french tokens are related to french, we also need to learn how english tokens are related to french!!!</p> <p>Luckily this isn’t too bad or different from what we have done so far but we have some new complexities:</p> <p>1) English will have a different length than French (this means our attention matricies of how each english token is related to french is no longer a square!) 2) We still train in batches, so we will have a batch of english and a batch of french. This also means that the enlish tokens will have padding, but the french tokens will have its own padding, and they can be totally different from each other!</p> <h3 id="cross-attention-queries-keys-and-values">Cross Attention Queries, Keys and Values</h3> <p>So the first question, before we had some data that we projected to our Queries, Keys and Values. But now we have two sources of data, English and French. Lets pretend we are translating from English to French. This means to understand how the French is related to the English to inform generating the French we will set the Queries as the projection of our French Embeddings and the Keys/Values as the projection of our English Embeddings. Typically, we set the queries to the thing we care about (what we want to produce), and they keys/values to what we want to relate it to.</p> <p>We can think about this in another way as well: Lets say we are doing Text to Image Generation. Again, because we have two sources we are translating between, we can use Cross Attention. Attention will tell us how pieces of our image are related to the text embeddings. In the end we want the image, so we can set them to the queries, and then the text will be the keys/values, which will be how we want to weight the relation between the image pieces and our text tokens.</p> <h3 id="tensor-shapes-for-cross-attention">Tensor Shapes for Cross Attention</h3> <p>We spent a lot of time understanding the tensor shapes for self-attention, lets do the same thing now for cross attention! We will be using our English to French translation example for this, so lets start with producing our Queries/Keys/Values!</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/cross_atttenion_qkv.png?raw=true" width="600"/> </div> <p>Lets pretend we have an input of French embeddings with \(N\) tokens (with an embedding dimension of \(E\)) and then also English Embeddings with \(L\) tokens (again with an embedding dimension of \(E\)). Again, because the French is what we want, we will project our French embeddings to be our Queries, and then our English Embeddings to our Keys/Values.</p> <ul> <li>French Query Embeddings: (B x N x E)</li> <li>English Key Embeddings: (B x L x E)</li> <li>English Value Embeddings: (B x L x E)</li> </ul> <h3 id="performing-the-attention-computation">Performing the Attention Computation</h3> <p>Nothing changes about our attention formula! This attention computation is identical to earlier, its just because our Queries is of length \(N\) and the Keys are of length \(L\), our final attention matrix will be an (B x N x L) matrix.</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/cross_attention_matmul.png?raw=true" width="600"/> </div> <p>We can take a closer look at our attention matrix now as well, and we see that it isn’t a square matrix anymore, but it still does the same thing: How are each French token related to each English token?</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/cross_attention_mat_vals.png?raw=true" width="600"/> </div> <h4 id="the-most-important-part">THE MOST IMPORTANT PART!!!</h4> <p>Once our attention matrix (B x N x L) is computed, we can again multiply it by our Values matrix which is also from our English embeddings, so we are multiplying a (B x N x L) by a (B x L x E), producing the final output of (B x N x E)! Why does this matter? Because our original French was also (B x N x E)! Remember, our attention computation told us how every French token is related to every English token. Then, the output of our full attention computation does the weighted average, so our new (B x N x E) is the weighted average of those english tokens! Lets see what that looks like:</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/cross_attention_weight_by_val.png?raw=true" width="600"/> </div> <h3 id="basically-nothing-changed">Basically Nothing Changed</h3> <p>Nothing really changed in the end! Cross Attention is just attention between two different sequences rather than than one sequence to itself. This means we also have multihead attention (although I didn’t draw it), so nothing changes at all! Cross Attention is <strong>ALWAYS</strong> an encoder, not autoregressive. It doesnt make sense to have a causal mask here because one sequence can look at the entirety of the other (i.e. our input english can look at all of the generated french). But there is one more caveat:</p> <h3 id="padding-on-english-and-french">Padding on English and French</h3> <p>We are now training a language translation model, this means that when we batch data, both english and french can be padded, and they can be differently padded. For example lets say we have the following data (pairs of english and french tokens)</p> <ul> <li>English: [a1, a2, a3, a4], French: [b1, b2, b3, b4, b5]</li> <li>English: [c1, c2, c3], French: [d1, d2]</li> </ul> <p>This means, when we batch the english we have to pad the sequence of length 3 to the sequence of length 4.</p> <ul> <li>[a1, a2, a3, a4]</li> <li>[c1, c2, c3, <strong>&lt;PAD&gt;</strong>]</li> </ul> <p>And similarly, when we batch the french, we have to pad the sequence of length 2 to the sequence of length 5.</p> <ul> <li>[b1, b2, b3, b4, b5]</li> <li>[d1, d2, <strong>&lt;PAD&gt;</strong>, <strong>&lt;PAD&gt;</strong>, <strong>&lt;PAD&gt;</strong>]</li> </ul> <p>So we have padding on both dimensions, but again nothing changes! Remember before (in self attention), when we have pad tokens we just zeroed out the columns of the cooresponding pad tokens. But really, we are zeroing out the <strong>Keys</strong> columns. We want to make sure that when we learn how the French is related to the English, that the French is <strong>not looking at English Pad Tokens</strong>. It is ok though for French pad tokens to look at english. (i.e. the Query pad tokens can look at the keys, but the Query non-pad tokens should not look at the Key pad tokens). This is what this would again look like:</p> <div> <img src="https://github.com/priyammaz/PyTorch-Adventures/blob/main/src/visuals/cross_attention_padding.png?raw=true" width="600"/> </div> <p>As we can see here, all French tokens (french padding included) can look at all non-padding english tokens! This is identical to what we did before, its just now our attention matrix just isn’t square!</p> <h2 id="implementing-cross-attention">Implementing Cross Attention</h2> <p>Implementing Cross Attention shouldn’t be all that different from before. There is no causality here, so we only have to keep an eye on the attention mask. We will have two attention masks now, one for the english and another for the french. But when doing English to French Translation, we really just care about the English padding mask, so we can remove those columns from our attention computation. We do need the French padding mask though when we are doing french self-attention, but lets focus on the Cross Attention now!</p> <p>In this implementation, we will be passing in both a <code class="language-plaintext highlighter-rouge">src</code> (our English) and <code class="language-plaintext highlighter-rouge">tgt</code> (our French).</p> <h4 id="matching-shapes-for-multihead-attention-1">Matching Shapes for MultiHead Attention</h4> <p>We again have our attention mask and need to do the reshapes and repeats to get it to the shape necessary to mask our attention computation! Lets take a look at our shapes for Cross Attention again.</p> <p><code class="language-plaintext highlighter-rouge">attn.shape</code> - (Batch x num_heads x french_seq_len x english_seq_len)</p> <p><code class="language-plaintext highlighter-rouge">mask.shape</code> - (Batch x english_seq_len)</p> <p>So just like before, we can take our mask, and add dimensions from (Batch x english_seq_len) to (Batch x 1 x 1 x english_seq_len) which would be enough, except our Flash Attention implementation will expect a (Batch x 1 x french_seq_len x english_seq_len). So then we can take our (Batch x 1 x 1 x english_seq_len) and repeat the dummy 1 dimension which is a placeholder for the french_seq_len and repeat it french_seq_len times!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Cross Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
               <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
               <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
               <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">

        Args:
            embed_dim: Transformer Embedding Dimension
            num_heads: Number of heads of computation for Attention
            attn_p: Probability for Dropout2d on Attention cube
            proj_p: Probability for Dropout on final Projection
        </span><span class="sh">"""</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">CrossAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###
</span>        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>

        <span class="c1">### Define all our Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">src_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">tgt_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q (on our tgt French) ###
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">tgt</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tgt_seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="c1">### Compute K, V (on src English) Projections ###
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="c1">### Perform Attention Computation ###
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="c1">####################################################################################
</span>        <span class="c1">### FILL ATTENTION MASK WITH -Infinity ###
</span>
        <span class="c1">### NOTE:
</span>        <span class="c1">### attn.shape - (Batch x num_heads x french_seq_len x english_seq_len)
</span>        <span class="c1">### mask.shape - (Batch x english_seq_len)
</span>
        <span class="c1">### Need to expand mask (Batch x english_seq_len) -&gt; (Batch x 1 x 1 x english_seq_len) -&gt; (Batch x 1 x french_seq_len x english_seq_len)
</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tgt_seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>

        <span class="c1">####################################################################################
</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>

        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tgt_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="c1">### We will now have sequences of different lengths, identify the number of tokens in each sequence ###
</span><span class="n">english_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">french_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">18</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###
### This will be a tensor upto the max(seq_lens) ###
</span><span class="n">rand_english</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>
<span class="n">rand_french</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>


<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###
</span><span class="n">english_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">english_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>
<span class="n">french_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">french_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">French Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">french_masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">a</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">rand_english</span><span class="p">,</span> <span class="n">tgt</span><span class="o">=</span><span class="n">rand_french</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <h3 id="flash-attention">Flash Attention</h3> <p>Although our attention computations that we have done are completely correct, they are not the most efficient! <a href="https://github.com/Dao-AILab/flash-attention">Flash Attention</a> is a highly optimized hardware aware attention implementation. Due to the structure of the attention computation you can actually opt for a faster tiled matrix multiplication that fuses the matrix multiplication, softmax and scaling into a single CUDA kernel. We were obviously doing this as separate steps, beacause we don’t have that level of control in PyTorch over CUDA. The most expensive part of GPU operations is copying between global memory and GPU shared memory, and so by merging a bunch of operations together, we just get faster attention computations especially for long sequences.</p> <p>There are a few places we can access this, but the easiest for us is to use <code class="language-plaintext highlighter-rouge">torch.scaled_dot_product_attention</code> (<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">documentation</a></p> <p>This method allows us to pass in:</p> <ul> <li><code class="language-plaintext highlighter-rouge">Queries</code>: (B x H x L x E)</li> <li><code class="language-plaintext highlighter-rouge">Keys</code>: (B x H x S x E)</li> <li><code class="language-plaintext highlighter-rouge">Values</code>: (B x H x S x E)</li> <li><code class="language-plaintext highlighter-rouge">attn_mask</code>: (B x 1 x L x S)</li> </ul> <p>So as you can see, Flash Attention supports our <code class="language-plaintext highlighter-rouge">Queries</code> being different from <code class="language-plaintext highlighter-rouge">Keys</code>/<code class="language-plaintext highlighter-rouge">Values</code> as we saw in our Cross Attention implementation. In the case of self-attention \(L = S\) which also isn’t a problem. The only extra step on our end is to make sure our <code class="language-plaintext highlighter-rouge">attn_mask</code> is of shape (B x 1 x L x S), which means we have to do the extra repeat, as it wont automatically broadcast along the \(L\) dimension if we left it as (B x 1 x 1 x S). Flash Attention also expects tokens we don’t want to compute attention on to be <code class="language-plaintext highlighter-rouge">False</code> in the attention mask!</p> <p>The other option that is important it:</p> <ul> <li>is_causal: True/False Boolean indicating if we want a causal mask. The method will apply the causal mask itself so we don’t need to do it!</li> </ul> <h3 id="lets-add-flash-attention-to-our-self-attention">Lets Add Flash Attention to our Self Attention!</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                 <span class="n">attn_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">proj_p</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">fused_attn</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">

        Args:
            embed_dim: Transformer Embedding Dimension
            num_heads: Number of heads of computation for Attention
            attn_p: Probability for Dropout2d on Attention cube
            proj_p: Probability for Dropout on final Projection
            causal: Do you want to apply a causal mask?
        </span><span class="sh">"""</span>

        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1">### Make Sure Embed Dim is Divisible by Num Heads ###
</span>        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fused</span> <span class="o">=</span> <span class="n">fused_attn</span>

        <span class="c1">### Define all our Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_p</span><span class="p">)</span>

        <span class="c1">### Define Post Attention Projection ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1">### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

        <span class="c1">### Use Flash Attention ###
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">fused</span><span class="p">:</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">,</span>
                                               <span class="n">is_causal</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">causal</span><span class="p">,</span>
                                               <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="c1">### Perform Attention Computation ###
</span>            <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>



            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">causal</span><span class="p">:</span>
                <span class="c1">####################################################################################
</span>                <span class="c1">### Create the Causal Mask (On the correct device) ###
</span>
                <span class="c1">### Create a Seq_Len x Seq_Len tensor full of Ones
</span>                <span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">attn</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

                <span class="c1">### Fill Top right triangle with Zeros (as we dont want to attend to them) ###
</span>                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>

                <span class="c1">### Add extra dimensions for Batch size and Number of Heads ###
</span>                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="n">seq_len</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>

                <span class="c1">### If we have padding mask, then update our causal mask ###
</span>                <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>

                    <span class="c1">### Each sample could have a different number of pad tokens, so repeat causal mask for batch size ###
</span>                    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                    <span class="c1">### Expand and repeat the Padding Mask (b x s) -&gt; (b x 1 x s x s)###
</span>                    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

                    <span class="c1">### Fill causal mask where attention mask is False with False (to ensure all padding tokens are masked out) ###
</span>                    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

                <span class="c1">### Fill attn with -inf wherever causal mask is False ###
</span>                <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>

            <span class="c1">####################################################################################
</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">v</span>

        <span class="c1">### Bring Back to [Batch x Seq Len x Embed Dim] ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1">### Pass through Projection so Heads get to know each other ###
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="putting-it-all-together">Putting it All Together!</h3> <p>Obviously, we would like to use Flash Attention for Everything, but we have a few moving pieces right now:</p> <p>1) We can have self-attention on a source with padding mask 2) We can have self-attention on a source with padding mask and causal mask 3) We can have cross attention between a source and target</p> <p>Lets write our final Attention computation, putting all of these things together! This is the Attention mechanism we will be using later for our full implementation of Neural Machine Translation with Transformers!</p> <p>Again, lets set the stage in terms of and English (src) to French (tgt) translation</p> <h3 id="attention-class-details">Attention Class Details</h3> <p>This class will handle all the cases we need. Lets pretend we are doing English to French</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- We can provide English as the src along with its padding mask for Encoder self-attention
- We can provide French as the src along with its padding mask and causal as True for decoder self-attention
- We can provide English as src and French as tgt along with the src padding_mask for cross attention
</code></pre></div></div> <p>All of this should be very familiar now as we have implemented all the pieces of this already! Its just time to put it together!</p> <h3 id="attention-mask-for-self-attention">Attention Mask for Self-Attention</h3> <p>Attention Mask is in (Batch x Sequence Length) where we have False for tokens we don’t want to attend to. F.scaled_dot_product_attention expects a mask of the shape (Batch x …, x Seq_len x Seq_len) the “…” in this case is any extra dimensions (such as heads of attention). lets expand our mask to (Batch x 1 x Seq_len x Seq_len) The 1 in this case refers to the number of heads of attention we want, so it is a dummy index to broadcast over In each (Seq_len x Seq_len) matrix for every batch, we want False for all columns corresponding to padding tokens</p> <h3 id="attention-mask-for-cross-attention">Attention Mask for Cross Attention</h3> <p>When doing cross attention, our French will be (Batch x french_len x embed_dim) and our English will be (Batch x english_len x embed_dim) In typical cross attention fashion, the queries will be the thing we want and Keys/Values will be the thing we are crossing with. In our Decoder Cross Attention, we want to learn how our generated French is related to the encoded english from the Encoder. So our Queries will be French and Keys/Values will be the encoded English.</p> <p>Q @ K^T will then give a shape (Batch x … x french_len x english_len). This means our attention mask also has to have this shape! Just like before, we want to mask out the columns of the attention mask, so our french tokens dont attend to any english padding tokens. We can then take our english padding mask which is (Batch x english_len), add extra dimensions for head and src_len dimension which will give a (Batch x 1 x 1 x english_len) and then repeat the mask for the source length (batc x 1 x french_len x english_len)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Regular Self-Attention but in this case we utilize flash_attention
    incorporated in the F.scaled_dot_product_attention to speed up our training.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embedding_dimension</span> <span class="o">=</span> <span class="n">embedding_dimension</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

        <span class="c1">### Sanity Checks ###
</span>        <span class="k">assert</span> <span class="n">embedding_dimension</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">Double check embedding dim divisible by number of heads</span><span class="sh">"</span>

        <span class="c1">### Attention Head Dim ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embedding_dimension</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1">### Attention Projections ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>

        <span class="c1">### Post Attention Projection ###
</span>        <span class="n">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
                <span class="n">src</span><span class="p">,</span>
                <span class="n">tgt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">causal</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="sh">"""</span><span class="s">
        By default, self-attention will be computed on src (with optional causal and/or attention mask). If tgt is provided, then
        we are doing cross attention. In cross attention, an attention_mask can be used, but no causal mask can be applied.
        </span><span class="sh">"""</span>

        <span class="c1">### Grab Shapes ###
</span>        <span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">src</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1">### If target is not provided, we are doing self attention (with potential causal mask) ###
</span>        <span class="k">if</span> <span class="n">tgt</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>

                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">bool</span><span class="p">()</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">src_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">attention_out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">,</span>
                                                           <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                                                           <span class="n">dropout_p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
                                                           <span class="n">is_causal</span><span class="o">=</span><span class="n">causal</span><span class="p">)</span>

        <span class="c1">### If target is provided then we are doing cross attention ###
</span>        <span class="c1">### Our query will be the target and we will be crossing it with the encoder source (keys and values) ###
</span>        <span class="c1">### The src_attention_mask will still be the mask here, just repeated to the target size ###
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_len</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">tgt</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">src</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">src_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>

                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">bool</span><span class="p">()</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">tgt_len</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">attention_out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">,</span>
                                                           <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                                                           <span class="n">dropout_p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
                                                           <span class="n">is_causal</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1">### Reshape and Project ###
</span>        <span class="n">attention_out</span> <span class="o">=</span> <span class="n">attention_out</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">attention_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_proj</span><span class="p">(</span><span class="n">attention_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attention_out</span>

<span class="c1">### Test Out Self-Attention!! ###
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TESTING SELF-ATTENTION!!!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-------------------------</span><span class="sh">"</span><span class="p">)</span>
<span class="n">seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###
### This will be a tensor upto the max(seq_lens) ###
</span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###
</span><span class="n">masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">a</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">masks</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TESTING CROSS-ATTENTION!!!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">-------------------------</span><span class="sh">"</span><span class="p">)</span>
<span class="c1">### Test out Cross Attention
### We will now have sequences of different lengths, identify the number of tokens in each sequence ###
</span><span class="n">english_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">french_seq_lens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>

<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">a</span> <span class="o">=</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>

<span class="c1">### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###
### This will be a tensor upto the max(seq_lens) ###
</span><span class="n">rand_english</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">english_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>
<span class="n">rand_french</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="nf">max</span><span class="p">(</span><span class="n">french_seq_lens</span><span class="p">),</span><span class="n">embed_dim</span><span class="p">)</span>


<span class="c1">### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###
</span><span class="n">english_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">english_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>
<span class="n">french_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pad_sequence</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">french_seq_lens</span><span class="p">],</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">French Attention Mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">french_masks</span><span class="p">)</span>

<span class="c1">### Pass through MHA ###
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">a</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">rand_english</span><span class="p">,</span> <span class="n">tgt</span><span class="o">=</span><span class="n">rand_french</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">english_masks</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Final Output:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="transformers,"/><category term="attention,"/><category term="flash-attention"/><summary type="html"><![CDATA[Exploring attention from RNN to FlashAttention]]></summary></entry><entry><title type="html">Can AI Achieve True Creativity?</title><link href="https://xmarva.github.io/blog/2025/creative-ai/" rel="alternate" type="text/html" title="Can AI Achieve True Creativity?"/><published>2025-02-13T15:00:00+00:00</published><updated>2025-02-13T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/creative-ai</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/creative-ai/"><![CDATA[<p>The criticism that AI cannot create something fundamentally new often overlooks a key fact: human creativity itself is built on combining existing knowledge, concepts, and imagery.</p> <p>Modern research reveals that when humans invent something novel, the brain doesn’t conjure elements “from nothing” but recombines fragments of prior experiences. Neural networks operate similarly.</p> <p>When GPT-4 generates text, it relies on statistical patterns learned from existing data rather than conscious intent.</p> <h3 id="the-brains-creative-networks-vs-gans">The Brain’s Creative Networks vs. GANs</h3> <p>In one experiment, participants were asked to devise unconventional uses for everyday objects (e.g., a coffee cup). fMRI scans showed two activated networks during creativity:</p> <ul> <li>The default mode network (associated with daydreaming and associations)</li> <li>The frontoparietal network (linked to focus and control)</li> </ul> <p>The dorsolateral prefrontal cortex also activates during musical improvisation. This interaction mirrors generative adversarial networks (GANs), where one component generates ideas and another evaluates their plausibility. Both biological and artificial systems balance freedom and constraints to produce novelty.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/0-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/0-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="fMRI activation during creative tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fMRI activation patterns during creative tasks (Source: Beaty et al., 2016) </div> <h3 id="the-limits-of-perception-and-data">The Limits of Perception and Data</h3> <p>Human perception constrains creativity. We can’t imagine colors beyond the visible spectrum—any new shade is a recombination of known hues. Similarly, Stable Diffusion can’t generate images of objects absent from its training data. It blends learned features, much like the brain combines memories.</p> <p>Blind individuals describe colors through analogies to sounds or textures (e.g., red as “loud noise,” blue as “smooth surface”). This suggests creativity is rooted in sensory experience—a dimension AI lacks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/1-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/1-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stable Diffusion-generated floral carpet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Floral carpet generated by Stable Diffusion </div> <h3 id="embodied-vs-abstract-learning">Embodied vs. Abstract Learning</h3> <p>A key difference lies in how humans and AI acquire knowledge:</p> <ul> <li>Humans learn through physical interaction (e.g., toddlers touching objects to link tactile and visual data)</li> <li>AI processes abstract tokens/pixels</li> </ul> <p>While LLMs develop semantic relationships (e.g., “apple” vectors near “fruit” and “tree”), they lack embodied experiences stored in the brain’s sensorimotor cortex. When we think “run,” motor neurons activate—a connection AI can’t replicate.</p> <h3 id="the-myth-of-intentional-creativity">The Myth of Intentional Creativity</h3> <p>Critics argue humans possess creative “intent,” but jazz improvisation studies show decreased prefrontal cortex activity during spontaneous creation. Ideas emerge automatically from learned patterns—a process strikingly similar to how neural networks operate.</p> <h3 id="the-originality-debate">The Originality Debate</h3> <p>If human innovation is recombination, demanding absolute novelty from AI is flawed. Both are constrained by their “training data”:</p> <ul> <li>Human brains: Biological experiences</li> <li>AI models: Digital datasets</li> </ul> <p>The distinction lies in complexity and emotional embodiment. While AI manipulates mathematical structures, the brain ties patterns to emotions and bodily states—for now.</p> <p><strong>References</strong><br/> [1] Beaty, R. E., Benedek, M., Silvia, P. J., &amp; Schacter, D. L. (2016). Creative Cognition and Brain Network Dynamics. <em>Trends in Cognitive Sciences</em><br/> [2] De Borst, A. W., &amp; de Gelder, B. (2018). Mental Imagery Follows Similar Cortical Reorganization as Perception: Intra-Modal and Cross-Modal Plasticity in Congenitally Blind. <em>Cerebral Cortex</em><br/> [3] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dea, J. (2013). Distributed Representations of Words and Phrases and their Compositionality<br/> [4] Limb, C. J., &amp; Braun, A. R. (2008). Neural Substrates of Spontaneous Musical Performance: An fMRI Study of Jazz Improvisation</p>]]></content><author><name></name></author><category term="featured-posts"/><category term="ai,"/><category term="neuroscience,"/><category term="creativity"/><summary type="html"><![CDATA[Exploring the parallels between human creativity and neural networks]]></summary></entry><entry><title type="html">Algebraic Foundations of Low-Rank Adaptation</title><link href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/" rel="alternate" type="text/html" title="Algebraic Foundations of Low-Rank Adaptation"/><published>2024-12-30T15:09:00+00:00</published><updated>2024-12-30T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/lora-algorithm-for-llms</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><![CDATA[<h2 id="the-paradox-of-scale">The Paradox of Scale</h2> <p>The evolution of language models presents us with an intriguing paradox: while increasing model size enhances general capability, it simultaneously complicates practical deployment through prohibitive computational demands. This tension between capacity and practicality forms the crucible where Low-Rank Adaptation (LoRA) emerges as an elegant solution. To understand its mechanisms, we must first establish fundamental mathematical constructs.</p> <h2 id="matrix-theory-foundations">Matrix Theory Foundations</h2> <h3 id="the-algebraic-scaffolding">The Algebraic Scaffolding</h3> <p>A matrix \(A \in \mathbb{R}^{m \times n}\) represents a linear transformation between vector spaces \(\mathbb{R}^n \to \mathbb{R}^m\). Each element \(a_{ij}\) encodes the transformation coefficient between basis vectors \(e_j\) and \(e_i\). In neural networks, these matrices become learned representations of feature interactions.</p> <p>The <strong>rank</strong> of a matrix, denoted \(\rho(A)\), measures its column space dimensionality through the maximal number of linearly independent columns. Formally:</p> \[\rho(A) = \dim(\text{col}(A)) = \dim(\text{row}(A))\] <p>This duality between row and column space dimensionalities (proven via the Fundamental Theorem of Linear Algebra) becomes crucial for understanding parameter efficiency.</p> <h3 id="rank-constrained-transformations">Rank-Constrained Transformations</h3> <p>Consider two matrices \(B \in \mathbb{R}^{m \times r}\) and \(A \in \mathbb{R}^{r \times n}\). Their product \(BA\) inherently satisfies:</p> \[\rho(BA) \leq \min(\rho(B), \rho(A)) \leq r\] <p>This rank upper bound enables dramatic parameter reduction when \(r \ll \min(m,n)\). For a neural layer with \(m \times n\) weights, replacing full updates with low-rank factors reduces trainable parameters from \(mn\) to \(r(m+n)\) – an efficiency gain of \(\frac{mn}{r(m+n)}\). For typical layers (\(m,n \sim 10^3\), \(r \sim 10^1\)), this yields ~100x parameter reduction.</p> <h2 id="the-low-rank-adaptation-hypothesis">The Low-Rank Adaptation Hypothesis</h2> <h3 id="intrinsic-dimensionality-of-task-adaptation">Intrinsic Dimensionality of Task Adaptation</h3> <p>Modern language models exhibit an intriguing property: while pretrained on broad corpora, task-specific adaptation appears to operate in low-dimensional subspaces. This phenomenon aligns with the <strong>manifold hypothesis</strong>, suggesting high-dimensional data actually resides on lower-dimensional manifolds.</p> <p>Let \(\Delta W \in \mathbb{R}^{m \times n}\) represent weight updates during fine-tuning. The LoRA conjecture posits:</p> \[\rho(\Delta W) \leq r \ll \min(m,n)\] <p>Experimental validation shows task adaptation often requires surprisingly low ranks (\(r=8\) achieves strong performance). This implies that while the original parameter space is vast, task-specific adjustments occupy a small subspace.</p> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Visualize the weight matrix as a point in \(\mathbb{R}^{mn}\). Full fine-tuning moves this point through the high-dimensional space. LoRA constrains movement to a low-dimensional <strong>adaptation manifold</strong> spanned by \(B\) and \(A\):</p> \[\mathcal{M}_r = \{ W + BA \mid B \in \mathbb{R}^{m \times r}, A \in \mathbb{R}^{r \times n} \}\] <p>The approximation error is bounded by the Eckart–Young theorem:</p> \[\min_{\rho(BA)\leq r} \| \Delta W - BA \|_F = \sum_{i=r+1}^{\min(m,n)} \sigma_i(\Delta W)\] <p>where \(\sigma_i\) denotes singular values. Rapidly decaying singular values in \(\Delta W\) (as observed empirically) enable accurate low-rank approximation.</p> <h2 id="algorithmic-implementation">Algorithmic Implementation</h2> <h3 id="parameterization-and-initialization">Parameterization and Initialization</h3> <p>For a pretrained weight matrix \(W_0\), LoRA constructs:</p> \[W = W_0 + \frac{\alpha}{r}BA\] <p>Where:</p> <ul> <li>\(B\) initialized with \(\mathcal{N}(0, \sigma^2)\)</li> <li>\(A\) initialized to zero</li> <li>\(\alpha\): learning rate scaling factor</li> </ul> <p>The initialization strategy ensures \(\Delta W = 0\) at training onset, preserving original model behavior. The \(\alpha/r\) scaling normalizes parameter updates across different ranks, maintaining stable learning dynamics.</p> <h3 id="gradient-dynamics">Gradient Dynamics</h3> <p>Let \(\mathcal{L}\) be the loss function. The gradient through the LoRA parameters becomes:</p> \[\nabla_B \mathcal{L} = \frac{\alpha}{r} (\nabla_{W} \mathcal{L}) A^T \\ \nabla_A \mathcal{L} = \frac{\alpha}{r} B^T (\nabla_{W} \mathcal{L})\] <p>This reveals an important property: gradient signals flow through both low-rank factors, with the scaling term modulating update magnitudes. The rank \(r\) therefore acts as a gradient multiplier – higher ranks enable stronger gradient signals but increase parameter count.</p> <h2 id="practical-considerations-and-variations">Practical Considerations and Variations</h2> <h3 id="rank-selection-tradeoffs">Rank Selection Tradeoffs</h3> <p>The choice of \(r\) balances expressivity vs efficiency:</p> <ul> <li><strong>Lower ranks (r=1-4):</strong> Maximize parameter efficiency, suitable for similar source/target tasks</li> <li><strong>Medium ranks (r=8-16):</strong> General-purpose setting for domain adaptation</li> <li><strong>Higher ranks (r=32+):</strong> Needed for complex task transfers or low-data scenarios</li> </ul> <p>Empirical studies show performance follows logarithmic scaling:</p> \[\text{Performance}(r) \approx \text{Performance}(\text{full}) - c/\log r\] <p>Where \(c\) is task-dependent. This suggests diminishing returns beyond certain ranks.</p> <h3 id="architectural-variants">Architectural Variants</h3> <ol> <li><strong>Bottleneck Adaptation:</strong> Stack multiple low-rank layers (\(W_0 + B_1A_1 + B_2A_2\)) for hierarchical adaptation</li> <li><strong>Sparse LoRA:</strong> Combine with magnitude pruning on \(BA\) product</li> <li><strong>Dynamic Rank Allocation:</strong> Use singular value thresholds to automatically select per-layer ranks</li> <li><strong>LoRA++:</strong> Introduce learned scaling factors per layer instead of fixed \(\alpha/r\)</li> </ol> <h3 id="compositional-adaptation">Compositional Adaptation</h3> <p>For multi-task learning, LoRA enables parameter composition:</p> \[W = W_0 + \sum_{k=1}^K B_kA_k\] <p>Where each \(B_kA_k\) captures task-specific adaptations. During inference, select subsets of adapters via:</p> \[W = W_0 + \sum_{k \in S} B_kA_k\] <p>This facilitates efficient multi-task serving with \(\mathcal{O}(Kr)\) storage instead of \(\mathcal{O}(K)\) full models.</p> <h2 id="theoretical-implications">Theoretical Implications</h2> <h3 id="implicit-regularization">Implicit Regularization</h3> <p>The low-rank constraint acts as a strong regularizer, preventing overfitting to small datasets. Consider the Rademacher complexity for a LoRA-adapted layer:</p> \[\mathcal{R}_n(\mathcal{H}_{\text{LoRA}}) \leq \frac{\alpha \sqrt{2r\log(2mn)}}{n}\] <p>Compared to full fine-tuning’s \(\mathcal{O}(\sqrt{mn/n})\) complexity, LoRA’s bound is significantly tighter, explaining its improved generalization in low-data regimes.</p> <h3 id="information-bottleneck-perspective">Information Bottleneck Perspective</h3> <p>Interpreting through the information bottleneck lens, LoRA enforces:</p> \[\min_{B,A} I(W; BA) \quad \text{s.t.} \quad I(BA; \mathcal{T}) \geq I_c\] <p>Where \(\mathcal{T}\) is the target task and \(I_c\) the required information. The low-rank structure naturally minimizes irrelevant information from \(W\) while preserving task-relevant features.</p> <h2 id="epilogue">Epilogue</h2> <p>LoRA epitomizes the principle that profound solutions often arise from deep mathematical insight rather than brute-force computation. By reconceptualizing adaptation as a low-rank update process, it achieves an elegant synthesis of efficiency and effectiveness – a reminder that in machine learning as in mathematics, constraints often breed creativity.</p> <p>The road ahead suggests intriguing possibilities: could other matrix properties (e.g., sparsity patterns, eigenvalue distributions) inspire new adaptation paradigms? As language models continue evolving, such algebraic perspectives will likely remain essential tools for harnessing their potential.</p>]]></content><author><name></name></author><category term="nlp,"/><category term="llm,"/><category term="lora"/><summary type="html"><![CDATA[Mathematical exploration of parameter-efficient fine-tuning through matrix rank theory]]></summary></entry><entry><title type="html">LLMs for Those Who Missed Out</title><link href="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/" rel="alternate" type="text/html" title="LLMs for Those Who Missed Out"/><published>2024-04-24T15:09:00+00:00</published><updated>2024-04-24T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/"><![CDATA[<p>An LLM (large language model) consists of just two files:</p> <ul> <li>Large file with billions of parameters (weights)</li> <li>Small file with about 500 lines of code to run the model</li> </ul> <p>LLM doesn’t take up much space and doesn’t require extensive computing power.</p> <p>For example, the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama-2–70b model</a> from Meta, which is open source and has 70 billion parameters, is just 140 GB and can be run locally on a MacBook without internet access. You can <a href="https://www.llama.com/llama-downloads/">download this model</a> from Meta’s website and use it for free.</p> <p>However, you need significant computing resources to get these parameters (train the model).</p> <h2 id="how-to-train-a-model">How to Train a Model</h2> <h3 id="stage-1-pretraining-training-the-base-model">Stage 1: Pretraining (Training the base model)</h3> <p>Think of this process as compressing the Internet into a neural network, similar to how a ZIP file compresses documents into an archive.</p> <p>Here’s what you need to get a model like Llama-2:</p> <ul> <li>Take a “slice of the internet,” approximately ten terabytes of text in size</li> <li>Use a cluster of ~6,000 GPUs (specialized graphics processors used for heavy computations)</li> <li>Compress the text into the neural network. This takes about 12 days and costs around $2M</li> <li>Acquire the base model (the file with parameters)</li> </ul> <p>The primary function of the base model is to predict the next word. You input a sequence of words, and it outputs the next most likely word based on the texts it was trained on.</p> <p>For example, for the phrase “cat sat on a,” the LLM might predict the word “mat” with 97% probability.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/0-480.webp 480w,/assets/img/posts/2024-04-24-llms/0-800.webp 800w,/assets/img/posts/2024-04-24-llms/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image from the presentation illustrating how the neural network works" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image from the presentation illustrating how the neural network works </div> <p>That’s basically how compression works: if you can predict the next word accurately, you can use this information to compress the original data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/1-480.webp 480w,/assets/img/posts/2024-04-24-llms/1-800.webp 800w,/assets/img/posts/2024-04-24-llms/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="I checked, and ChatGPT indeed responds with “mat”" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> I checked, and ChatGPT indeed responds with “mat” </div> <p>However, unlike a ZIP file, where compression is lossless, LLM “compresses” the internet with losses. This is why models can hallucinate: they make up information that doesn’t actually exist but appears plausible (similar to what they have seen in the training data).</p> <p>Moreover, models can sometimes produce completely unexpected things. For example, ChatGPT knows the answer to the question, “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer), but it doesn’t know the answer to the question, “Who is Mary Lee Pfeiffer’s son?”</p> <p>This isn’t a typical database that simply stores information; it’s a different format we don’t fully understand.</p> <p>This clearly illustrates that we don’t quite grasp how this whole thing works and can only see the results it produces.</p> <h3 id="stage-2-finetuning-additional-training">Stage 2: Finetuning (Additional training)</h3> <p>The base model isn’t very practical for everyday use. We don’t just want to receive continuous word sequences; we want to ask questions and get answers.</p> <p>This requires finetuning — a process in which we develop an assistant model that answers questions.</p> <p>The training process for the assistant model is similar to that of the base model, but now we train the model not on internet texts but on data we manually collect. Companies hire people who write questions and answer them.</p> <p>If the training of the base model occurs on huge volumes of often low-quality text, the training of the assistant model involves comparatively smaller volumes (say, 100,000 documents), but they are all of very high quality.</p> <p>After such finetuning, the model understands that when asked a question, it should respond in the style of a helpful assistant.</p> <p>Unlike the training of the base model, which is conducted at most 2–3 times a year, finetuning can be done regularly, even weekly, as it is much less costly.</p> <h3 id="stage-3-optional-comparisons">Stage 3 (optional): Comparisons</h3> <p>In many cases it’s easier not to write a response from scratch but to compare several different answer options and choose the best one.</p> <p>The data from these comparisons is used to train the model further.</p> <p>At OpenAI, this is called RLHF (Reinforcement Learning from Human Feedback).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/2-480.webp 480w,/assets/img/posts/2024-04-24-llms/2-800.webp 800w,/assets/img/posts/2024-04-24-llms/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Difference between Open and Closed Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Difference between Open and Closed Models. Source: Chatbot leaderboards </div> <p>Models with open weights, such as Llama-2, are still inferior compared to proprietary models like GPT-4 and Claude.</p> <p>However, proprietary models cannot be downloaded for personal use or further training; they can only be operated through a web interface or an API (though the introduction of Custom GPTs at OpenAI is a first step towards customization)</p> <h2 id="capabilities-of-llms-using-chatgpt-as-an-example">Capabilities of LLMs (using ChatGPT as an example)</h2> <p>Prompted by an LLM, it can understand what the user needs and use external tools for this:</p> <ul> <li>Search the internet if the user needs up-to-date information (goes to Bing, retrieves a list of the latest links for the query, copies the full text of each link into its context window, and summarizes the information)</li> <li>Use a calculator to do calculations</li> <li>Use a Python library to draw graphs</li> <li>Draw images using DALL-E</li> <li>Write code</li> </ul> <p>Moreover, LLMs can be multimodal. They can recognize text, voice, images, or video and respond with voice, images, or video.</p> <h2 id="can-llms-make-decisions">Can LLMs Make Decisions?</h2> <p>There’s a well-known book by Kahneman, “Thinking, Fast and Slow.”</p> <p>The main idea of the book is that there are two systems in the brain: System 1, which has fast, automatic reactions, and System 2, which is slower, rational, and conscious and makes complex decisions.</p> <p>For example, the answer to question 2+2 comes from System 1 because this knowledge is automatic for us. But calculating 17×24 requires System 2.</p> <p>If we apply these terms, current LLMs only possess System 1. They can only provide the most likely next word in real time.</p> <p>It would be great if we could come to an LLM and say: here’s my question; you can think for 30 minutes, but I need a very accurate and high-quality answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/3-480.webp 480w,/assets/img/posts/2024-04-24-llms/3-800.webp 800w,/assets/img/posts/2024-04-24-llms/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Thought tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Thought tree </div> <p>No models can do that yet. But it would be desirable for a model to have something like a “thought tree,” through which it could navigate, analyze the results, go back and try again until it achieves a result it is most confident about.</p> <h2 id="can-llms-train-themselves">Can LLMs Train Themselves?</h2> <p>There’s a famous case where AlphaGo (a program that plays Go) was trained in two stages:</p> <p>First, it was trained on human games and learned to play very well. Then it began to train itself-playing against itself, trying to maximize the likelihood of winning-and significantly improved its quality.</p> <p>But in LLMs, we’re only at stage 1-training only occurs on materials created by humans.</p> <p>Why is this?</p> <p>In the game of Go, there is a very clear criterion for success-a won game- and you can train the model to maximize the likelihood of winning.</p> <p>In LLMs, the criteria are not so obvious. It’s not clear how to assess whether the result is good.</p> <p>Such criteria can be found in some narrow topics, but in general, it’s still hard to imagine.</p> <h2 id="what-llms-will-be-able-to-do-in-a-few-years">What LLMs Will Be Able to Do in a Few Years</h2> <p>Finally, a small forecast.</p> <p>In a few years, LLMs:</p> <ul> <li>Will have more knowledge than any human on all subjects</li> <li>Can operate on the internet</li> <li>Can use existing software infrastructure (calculator, Python, mouse/keyboard)</li> <li>Can see and generate images and videos</li> <li>Can hear and speak, and also generate music</li> <li>Can think for a long period using System 2</li> <li>Can train themselves in areas where there are clear criteria</li> <li>Can be customized and refined for specific tasks. Many versions will exist in app stores</li> <li>Can interact with other LLMs</li> </ul>]]></content><author><name></name></author><category term="old-posts"/><category term="nlp,"/><category term="llm"/><summary type="html"><![CDATA[Let's talk about large language models. Once again.]]></summary></entry><entry><title type="html">How to Write Good Python Code</title><link href="https://xmarva.github.io/blog/2023/python-code/" rel="alternate" type="text/html" title="How to Write Good Python Code"/><published>2023-02-07T15:09:00+00:00</published><updated>2023-02-07T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2023/python-code</id><content type="html" xml:base="https://xmarva.github.io/blog/2023/python-code/"><![CDATA[<p>Python is a fantastic programming language!</p> <p>It can be used for many things, like building websites, exploring data, and teaching machines to learn.</p> <p>If you already know Python or are just beginning, writing code that is strong, easy to read, and easy to keep up with is important.</p> <p>In this bogpost, we’ll look at the basic rules for writing great Python code and share some tips to help you make your programs even better.</p> <h2 id="-use-meaningful-naming-conventions">📚 Use Meaningful Naming Conventions</h2> <p>One of the most important aspects of good Python code is meaningful naming conventions.</p> <p>Choosing descriptive and concise names for variables, functions, and classes can help make your code more readable and understandable.</p> <p>Using proper naming conventions can also help you avoid naming conflicts, reduce the risk of errors, and simplify maintenance.</p> <p>For example, these are bad variable names:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>And these are better ones:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_number</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">second_number</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sum_of_numbers</span> <span class="o">=</span> <span class="n">first_number</span> <span class="o">+</span> <span class="n">second_number</span>
<span class="n">double_sum</span> <span class="o">=</span> <span class="n">sum_of_numbers</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>This includes using proper indentation, white space, line breaks, and following a code style guide like the PEP 8 style guide.</p> <p>Clear, organized code makes it easier to understand and modify and reduces the risk of errors.</p> <p>Here’s an example of lousy code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sum: </span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Difference: </span><span class="sh">"</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Product: </span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Quotient: </span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
</code></pre></div></div> <p>And here’s an example of good code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Sum</span><span class="sh">"</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Difference</span><span class="sh">"</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Product</span><span class="sh">"</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Quotient</span><span class="sh">"</span><span class="p">:</span> <span class="n">q</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h2 id="-write-comments">💬 Write Comments</h2> <p>Adding comments to your code is a great way to explain what it does and provide context for other developers.</p> <p>Comments should be used to explain complex code, provide additional information about the purpose of the code, and describe your thought process.</p> <p>Writing comments can also help you better understand your code when you return to it later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># function to calculate sum
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># this function calculates sum of two numbers
</span><span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments are not very descriptive or helpful in understanding the purpose of the functions.</p> <p>The first comment is trivial and adds no additional information. The second comment repeats what the function name already tells us.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the sum of two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns their sum.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the sum of `a` and `b`
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the difference between two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns the difference of `a` and `b`.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the difference between `a` and `b`
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments provide a clear and concise explanation of the purpose and behaviour of each function.</p> <p>The use of docstrings makes it easy to understand what the functions do and what arguments they take in. This makes the code more readable and maintainable.</p> <h2 id="-use-modules-and-packages">🧰 Use Modules and Packages</h2> <p>Modules and packages are a great way to organize your code into reusable blocks.</p> <p>They allow you to group related code together and make it easier to manage, understand, and maintain.</p> <p>The Python Standard Library is an good resource for finding pre-existing modules and packages. You can import it into your programs to save time and effort.</p> <p>Consider a project to build a simple weather application that provides a given city’s current temperature and conditions. We can structure the project as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weather_app/
    __init__.py
    weather.py
    utils/
        __init__.py
        api.py
        data_processing.py
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">weather.py</code> is the main module that the user interacts with, which provides a single function to get the current weather information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_current_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Gets the current weather information for the given city.

    Args:
        city (str): The city for which to get the weather information.

    Returns:
        dict: The weather information for the given city.
    </span><span class="sh">"""</span>
    <span class="n">weather_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">api</span><span class="p">.</span><span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>
    <span class="n">processed_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">data_processing</span><span class="p">.</span><span class="nf">process_weather_data</span><span class="p">(</span><span class="n">weather_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <p>The utils package contains two modules, <code class="language-plaintext highlighter-rouge">api.py</code> and <code class="language-plaintext highlighter-rouge">data_processing.py</code>, which contain helper functions to retrieve the raw weather data from an API and to process the raw data into a more readable format, respectively.</p> <p>These modules can be reused across different projects, so it makes sense to organize them into a separate package.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># api.py
</span><span class="k">def</span> <span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Retrieves the raw weather data for the given city.

    Args:
        city (str): The city for which to retrieve the weather data.

    Returns:
        dict: The raw weather data for the given city.
    </span><span class="sh">"""</span>
    <span class="c1"># code to retrieve data from API
</span>    <span class="k">return</span> <span class="n">raw_data</span>

<span class="c1"># data_processing.py
</span><span class="k">def</span> <span class="nf">process_weather_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Processes the raw weather data into a more readable format.

    Args:
        raw_data (dict): The raw weather data.

    Returns:
        dict: The processed weather data.
    </span><span class="sh">"""</span>
    <span class="c1"># code to process data
</span>    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <h2 id="-test-your-code">🧪 Test Your Code</h2> <p>Testing your code helps you catch bugs and ensure that your code works as expected.</p> <p>Writing test cases is also an good way to document your code and help others understand it. Try all possible scenarios when testing your code, including edge cases and error conditions.</p> <p>Consider a module <code class="language-plaintext highlighter-rouge">calculator.py</code> that implements a simple calculator with basic arithmetic operations. We can write test cases for each operation using a testing framework such as <code class="language-plaintext highlighter-rouge">unittest</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">unittest</span>
<span class="kn">import</span> <span class="n">calculator</span>

<span class="k">class</span> <span class="nc">TestCalculator</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_addition</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_subtraction</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_multiplication</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_division</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">unittest</span><span class="p">.</span><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <p>Each test case tests a single operation in the calculator module and uses the <code class="language-plaintext highlighter-rouge">assertEqual</code> method to verify that the result of the operation is as expected.</p> <p>If any test fails, an error will be raised, and the test result will be reported as failed.</p> <p>For debugging we can use the <code class="language-plaintext highlighter-rouge">print</code> statement to print the intermediate results or the values of variables in the code, or use a debugger such as <code class="language-plaintext highlighter-rouge">pdb</code> to step through the code and inspect the values of variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">calculator</span>
<span class="kn">import</span> <span class="n">pdb</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span> <span class="c1"># Set a breakpoint
</span><span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-document-your-code">📜 Document Your Code</h2> <p>Documenting your code with docstrings can help others understand what it does and how it works.</p> <p>Docstrings should provide a high-level overview of the code, including its purpose, usage, and limitations.</p> <p>They should also be written in a clear and natural language style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Circle</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Class to represent a circle with a given radius.

    Attributes:
        radius (float): The radius of the circle.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the Circle class with a given radius.

        Args:
            radius (float): The radius of the circle.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">=</span> <span class="n">radius</span>

    <span class="k">def</span> <span class="nf">area</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the area of the circle.

        Returns:
            float: The area of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">circumference</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the circumference of the circle.

        Returns:
            float: The circumference of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">radius</span>
</code></pre></div></div> <p>The class has a docstring explaining its purpose and the attributes it has.</p> <p>Each method has its docstring explaining what it does and what arguments it takes and returns.</p> <p>This makes the code easier to understand and maintain and more accessible for others to use and build upon.</p> <h2 id="-handle-exceptions-gracefully">💥 Handle Exceptions Gracefully</h2> <p>Handling exceptions in your code is essential for ensuring that it continues to run even when unexpected events occur.</p> <p>Use <code class="language-plaintext highlighter-rouge">try</code> and <code class="language-plaintext highlighter-rouge">except</code> statements to handle exceptions and provide helpful error messages that explain what went wrong and how to fix it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="n">result</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="mi">0</span>
<span class="k">except</span> <span class="nb">ZeroDivisionError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a non-zero value for division</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The code inside the <code class="language-plaintext highlighter-rouge">try</code> block may raise a <code class="language-plaintext highlighter-rouge">ZeroDivisionError</code> exception.</p> <p>The <code class="language-plaintext highlighter-rouge">except</code> block handles the exception and prints a helpful error message to the user.</p> <p>This way, the program can continue running even when an unexpected error occurs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">file.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">FileNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a valid file path</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle any other exceptions
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An unexpected error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, the code inside the try block may raise a <code class="language-plaintext highlighter-rouge">FileNotFoundError</code> or any other exception.</p> <p>The first <code class="language-plaintext highlighter-rouge">except</code> block handles the FileNotFoundError and provides a helpful error message for the user.</p> <p>The second <code class="language-plaintext highlighter-rouge">except</code> block handles any other exceptions that may occur and provides a generic error message.</p> <p>This way the program can continue running even when unexpected errors occur and provide helpful error messages to the user.</p> <h2 id="-use-keyword-arguments">🔑 Use Keyword Arguments</h2> <p>Keyword arguments are a powerful feature of Python that allows you to specify default values for function arguments and make your code more readable and flexible.</p> <p>Using keyword arguments can also help you reduce the number of lines of code in your programs and make them easier to understand.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">!</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">John</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hello, John!
</span><span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">Jane</span><span class="sh">"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hi, Jane!
</span></code></pre></div></div> <p>In this example, the greet function takes in two arguments: name and message. The message argument has a default value of “Hello”.</p> <p>When we call <code class="language-plaintext highlighter-rouge">greet("John")</code>, the default value of <code class="language-plaintext highlighter-rouge">"Hello"</code> is used for the message argument. But when we call <code class="language-plaintext highlighter-rouge">greet("Jane", message="Hi")</code>, the keyword argument is used instead, and the output is <code class="language-plaintext highlighter-rouge">"Hi, Jane!"</code>.</p> <h2 id="️-follow-the-zen-of-python">🧘‍♀️ Follow the Zen of Python</h2> <p>The Zen of Python is a collection of principles and guidelines for writing good Python code.</p> <p>It includes tips on writing simple, clear, and maintainable code and advice on choosing between different solutions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">this</span>

<span class="k">def</span> <span class="nf">sort_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Simple is better than complex
</span>    <span class="n">data</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Readability counts
</span><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Explicit is better than implicit
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Flat is better than nested
</span><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">lists</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Use meaningful names
</span><span class="k">def</span> <span class="nf">calculate_average_score</span><span class="p">(</span><span class="n">scores</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">score</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># One obvious way to do it
</span>    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="n">count</span>
</code></pre></div></div> <p>We follow the Zen of Python by:</p> <ul> <li>Writing straightforward code (e.g. the <code class="language-plaintext highlighter-rouge">sort_data</code> function)</li> <li>Choosing meaningful names for variables and functions (e.g. <code class="language-plaintext highlighter-rouge">calculate_average_score</code>)</li> <li>Keeping the code flat and avoiding nested structures where possible (e.g. the flatten function)</li> <li>Being explicit and transparent in our code (e.g. using return statements)</li> </ul> <h2 id="-refactor-your-code-regularly">🛠 Refactor Your Code Regularly</h2> <p>Refactoring is improving the structure and quality of your code without changing its external behaviour.</p> <p>It can help you identify areas that need improvement and make your code more maintainable over time. This can be especially important in projects with a long lifespan or requiring continuous updates.</p> <p>So you can simplify complex sections, make your code more efficient, and eliminate any redundant or unnecessary parts. You can also take advantage of new features or libraries that have become available since you wrote the original code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">number</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">number</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Refactored code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span>
</code></pre></div></div> <p>We have refactored the <code class="language-plaintext highlighter-rouge">calculate_sum</code> function to use the built-in sum function instead of manually iterating over the numbers and adding them up. This code is more efficient and readable and takes advantage of a built-in feature of Python that can perform the same calculation.</p>]]></content><author><name></name></author><category term="old-posts"/><category term="python,"/><category term="coding"/><summary type="html"><![CDATA[Let's look at the basic rules for writing great Python code]]></summary></entry></feed>