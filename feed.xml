<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://xmarva.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xmarva.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-07T20:51:21+00:00</updated><id>https://xmarva.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding Byte-Pair Encoding Algorithm</title><link href="https://xmarva.github.io/blog/2025/tokenization/" rel="alternate" type="text/html" title="Understanding Byte-Pair Encoding Algorithm"/><published>2025-04-01T15:00:00+00:00</published><updated>2025-04-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/tokenization</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/tokenization/"><![CDATA[<h1 id="tokenization">Tokenization</h1> <p><a href="https://www.kaggle.com/code/qmarva/1-bpe-tokenization-algorithm-eng?scriptVersionId=231677033"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat-square&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"/></a> <a href="https://colab.research.google.com/drive/1lmfuMdC8v-lXL_MuyC0uBewdLLCTQzCO?usp=sharing"><img src="https://img.shields.io/badge/Colab-F9AB00?style=flat-square&amp;logo=google-colab&amp;logoColor=white" alt="Colab"/></a></p> <p>Tokenization is a fundamental stage in natural language processing, the task of which is to split text into meaningful units (tokens).</p> <p>These units can be words, parts of words, or even characters. Historically, simple methods were used: splitting by spaces, regular expressions for extracting words and punctuation, manual rules for handling abbreviations. However, such approaches scaled poorly for languages with agglutinative morphology (e.g., Russian or Finnish) and complex word combinations.</p> <p>Traditional tokenization methods like space splitting or manual rules often prove ineffective in real-world scenarios: they struggle with typos, rare words, and multilingual texts. For example, words like “gooood” or mixed languages in a single sentence can break a classical tokenizer.</p> <p>In modern NLP, subword tokenization algorithms like <a href="https://arxiv.org/pdf/1508.07909">BPE (Byte Pair Encoding)</a> dominate, balancing the semantic integrity of tokens with efficient vocabulary usage. In this notebook, we will examine the BPE algorithm in detail and learn to work with tokenizers from the Hugging Face library.</p> <p>First, we will import all libraries and functions needed for this notebook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>
</code></pre></div></div> <hr/> <h2 id="loading-data">Loading Data</h2> <p>For demonstration, we will load the parallel English-Russian <a href="https://arxiv.org/abs/1812.10464">Tatoeba</a> corpus from Artetxe et al. (2019) via the <a href="http://huggingface.co/docs/datasets/loading">Hugging Face Datasets</a> library.</p> <p><a href="https://tatoeba.org/en/sentences/index">Tatoeba</a> is a free collection of translated example sentences for language learners, available in over 400 languages. Its name comes from the Japanese phrase «tatoeba» (例えば), meaning “for example.” It is written and maintained by a community of volunteers through open collaboration. Individual contributors are known as Tatoebans.</p> <p>We will use only the English and Russian subsets. All examples in this dataset are short everyday phrases: “Let’s try something.” → “Давайте что-нибудь попробуем!”.</p> <p>This format is convenient for training transformers, which work with sequences of limited length. In this notebook, we will not delve into transformer architecture but focus on text data preprocessing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_translation_dataset</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading Tatoeba en-ru...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Helsinki-NLP/tatoeba</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="sh">"</span><span class="s">en</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="sh">"</span><span class="s">ru</span><span class="sh">"</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error while loading dataset: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Dataset structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Data sample:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EN: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">RU: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
    
    <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <hr/> <h2 id="data-analysis">Data Analysis</h2> <p>Let’s take a quick look at the dataset to understand what we’re dealing with. We won’t dive deep into data analysis methods but will examine basic statistics.</p> <p>The <code class="language-plaintext highlighter-rouge">analyze_dataset</code> function shows that the average length of English sentences is 7.2 words, Russian — 6.2. The maximum lengths (30 and 28 words) indicate the presence of outliers that may require truncation.</p> <p>The histograms show right-skewed distributions: most sentences are shorter than 15 words. These observations influence model hyperparameter choices, e.g., <code class="language-plaintext highlighter-rouge">max_length=64</code> provides padding headroom even if actual sequences are shorter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>

    <span class="n">en_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">ru_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Analysis based on first </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s"> samples:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">English sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">English Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Russian Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="simple-tokenizer">Simple Tokenizer</h2> <p>Now we will write a <code class="language-plaintext highlighter-rouge">BaseTokenizer</code> class for text preprocessing, building a token vocabulary, and collecting token frequency statistics. This class will serve as the foundation for more complex tokenizers and provide a common structure for processing text data.</p> <p>We declare the class using the <a href="https://docs.python.org/3/library/dataclasses.html">@dataclass</a> decorator to auto-generate the constructor. Parameters we need: <code class="language-plaintext highlighter-rouge">language</code> (text language), <code class="language-plaintext highlighter-rouge">vocab_size</code> (max vocabulary size), <code class="language-plaintext highlighter-rouge">min_freq</code> (minimum frequency for including a token in the vocabulary), and <code class="language-plaintext highlighter-rouge">special_tokens</code> (list of special tokens).</p> <p>If <code class="language-plaintext highlighter-rouge">special_tokens</code> are not specified, default values are used: <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called immediately after object initialization. Here, we initialize the <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">id2token</code> dictionaries that map tokens to their numeric IDs. Special tokens must be added to the vocabulary first. For example, <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> gets ID 0, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> — 1, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">preprocess_text</code> method is used to preprocess text. We will convert text to lowercase and split it into tokens using a regular expression.</p> <p>The pattern <code class="language-plaintext highlighter-rouge">r"\w+[\w']*|['’][a-z]+|[^\w\s]"</code> captures:</p> <ul> <li>Words with apostrophes (e.g., <code class="language-plaintext highlighter-rouge">don't</code> → <code class="language-plaintext highlighter-rouge">["don't"]</code>).</li> <li>Contractions starting with an apostrophe (e.g., <code class="language-plaintext highlighter-rouge">'s</code> → <code class="language-plaintext highlighter-rouge">["'s"]</code>).</li> <li>Individual punctuation marks (e.g., <code class="language-plaintext highlighter-rouge">"!"</code> → <code class="language-plaintext highlighter-rouge">["!"]</code>).</li> </ul> <p>Note that the regex may not cover all edge cases (e.g., emojis or compound symbols), requiring modification for specific tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">get_stats</code> method collects token frequency statistics. For each text in the <code class="language-plaintext highlighter-rouge">examples</code> list, the <code class="language-plaintext highlighter-rouge">preprocess_text</code> function is called, then the <code class="language-plaintext highlighter-rouge">Counter</code> is updated.</p> <p>For example, the text <code class="language-plaintext highlighter-rouge">"Hello, world!"</code> returns a counter with keys <code class="language-plaintext highlighter-rouge">["hello", ",", "world", "!"]</code> and their frequencies. This method is used during tokenizer training to select tokens for the vocabulary based on <code class="language-plaintext highlighter-rouge">min_freq</code> and <code class="language-plaintext highlighter-rouge">vocab_size</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <p>Below, we consolidate all code into a single class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
</code></pre></div></div> <p>In reality, this basic approach has many drawbacks. For example, converting text to lowercase may lose case information. Additionally, this tokenization ignores word morphology, leading to issues with rare words or homonyms.</p> <p>Let’s write an <code class="language-plaintext highlighter-rouge">analyze_token_statistics</code> function to count unique tokens and their frequencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_stats</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Token statistics for </span><span class="si">{</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total unique tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Top 10 most frequent tokens:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">en_tokenizer</span><span class="p">)</span>
<span class="n">ru_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ru_tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>The difference in token counts for English (1337) and Russian (2065) stems from language features: Russian has richer morphology (endings, prefixes) and more word forms. The dominance of punctuation (. and , in the top) suggests the need for their pre-filtering or separate handling.</p> <p>Interestingly, the <code class="language-plaintext highlighter-rouge">"</code> token appears more frequently in English (146 times) — likely due to translation specifics in Tatoeba.</p> <p>Critically, this approach does not split words into subword units, leaving rare words intact and inflating vocabulary size. For comparison, we will explore the BPE tokenizer in subsequent experiments.</p> <hr/> <h2 id="bpe-tokenization-algorithm">BPE Tokenization Algorithm</h2> <p>Now let’s examine how the <strong>BPE (Byte Pair Encoding)</strong> tokenizer works. The core idea is to iteratively merge the most frequent character or token pairs, gradually forming a subword vocabulary. This efficiently handles rare and complex words by splitting them into known components.</p> <h3 id="bpetokenizer-class">BPETokenizer Class</h3> <p>First, declare the class with the <code class="language-plaintext highlighter-rouge">@dataclass</code> decorator. Since it inherits from <code class="language-plaintext highlighter-rouge">BaseTokenizer</code>, it already includes parameters <code class="language-plaintext highlighter-rouge">language</code>, <code class="language-plaintext highlighter-rouge">vocab_size</code>, <code class="language-plaintext highlighter-rouge">min_freq</code>, and <code class="language-plaintext highlighter-rouge">special_tokens</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
</code></pre></div></div> <h3 id="initialization">Initialization</h3> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called after object creation. Here we:</p> <ol> <li>Call the parent <code class="language-plaintext highlighter-rouge">__post_init__</code> to initialize base structures like <code class="language-plaintext highlighter-rouge">token2id</code>.</li> <li>Add a <code class="language-plaintext highlighter-rouge">merges</code> dictionary to store character pairs and their merged versions (e.g., <code class="language-plaintext highlighter-rouge">('h', 'e')</code> → <code class="language-plaintext highlighter-rouge">'he'</code>).</li> <li>Initialize <code class="language-plaintext highlighter-rouge">vocab</code> with special tokens.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>
</code></pre></div></div> <h3 id="generating-character-pairs">Generating Character Pairs</h3> <p>The <code class="language-plaintext highlighter-rouge">get_pairs</code> method splits a word into consecutive character pairs. For example, the word <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code> returns pairs <code class="language-plaintext highlighter-rouge">[('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')]</code>. These pairs are analyzed during training to find the most frequent combinations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div> <hr/> <h2 id="training-the-tokenizer">Training the Tokenizer</h2> <p>The <code class="language-plaintext highlighter-rouge">train</code> method is the core of BPE. It has several stages:</p> <p><strong>Collect Initial Statistics:</strong></p> <ul> <li>Split each token into characters and count character sequence frequencies. For example, token <code class="language-plaintext highlighter-rouge">"hello"</code> becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>, and its frequency increments the counter for <code class="language-plaintext highlighter-rouge">'h e l l o'</code>.</li> <li>Collect all unique characters from the text.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Adding Characters to Vocabulary:</strong></p> <ul> <li>Each unique character (e.g., <code class="language-plaintext highlighter-rouge">'h'</code>, <code class="language-plaintext highlighter-rouge">'e'</code>) is added to <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">vocab</code> if not already present. This ensures even individual characters have IDs.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Main Merge Loop:</strong></p> <ul> <li>Each iteration counts the frequency of all possible character pairs in the current word representations. For example, the word <code class="language-plaintext highlighter-rouge">'h e l l o'</code> has pairs <code class="language-plaintext highlighter-rouge">('h', 'e')</code>, <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, etc.</li> <li>Select the most frequent pair (e.g., <code class="language-plaintext highlighter-rouge">('l', 'l')</code> for <code class="language-plaintext highlighter-rouge">hello</code>) and create a new token <code class="language-plaintext highlighter-rouge">'ll'</code>.</li> <li>Update <code class="language-plaintext highlighter-rouge">merges</code>, <code class="language-plaintext highlighter-rouge">vocab</code>, <code class="language-plaintext highlighter-rouge">token2id</code>, and <code class="language-plaintext highlighter-rouge">id2token</code>.</li> <li>Recalculate word frequencies by replacing the selected pair with the new token. For example, <code class="language-plaintext highlighter-rouge">'h e l l o'</code> becomes <code class="language-plaintext highlighter-rouge">'h e ll o'</code> after merging <code class="language-plaintext highlighter-rouge">('l', 'l')</code>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
    <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="c1"># Count pair frequencies
</span>    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
        <span class="k">break</span>  <span class="c1"># No pairs left → stop
</span>
    <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

    <span class="c1"># Update vocabulary
</span>    <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

    <span class="c1"># Recalculate frequencies with new token
</span>    <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
        <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>
</code></pre></div></div> <p>If <code class="language-plaintext highlighter-rouge">num_merges</code> is too high and pairs are exhausted early, training stops. Progress is printed every 1000 iterations to track vocabulary growth.</p> <hr/> <h3 id="text-tokenization">Text Tokenization</h3> <p>The <code class="language-plaintext highlighter-rouge">tokenize</code> method converts text to token IDs:</p> <ol> <li>Text is split into tokens via <code class="language-plaintext highlighter-rouge">preprocess_text</code>.</li> <li>The <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> special token is prepended.</li> <li>For each token (e.g., <code class="language-plaintext highlighter-rouge">"hello"</code>): <ul> <li>Characters are split into <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>.</li> <li>Merges from <code class="language-plaintext highlighter-rouge">merges</code> are applied iteratively. For example, if <code class="language-plaintext highlighter-rouge">('l', 'l')</code> is in <code class="language-plaintext highlighter-rouge">merges</code>, the character list becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'll', 'o']</code>, then remaining pairs are checked.</li> <li>Unknown characters (e.g., <code class="language-plaintext highlighter-rouge">'#'</code>) are replaced with <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</li> </ul> </li> <li>The <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is appended.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
            <span class="c1"># Find first available merge pair
</span>            <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                    <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Replace pair with new token
</span>            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

        <span class="c1"># Add final symbols to result
</span>        <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>During tokenization, merges are applied left-to-right, and the <strong>first</strong> available pair from <code class="language-plaintext highlighter-rouge">merges</code> is chosen. This can yield different results depending on the merge order. For example, if <code class="language-plaintext highlighter-rouge">merges</code> contains <code class="language-plaintext highlighter-rouge">('h', 'e')</code> and <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, the first encountered pair is merged.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
                <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>

        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training BPE tokenizer for </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
            <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

            <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
                <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>

            <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Merges completed: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_merges</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

            <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                        <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

            <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>When applying the tokenizer to convert text to tokens, the algorithm first splits text into base characters, then iteratively merges character pairs using the built merge dictionary. Each word in the text is represented as a sequence of subwords (or tokens) created during training.</p> <p>The number of merges (<code class="language-plaintext highlighter-rouge">num_merges</code> parameter) determines how many times the algorithm will merge characters into new tokens. More merges create larger, more informative tokens. However, excessive merges can lead to loss of fine-grained details.</p> <p>This algorithm performs well with large text corpora and helps models handle rare or unseen words by replacing them with subwords from more frequent character combinations. Additionally, BPE works with any language, even those with unusual or complex alphabets, as it starts from base characters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">80000</span>
<span class="n">train_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
<span class="n">en_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>
<span class="n">ru_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>

<span class="n">en_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">en_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">ru_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">ru_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">English vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Russian vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ru_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span>
<span class="n">ru_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">en_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">en_sample</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ru_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">ru_sample</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">)</span>
</code></pre></div></div> <p>Overall, BPE effectively addresses rare and complex words, improving tokenization quality and NLP model performance.</p> <p>However, even after training, artifacts remain. For example, “useless” splits into [“us”, “el”, “ess”], and “бесполезно” into [“бес”, “пол”, “ез”, “но”]. This stems from the limited number of merges and the lack of explicit morpheme boundary consideration in our educational implementation.</p> <p>In production tokenizers (e.g., Hugging Face’s), such issues are mitigated by pretraining on massive corpora and tens of thousands of merges.</p> <hr/> <h2 id="batch-preparation">Batch Preparation</h2> <p>The <code class="language-plaintext highlighter-rouge">prepare_batch</code> function converts tokenized sequences into tensors suitable for training. Each sentence is padded to a fixed length (<code class="language-plaintext highlighter-rouge">max_length=64</code>) with the <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token, and attention masks tell the model to ignore these “empty” positions.</p> <p>For example, a sentence with 24 tokens becomes a vector of length 64, where the last 40 elements are zeros (ID <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>). Masking is critical for transformers, as the attention mechanism would otherwise account for meaningless padding tokens, distorting weights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
                 <span class="n">src_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">tgt_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="n">src_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">src_texts</span><span class="p">]</span>
    <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">tgt_texts</span><span class="p">]</span>

    <span class="n">src_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">src_masks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_masks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>

        <span class="n">src_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_pad</span><span class="p">)</span>
        <span class="n">tgt_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_pad</span><span class="p">)</span>
        <span class="n">src_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_mask</span><span class="p">)</span>
        <span class="n">tgt_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_masks</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_masks</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="tokenizer-verification">Tokenizer verification</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Prepared batch shapes:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Example source tokens:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Corresponding mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">base_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Base tokenization: </span><span class="si">{</span><span class="n">base_tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Number of merges learned: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample merges (first 5):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">merged</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">merged</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample vocabulary items (first 10):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final tokenization:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoded tokens: </span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing English tokenizer:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr/> <h2 id="hugging-face-tokenizers">Hugging Face Tokenizers</h2> <p>All this seems quite complex. Our current tokenizer works imperfectly and is slow. Fortunately, programmers avoid reinventing the wheel. In practice, it’s much easier to use a ready-made tokenizer via <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> from the <code class="language-plaintext highlighter-rouge">transformers</code> library.</p> <p>The <code class="language-plaintext highlighter-rouge">opus-mt-en-ru</code> model already has a pretrained BPE vocabulary optimized for the language pair. The tokenizer automatically adds special tokens, handles case, and rare symbols. When processing the dataset, the <code class="language-plaintext highlighter-rouge">map</code> function applies tokenization in parallel to all examples, speeding up work via batching.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Helsinki-NLP/opus-mt-en-ru</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
<span class="p">):</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="n">source_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>
        <span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>

        <span class="n">source_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">source_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="n">target_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">target_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">decoder_attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span>
        <span class="n">preprocess_function</span><span class="p">,</span>
        <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">column_names</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="comparing-tokenizers">Comparing Tokenizers</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Custom BPE Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of prepared batches:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">array</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (dtype: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sample data from first batch:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Source tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Target tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Source mask (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hugging Face Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset features: </span><span class="si">{</span><span class="n">processed_dataset</span><span class="p">.</span><span class="n">features</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of examples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">first_example</span> <span class="o">=</span> <span class="n">processed_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">First example details:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input IDs shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded input:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Labels shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded labels:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention mask sample:</span><span class="sh">"</span><span class="p">,</span> <span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>


<span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="featured-posts"/><category term="tokenization,"/><category term="bpe,"/><category term="algorithms,"/><category term="nlp"/><summary type="html"><![CDATA[Implement one of the most popular tokenization algorithms and learn how to use ready-made solutions.]]></summary></entry><entry><title type="html">Can AI Achieve True Creativity?</title><link href="https://xmarva.github.io/blog/2025/creative-ai/" rel="alternate" type="text/html" title="Can AI Achieve True Creativity?"/><published>2025-02-13T15:00:00+00:00</published><updated>2025-02-13T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/creative-ai</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/creative-ai/"><![CDATA[<p>The criticism that AI cannot create something fundamentally new often overlooks a key fact: human creativity itself is built on combining existing knowledge, concepts, and imagery.</p> <p>Modern research reveals that when humans invent something novel, the brain doesn’t conjure elements “from nothing” but recombines fragments of prior experiences. Neural networks operate similarly.</p> <p>When GPT-4 generates text, it relies on statistical patterns learned from existing data rather than conscious intent.</p> <h3 id="the-brains-creative-networks-vs-gans">The Brain’s Creative Networks vs. GANs</h3> <p>In one experiment, participants were asked to devise unconventional uses for everyday objects (e.g., a coffee cup). fMRI scans showed two activated networks during creativity:</p> <ul> <li>The default mode network (associated with daydreaming and associations)</li> <li>The frontoparietal network (linked to focus and control)</li> </ul> <p>The dorsolateral prefrontal cortex also activates during musical improvisation. This interaction mirrors generative adversarial networks (GANs), where one component generates ideas and another evaluates their plausibility. Both biological and artificial systems balance freedom and constraints to produce novelty.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/0-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/0-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="fMRI activation during creative tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fMRI activation patterns during creative tasks (Source: Beaty et al., 2016) </div> <h3 id="the-limits-of-perception-and-data">The Limits of Perception and Data</h3> <p>Human perception constrains creativity. We can’t imagine colors beyond the visible spectrum—any new shade is a recombination of known hues. Similarly, Stable Diffusion can’t generate images of objects absent from its training data. It blends learned features, much like the brain combines memories.</p> <p>Blind individuals describe colors through analogies to sounds or textures (e.g., red as “loud noise,” blue as “smooth surface”). This suggests creativity is rooted in sensory experience—a dimension AI lacks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/1-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/1-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stable Diffusion-generated floral carpet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Floral carpet generated by Stable Diffusion </div> <h3 id="embodied-vs-abstract-learning">Embodied vs. Abstract Learning</h3> <p>A key difference lies in how humans and AI acquire knowledge:</p> <ul> <li>Humans learn through physical interaction (e.g., toddlers touching objects to link tactile and visual data)</li> <li>AI processes abstract tokens/pixels</li> </ul> <p>While LLMs develop semantic relationships (e.g., “apple” vectors near “fruit” and “tree”), they lack embodied experiences stored in the brain’s sensorimotor cortex. When we think “run,” motor neurons activate—a connection AI can’t replicate.</p> <h3 id="the-myth-of-intentional-creativity">The Myth of Intentional Creativity</h3> <p>Critics argue humans possess creative “intent,” but jazz improvisation studies show decreased prefrontal cortex activity during spontaneous creation. Ideas emerge automatically from learned patterns—a process strikingly similar to how neural networks operate.</p> <h3 id="the-originality-debate">The Originality Debate</h3> <p>If human innovation is recombination, demanding absolute novelty from AI is flawed. Both are constrained by their “training data”:</p> <ul> <li>Human brains: Biological experiences</li> <li>AI models: Digital datasets</li> </ul> <p>The distinction lies in complexity and emotional embodiment. While AI manipulates mathematical structures, the brain ties patterns to emotions and bodily states—for now.</p> <p><strong>References</strong><br/> [1] Beaty, R. E., Benedek, M., Silvia, P. J., &amp; Schacter, D. L. (2016). Creative Cognition and Brain Network Dynamics. <em>Trends in Cognitive Sciences</em><br/> [2] De Borst, A. W., &amp; de Gelder, B. (2018). Mental Imagery Follows Similar Cortical Reorganization as Perception: Intra-Modal and Cross-Modal Plasticity in Congenitally Blind. <em>Cerebral Cortex</em><br/> [3] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dea, J. (2013). Distributed Representations of Words and Phrases and their Compositionality<br/> [4] Limb, C. J., &amp; Braun, A. R. (2008). Neural Substrates of Spontaneous Musical Performance: An fMRI Study of Jazz Improvisation</p>]]></content><author><name></name></author><category term="featured-posts"/><category term="ai,"/><category term="neuroscience,"/><category term="creativity"/><summary type="html"><![CDATA[Exploring the parallels between human creativity and neural networks]]></summary></entry><entry><title type="html">Algebraic Foundations of Low-Rank Adaptation</title><link href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/" rel="alternate" type="text/html" title="Algebraic Foundations of Low-Rank Adaptation"/><published>2024-12-30T15:09:00+00:00</published><updated>2024-12-30T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/lora-algorithm-for-llms</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><![CDATA[<h2 id="the-paradox-of-scale">The Paradox of Scale</h2> <p>The evolution of language models presents us with an intriguing paradox: while increasing model size enhances general capability, it simultaneously complicates practical deployment through prohibitive computational demands. This tension between capacity and practicality forms the crucible where Low-Rank Adaptation (LoRA) emerges as an elegant solution. To understand its mechanisms, we must first establish fundamental mathematical constructs.</p> <h2 id="matrix-theory-foundations">Matrix Theory Foundations</h2> <h3 id="the-algebraic-scaffolding">The Algebraic Scaffolding</h3> <p>A matrix \(A \in \mathbb{R}^{m \times n}\) represents a linear transformation between vector spaces \(\mathbb{R}^n \to \mathbb{R}^m\). Each element \(a_{ij}\) encodes the transformation coefficient between basis vectors \(e_j\) and \(e_i\). In neural networks, these matrices become learned representations of feature interactions.</p> <p>The <strong>rank</strong> of a matrix, denoted \(\rho(A)\), measures its column space dimensionality through the maximal number of linearly independent columns. Formally:</p> \[\rho(A) = \dim(\text{col}(A)) = \dim(\text{row}(A))\] <p>This duality between row and column space dimensionalities (proven via the Fundamental Theorem of Linear Algebra) becomes crucial for understanding parameter efficiency.</p> <h3 id="rank-constrained-transformations">Rank-Constrained Transformations</h3> <p>Consider two matrices \(B \in \mathbb{R}^{m \times r}\) and \(A \in \mathbb{R}^{r \times n}\). Their product \(BA\) inherently satisfies:</p> \[\rho(BA) \leq \min(\rho(B), \rho(A)) \leq r\] <p>This rank upper bound enables dramatic parameter reduction when \(r \ll \min(m,n)\). For a neural layer with \(m \times n\) weights, replacing full updates with low-rank factors reduces trainable parameters from \(mn\) to \(r(m+n)\) – an efficiency gain of \(\frac{mn}{r(m+n)}\). For typical layers (\(m,n \sim 10^3\), \(r \sim 10^1\)), this yields ~100x parameter reduction.</p> <h2 id="the-low-rank-adaptation-hypothesis">The Low-Rank Adaptation Hypothesis</h2> <h3 id="intrinsic-dimensionality-of-task-adaptation">Intrinsic Dimensionality of Task Adaptation</h3> <p>Modern language models exhibit an intriguing property: while pretrained on broad corpora, task-specific adaptation appears to operate in low-dimensional subspaces. This phenomenon aligns with the <strong>manifold hypothesis</strong>, suggesting high-dimensional data actually resides on lower-dimensional manifolds.</p> <p>Let \(\Delta W \in \mathbb{R}^{m \times n}\) represent weight updates during fine-tuning. The LoRA conjecture posits:</p> \[\rho(\Delta W) \leq r \ll \min(m,n)\] <p>Experimental validation shows task adaptation often requires surprisingly low ranks (\(r=8\) achieves strong performance). This implies that while the original parameter space is vast, task-specific adjustments occupy a small subspace.</p> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Visualize the weight matrix as a point in \(\mathbb{R}^{mn}\). Full fine-tuning moves this point through the high-dimensional space. LoRA constrains movement to a low-dimensional <strong>adaptation manifold</strong> spanned by \(B\) and \(A\):</p> \[\mathcal{M}_r = \{ W + BA \mid B \in \mathbb{R}^{m \times r}, A \in \mathbb{R}^{r \times n} \}\] <p>The approximation error is bounded by the Eckart–Young theorem:</p> \[\min_{\rho(BA)\leq r} \| \Delta W - BA \|_F = \sum_{i=r+1}^{\min(m,n)} \sigma_i(\Delta W)\] <p>where \(\sigma_i\) denotes singular values. Rapidly decaying singular values in \(\Delta W\) (as observed empirically) enable accurate low-rank approximation.</p> <h2 id="algorithmic-implementation">Algorithmic Implementation</h2> <h3 id="parameterization-and-initialization">Parameterization and Initialization</h3> <p>For a pretrained weight matrix \(W_0\), LoRA constructs:</p> \[W = W_0 + \frac{\alpha}{r}BA\] <p>Where:</p> <ul> <li>\(B\) initialized with \(\mathcal{N}(0, \sigma^2)\)</li> <li>\(A\) initialized to zero</li> <li>\(\alpha\): learning rate scaling factor</li> </ul> <p>The initialization strategy ensures \(\Delta W = 0\) at training onset, preserving original model behavior. The \(\alpha/r\) scaling normalizes parameter updates across different ranks, maintaining stable learning dynamics.</p> <h3 id="gradient-dynamics">Gradient Dynamics</h3> <p>Let \(\mathcal{L}\) be the loss function. The gradient through the LoRA parameters becomes:</p> \[\nabla_B \mathcal{L} = \frac{\alpha}{r} (\nabla_{W} \mathcal{L}) A^T \\ \nabla_A \mathcal{L} = \frac{\alpha}{r} B^T (\nabla_{W} \mathcal{L})\] <p>This reveals an important property: gradient signals flow through both low-rank factors, with the scaling term modulating update magnitudes. The rank \(r\) therefore acts as a gradient multiplier – higher ranks enable stronger gradient signals but increase parameter count.</p> <h2 id="practical-considerations-and-variations">Practical Considerations and Variations</h2> <h3 id="rank-selection-tradeoffs">Rank Selection Tradeoffs</h3> <p>The choice of \(r\) balances expressivity vs efficiency:</p> <ul> <li><strong>Lower ranks (r=1-4):</strong> Maximize parameter efficiency, suitable for similar source/target tasks</li> <li><strong>Medium ranks (r=8-16):</strong> General-purpose setting for domain adaptation</li> <li><strong>Higher ranks (r=32+):</strong> Needed for complex task transfers or low-data scenarios</li> </ul> <p>Empirical studies show performance follows logarithmic scaling:</p> \[\text{Performance}(r) \approx \text{Performance}(\text{full}) - c/\log r\] <p>Where \(c\) is task-dependent. This suggests diminishing returns beyond certain ranks.</p> <h3 id="architectural-variants">Architectural Variants</h3> <ol> <li><strong>Bottleneck Adaptation:</strong> Stack multiple low-rank layers (\(W_0 + B_1A_1 + B_2A_2\)) for hierarchical adaptation</li> <li><strong>Sparse LoRA:</strong> Combine with magnitude pruning on \(BA\) product</li> <li><strong>Dynamic Rank Allocation:</strong> Use singular value thresholds to automatically select per-layer ranks</li> <li><strong>LoRA++:</strong> Introduce learned scaling factors per layer instead of fixed \(\alpha/r\)</li> </ol> <h3 id="compositional-adaptation">Compositional Adaptation</h3> <p>For multi-task learning, LoRA enables parameter composition:</p> \[W = W_0 + \sum_{k=1}^K B_kA_k\] <p>Where each \(B_kA_k\) captures task-specific adaptations. During inference, select subsets of adapters via:</p> \[W = W_0 + \sum_{k \in S} B_kA_k\] <p>This facilitates efficient multi-task serving with \(\mathcal{O}(Kr)\) storage instead of \(\mathcal{O}(K)\) full models.</p> <h2 id="theoretical-implications">Theoretical Implications</h2> <h3 id="implicit-regularization">Implicit Regularization</h3> <p>The low-rank constraint acts as a strong regularizer, preventing overfitting to small datasets. Consider the Rademacher complexity for a LoRA-adapted layer:</p> \[\mathcal{R}_n(\mathcal{H}_{\text{LoRA}}) \leq \frac{\alpha \sqrt{2r\log(2mn)}}{n}\] <p>Compared to full fine-tuning’s \(\mathcal{O}(\sqrt{mn/n})\) complexity, LoRA’s bound is significantly tighter, explaining its improved generalization in low-data regimes.</p> <h3 id="information-bottleneck-perspective">Information Bottleneck Perspective</h3> <p>Interpreting through the information bottleneck lens, LoRA enforces:</p> \[\min_{B,A} I(W; BA) \quad \text{s.t.} \quad I(BA; \mathcal{T}) \geq I_c\] <p>Where \(\mathcal{T}\) is the target task and \(I_c\) the required information. The low-rank structure naturally minimizes irrelevant information from \(W\) while preserving task-relevant features.</p> <h2 id="epilogue">Epilogue</h2> <p>LoRA epitomizes the principle that profound solutions often arise from deep mathematical insight rather than brute-force computation. By reconceptualizing adaptation as a low-rank update process, it achieves an elegant synthesis of efficiency and effectiveness – a reminder that in machine learning as in mathematics, constraints often breed creativity.</p> <p>The road ahead suggests intriguing possibilities: could other matrix properties (e.g., sparsity patterns, eigenvalue distributions) inspire new adaptation paradigms? As language models continue evolving, such algebraic perspectives will likely remain essential tools for harnessing their potential.</p>]]></content><author><name></name></author><category term="nlp,"/><category term="llm,"/><category term="lora"/><summary type="html"><![CDATA[Mathematical exploration of parameter-efficient fine-tuning through matrix rank theory]]></summary></entry><entry><title type="html">LLMs for Those Who Missed Out</title><link href="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/" rel="alternate" type="text/html" title="LLMs for Those Who Missed Out"/><published>2024-04-24T15:09:00+00:00</published><updated>2024-04-24T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/"><![CDATA[<p>An LLM (large language model) consists of just two files:</p> <ul> <li>Large file with billions of parameters (weights)</li> <li>Small file with about 500 lines of code to run the model</li> </ul> <p>LLM doesn’t take up much space and doesn’t require extensive computing power.</p> <p>For example, the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama-2–70b model</a> from Meta, which is open source and has 70 billion parameters, is just 140 GB and can be run locally on a MacBook without internet access. You can <a href="https://www.llama.com/llama-downloads/">download this model</a> from Meta’s website and use it for free.</p> <p>However, you need significant computing resources to get these parameters (train the model).</p> <h2 id="how-to-train-a-model">How to Train a Model</h2> <h3 id="stage-1-pretraining-training-the-base-model">Stage 1: Pretraining (Training the base model)</h3> <p>Think of this process as compressing the Internet into a neural network, similar to how a ZIP file compresses documents into an archive.</p> <p>Here’s what you need to get a model like Llama-2:</p> <ul> <li>Take a “slice of the internet,” approximately ten terabytes of text in size</li> <li>Use a cluster of ~6,000 GPUs (specialized graphics processors used for heavy computations)</li> <li>Compress the text into the neural network. This takes about 12 days and costs around $2M</li> <li>Acquire the base model (the file with parameters)</li> </ul> <p>The primary function of the base model is to predict the next word. You input a sequence of words, and it outputs the next most likely word based on the texts it was trained on.</p> <p>For example, for the phrase “cat sat on a,” the LLM might predict the word “mat” with 97% probability.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/0-480.webp 480w,/assets/img/posts/2024-04-24-llms/0-800.webp 800w,/assets/img/posts/2024-04-24-llms/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image from the presentation illustrating how the neural network works" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image from the presentation illustrating how the neural network works </div> <p>That’s basically how compression works: if you can predict the next word accurately, you can use this information to compress the original data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/1-480.webp 480w,/assets/img/posts/2024-04-24-llms/1-800.webp 800w,/assets/img/posts/2024-04-24-llms/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="I checked, and ChatGPT indeed responds with “mat”" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> I checked, and ChatGPT indeed responds with “mat” </div> <p>However, unlike a ZIP file, where compression is lossless, LLM “compresses” the internet with losses. This is why models can hallucinate: they make up information that doesn’t actually exist but appears plausible (similar to what they have seen in the training data).</p> <p>Moreover, models can sometimes produce completely unexpected things. For example, ChatGPT knows the answer to the question, “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer), but it doesn’t know the answer to the question, “Who is Mary Lee Pfeiffer’s son?”</p> <p>This isn’t a typical database that simply stores information; it’s a different format we don’t fully understand.</p> <p>This clearly illustrates that we don’t quite grasp how this whole thing works and can only see the results it produces.</p> <h3 id="stage-2-finetuning-additional-training">Stage 2: Finetuning (Additional training)</h3> <p>The base model isn’t very practical for everyday use. We don’t just want to receive continuous word sequences; we want to ask questions and get answers.</p> <p>This requires finetuning — a process in which we develop an assistant model that answers questions.</p> <p>The training process for the assistant model is similar to that of the base model, but now we train the model not on internet texts but on data we manually collect. Companies hire people who write questions and answer them.</p> <p>If the training of the base model occurs on huge volumes of often low-quality text, the training of the assistant model involves comparatively smaller volumes (say, 100,000 documents), but they are all of very high quality.</p> <p>After such finetuning, the model understands that when asked a question, it should respond in the style of a helpful assistant.</p> <p>Unlike the training of the base model, which is conducted at most 2–3 times a year, finetuning can be done regularly, even weekly, as it is much less costly.</p> <h3 id="stage-3-optional-comparisons">Stage 3 (optional): Comparisons</h3> <p>In many cases it’s easier not to write a response from scratch but to compare several different answer options and choose the best one.</p> <p>The data from these comparisons is used to train the model further.</p> <p>At OpenAI, this is called RLHF (Reinforcement Learning from Human Feedback).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/2-480.webp 480w,/assets/img/posts/2024-04-24-llms/2-800.webp 800w,/assets/img/posts/2024-04-24-llms/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Difference between Open and Closed Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Difference between Open and Closed Models. Source: Chatbot leaderboards </div> <p>Models with open weights, such as Llama-2, are still inferior compared to proprietary models like GPT-4 and Claude.</p> <p>However, proprietary models cannot be downloaded for personal use or further training; they can only be operated through a web interface or an API (though the introduction of Custom GPTs at OpenAI is a first step towards customization)</p> <h2 id="capabilities-of-llms-using-chatgpt-as-an-example">Capabilities of LLMs (using ChatGPT as an example)</h2> <p>Prompted by an LLM, it can understand what the user needs and use external tools for this:</p> <ul> <li>Search the internet if the user needs up-to-date information (goes to Bing, retrieves a list of the latest links for the query, copies the full text of each link into its context window, and summarizes the information)</li> <li>Use a calculator to do calculations</li> <li>Use a Python library to draw graphs</li> <li>Draw images using DALL-E</li> <li>Write code</li> </ul> <p>Moreover, LLMs can be multimodal. They can recognize text, voice, images, or video and respond with voice, images, or video.</p> <h2 id="can-llms-make-decisions">Can LLMs Make Decisions?</h2> <p>There’s a well-known book by Kahneman, “Thinking, Fast and Slow.”</p> <p>The main idea of the book is that there are two systems in the brain: System 1, which has fast, automatic reactions, and System 2, which is slower, rational, and conscious and makes complex decisions.</p> <p>For example, the answer to question 2+2 comes from System 1 because this knowledge is automatic for us. But calculating 17×24 requires System 2.</p> <p>If we apply these terms, current LLMs only possess System 1. They can only provide the most likely next word in real time.</p> <p>It would be great if we could come to an LLM and say: here’s my question; you can think for 30 minutes, but I need a very accurate and high-quality answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/3-480.webp 480w,/assets/img/posts/2024-04-24-llms/3-800.webp 800w,/assets/img/posts/2024-04-24-llms/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Thought tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Thought tree </div> <p>No models can do that yet. But it would be desirable for a model to have something like a “thought tree,” through which it could navigate, analyze the results, go back and try again until it achieves a result it is most confident about.</p> <h2 id="can-llms-train-themselves">Can LLMs Train Themselves?</h2> <p>There’s a famous case where AlphaGo (a program that plays Go) was trained in two stages:</p> <p>First, it was trained on human games and learned to play very well. Then it began to train itself-playing against itself, trying to maximize the likelihood of winning-and significantly improved its quality.</p> <p>But in LLMs, we’re only at stage 1-training only occurs on materials created by humans.</p> <p>Why is this?</p> <p>In the game of Go, there is a very clear criterion for success-a won game- and you can train the model to maximize the likelihood of winning.</p> <p>In LLMs, the criteria are not so obvious. It’s not clear how to assess whether the result is good.</p> <p>Such criteria can be found in some narrow topics, but in general, it’s still hard to imagine.</p> <h2 id="what-llms-will-be-able-to-do-in-a-few-years">What LLMs Will Be Able to Do in a Few Years</h2> <p>Finally, a small forecast.</p> <p>In a few years, LLMs:</p> <ul> <li>Will have more knowledge than any human on all subjects</li> <li>Can operate on the internet</li> <li>Can use existing software infrastructure (calculator, Python, mouse/keyboard)</li> <li>Can see and generate images and videos</li> <li>Can hear and speak, and also generate music</li> <li>Can think for a long period using System 2</li> <li>Can train themselves in areas where there are clear criteria</li> <li>Can be customized and refined for specific tasks. Many versions will exist in app stores</li> <li>Can interact with other LLMs</li> </ul>]]></content><author><name></name></author><category term="old-posts"/><category term="nlp,"/><category term="llm"/><summary type="html"><![CDATA[Let's talk about large language models. Once again.]]></summary></entry><entry><title type="html">How to Write Good Python Code</title><link href="https://xmarva.github.io/blog/2023/python-code/" rel="alternate" type="text/html" title="How to Write Good Python Code"/><published>2023-02-07T15:09:00+00:00</published><updated>2023-02-07T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2023/python-code</id><content type="html" xml:base="https://xmarva.github.io/blog/2023/python-code/"><![CDATA[<p>Python is a fantastic programming language!</p> <p>It can be used for many things, like building websites, exploring data, and teaching machines to learn.</p> <p>If you already know Python or are just beginning, writing code that is strong, easy to read, and easy to keep up with is important.</p> <p>In this bogpost, we’ll look at the basic rules for writing great Python code and share some tips to help you make your programs even better.</p> <h2 id="-use-meaningful-naming-conventions">📚 Use Meaningful Naming Conventions</h2> <p>One of the most important aspects of good Python code is meaningful naming conventions.</p> <p>Choosing descriptive and concise names for variables, functions, and classes can help make your code more readable and understandable.</p> <p>Using proper naming conventions can also help you avoid naming conflicts, reduce the risk of errors, and simplify maintenance.</p> <p>For example, these are bad variable names:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>And these are better ones:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_number</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">second_number</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sum_of_numbers</span> <span class="o">=</span> <span class="n">first_number</span> <span class="o">+</span> <span class="n">second_number</span>
<span class="n">double_sum</span> <span class="o">=</span> <span class="n">sum_of_numbers</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>This includes using proper indentation, white space, line breaks, and following a code style guide like the PEP 8 style guide.</p> <p>Clear, organized code makes it easier to understand and modify and reduces the risk of errors.</p> <p>Here’s an example of lousy code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sum: </span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Difference: </span><span class="sh">"</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Product: </span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Quotient: </span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
</code></pre></div></div> <p>And here’s an example of good code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Sum</span><span class="sh">"</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Difference</span><span class="sh">"</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Product</span><span class="sh">"</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Quotient</span><span class="sh">"</span><span class="p">:</span> <span class="n">q</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h2 id="-write-comments">💬 Write Comments</h2> <p>Adding comments to your code is a great way to explain what it does and provide context for other developers.</p> <p>Comments should be used to explain complex code, provide additional information about the purpose of the code, and describe your thought process.</p> <p>Writing comments can also help you better understand your code when you return to it later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># function to calculate sum
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># this function calculates sum of two numbers
</span><span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments are not very descriptive or helpful in understanding the purpose of the functions.</p> <p>The first comment is trivial and adds no additional information. The second comment repeats what the function name already tells us.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the sum of two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns their sum.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the sum of `a` and `b`
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the difference between two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns the difference of `a` and `b`.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the difference between `a` and `b`
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments provide a clear and concise explanation of the purpose and behaviour of each function.</p> <p>The use of docstrings makes it easy to understand what the functions do and what arguments they take in. This makes the code more readable and maintainable.</p> <h2 id="-use-modules-and-packages">🧰 Use Modules and Packages</h2> <p>Modules and packages are a great way to organize your code into reusable blocks.</p> <p>They allow you to group related code together and make it easier to manage, understand, and maintain.</p> <p>The Python Standard Library is an good resource for finding pre-existing modules and packages. You can import it into your programs to save time and effort.</p> <p>Consider a project to build a simple weather application that provides a given city’s current temperature and conditions. We can structure the project as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weather_app/
    __init__.py
    weather.py
    utils/
        __init__.py
        api.py
        data_processing.py
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">weather.py</code> is the main module that the user interacts with, which provides a single function to get the current weather information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_current_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Gets the current weather information for the given city.

    Args:
        city (str): The city for which to get the weather information.

    Returns:
        dict: The weather information for the given city.
    </span><span class="sh">"""</span>
    <span class="n">weather_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">api</span><span class="p">.</span><span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>
    <span class="n">processed_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">data_processing</span><span class="p">.</span><span class="nf">process_weather_data</span><span class="p">(</span><span class="n">weather_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <p>The utils package contains two modules, <code class="language-plaintext highlighter-rouge">api.py</code> and <code class="language-plaintext highlighter-rouge">data_processing.py</code>, which contain helper functions to retrieve the raw weather data from an API and to process the raw data into a more readable format, respectively.</p> <p>These modules can be reused across different projects, so it makes sense to organize them into a separate package.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># api.py
</span><span class="k">def</span> <span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Retrieves the raw weather data for the given city.

    Args:
        city (str): The city for which to retrieve the weather data.

    Returns:
        dict: The raw weather data for the given city.
    </span><span class="sh">"""</span>
    <span class="c1"># code to retrieve data from API
</span>    <span class="k">return</span> <span class="n">raw_data</span>

<span class="c1"># data_processing.py
</span><span class="k">def</span> <span class="nf">process_weather_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Processes the raw weather data into a more readable format.

    Args:
        raw_data (dict): The raw weather data.

    Returns:
        dict: The processed weather data.
    </span><span class="sh">"""</span>
    <span class="c1"># code to process data
</span>    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <h2 id="-test-your-code">🧪 Test Your Code</h2> <p>Testing your code helps you catch bugs and ensure that your code works as expected.</p> <p>Writing test cases is also an good way to document your code and help others understand it. Try all possible scenarios when testing your code, including edge cases and error conditions.</p> <p>Consider a module <code class="language-plaintext highlighter-rouge">calculator.py</code> that implements a simple calculator with basic arithmetic operations. We can write test cases for each operation using a testing framework such as <code class="language-plaintext highlighter-rouge">unittest</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">unittest</span>
<span class="kn">import</span> <span class="n">calculator</span>

<span class="k">class</span> <span class="nc">TestCalculator</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_addition</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_subtraction</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_multiplication</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_division</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">unittest</span><span class="p">.</span><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <p>Each test case tests a single operation in the calculator module and uses the <code class="language-plaintext highlighter-rouge">assertEqual</code> method to verify that the result of the operation is as expected.</p> <p>If any test fails, an error will be raised, and the test result will be reported as failed.</p> <p>For debugging we can use the <code class="language-plaintext highlighter-rouge">print</code> statement to print the intermediate results or the values of variables in the code, or use a debugger such as <code class="language-plaintext highlighter-rouge">pdb</code> to step through the code and inspect the values of variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">calculator</span>
<span class="kn">import</span> <span class="n">pdb</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span> <span class="c1"># Set a breakpoint
</span><span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-document-your-code">📜 Document Your Code</h2> <p>Documenting your code with docstrings can help others understand what it does and how it works.</p> <p>Docstrings should provide a high-level overview of the code, including its purpose, usage, and limitations.</p> <p>They should also be written in a clear and natural language style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Circle</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Class to represent a circle with a given radius.

    Attributes:
        radius (float): The radius of the circle.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the Circle class with a given radius.

        Args:
            radius (float): The radius of the circle.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">=</span> <span class="n">radius</span>

    <span class="k">def</span> <span class="nf">area</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the area of the circle.

        Returns:
            float: The area of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">circumference</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the circumference of the circle.

        Returns:
            float: The circumference of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">radius</span>
</code></pre></div></div> <p>The class has a docstring explaining its purpose and the attributes it has.</p> <p>Each method has its docstring explaining what it does and what arguments it takes and returns.</p> <p>This makes the code easier to understand and maintain and more accessible for others to use and build upon.</p> <h2 id="-handle-exceptions-gracefully">💥 Handle Exceptions Gracefully</h2> <p>Handling exceptions in your code is essential for ensuring that it continues to run even when unexpected events occur.</p> <p>Use <code class="language-plaintext highlighter-rouge">try</code> and <code class="language-plaintext highlighter-rouge">except</code> statements to handle exceptions and provide helpful error messages that explain what went wrong and how to fix it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="n">result</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="mi">0</span>
<span class="k">except</span> <span class="nb">ZeroDivisionError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a non-zero value for division</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The code inside the <code class="language-plaintext highlighter-rouge">try</code> block may raise a <code class="language-plaintext highlighter-rouge">ZeroDivisionError</code> exception.</p> <p>The <code class="language-plaintext highlighter-rouge">except</code> block handles the exception and prints a helpful error message to the user.</p> <p>This way, the program can continue running even when an unexpected error occurs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">file.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">FileNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a valid file path</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle any other exceptions
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An unexpected error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, the code inside the try block may raise a <code class="language-plaintext highlighter-rouge">FileNotFoundError</code> or any other exception.</p> <p>The first <code class="language-plaintext highlighter-rouge">except</code> block handles the FileNotFoundError and provides a helpful error message for the user.</p> <p>The second <code class="language-plaintext highlighter-rouge">except</code> block handles any other exceptions that may occur and provides a generic error message.</p> <p>This way the program can continue running even when unexpected errors occur and provide helpful error messages to the user.</p> <h2 id="-use-keyword-arguments">🔑 Use Keyword Arguments</h2> <p>Keyword arguments are a powerful feature of Python that allows you to specify default values for function arguments and make your code more readable and flexible.</p> <p>Using keyword arguments can also help you reduce the number of lines of code in your programs and make them easier to understand.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">!</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">John</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hello, John!
</span><span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">Jane</span><span class="sh">"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hi, Jane!
</span></code></pre></div></div> <p>In this example, the greet function takes in two arguments: name and message. The message argument has a default value of “Hello”.</p> <p>When we call <code class="language-plaintext highlighter-rouge">greet("John")</code>, the default value of <code class="language-plaintext highlighter-rouge">"Hello"</code> is used for the message argument. But when we call <code class="language-plaintext highlighter-rouge">greet("Jane", message="Hi")</code>, the keyword argument is used instead, and the output is <code class="language-plaintext highlighter-rouge">"Hi, Jane!"</code>.</p> <h2 id="️-follow-the-zen-of-python">🧘‍♀️ Follow the Zen of Python</h2> <p>The Zen of Python is a collection of principles and guidelines for writing good Python code.</p> <p>It includes tips on writing simple, clear, and maintainable code and advice on choosing between different solutions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">this</span>

<span class="k">def</span> <span class="nf">sort_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Simple is better than complex
</span>    <span class="n">data</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Readability counts
</span><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Explicit is better than implicit
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Flat is better than nested
</span><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">lists</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Use meaningful names
</span><span class="k">def</span> <span class="nf">calculate_average_score</span><span class="p">(</span><span class="n">scores</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">score</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># One obvious way to do it
</span>    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="n">count</span>
</code></pre></div></div> <p>We follow the Zen of Python by:</p> <ul> <li>Writing straightforward code (e.g. the <code class="language-plaintext highlighter-rouge">sort_data</code> function)</li> <li>Choosing meaningful names for variables and functions (e.g. <code class="language-plaintext highlighter-rouge">calculate_average_score</code>)</li> <li>Keeping the code flat and avoiding nested structures where possible (e.g. the flatten function)</li> <li>Being explicit and transparent in our code (e.g. using return statements)</li> </ul> <h2 id="-refactor-your-code-regularly">🛠 Refactor Your Code Regularly</h2> <p>Refactoring is improving the structure and quality of your code without changing its external behaviour.</p> <p>It can help you identify areas that need improvement and make your code more maintainable over time. This can be especially important in projects with a long lifespan or requiring continuous updates.</p> <p>So you can simplify complex sections, make your code more efficient, and eliminate any redundant or unnecessary parts. You can also take advantage of new features or libraries that have become available since you wrote the original code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">number</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">number</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Refactored code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span>
</code></pre></div></div> <p>We have refactored the <code class="language-plaintext highlighter-rouge">calculate_sum</code> function to use the built-in sum function instead of manually iterating over the numbers and adding them up. This code is more efficient and readable and takes advantage of a built-in feature of Python that can perform the same calculation.</p>]]></content><author><name></name></author><category term="old-posts"/><category term="python,"/><category term="coding"/><summary type="html"><![CDATA[Let's look at the basic rules for writing great Python code]]></summary></entry></feed>