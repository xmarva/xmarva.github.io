<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://xmarva.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xmarva.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-29T02:32:03+00:00</updated><id>https://xmarva.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for exploring the depths of natural and artificial intelligence. </subtitle><entry><title type="html">Somatic Marker Hypothesis by Antonio Damasio</title><link href="https://xmarva.github.io/blog/2025/antonio-damasio/" rel="alternate" type="text/html" title="Somatic Marker Hypothesis by Antonio Damasio"/><published>2025-07-15T10:00:00+00:00</published><updated>2025-07-15T10:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/antonio-damasio</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/antonio-damasio/"><![CDATA[<p><em>Why your gut feelings aren’t just superstition — they’re sophisticated neural machinery that’s been running your life all along</em></p> <p>Imagine this: You’re sitting across from a charming stranger on a first date. The conversation flows effortlessly, they’re attractive, successful, share your interests. On paper, they’re perfect. But something in your chest constricts with each laugh, each seemingly innocent comment. Your rational mind chides you for being paranoid, but your body whispers warnings you can’t quite articulate.</p> <p>Three months later, you discover they’ve been lying about everything that matters.</p> <p>How did you know? How could a collection of cells and neurons possibly detect deception more accurately than your sophisticated conscious analysis? The answer lies in one of the most revolutionary discoveries in neuroscience — and it turns everything we thought we knew about human intelligence upside down.</p> <h2 id="the-uncomfortable-truth-about-your-brain">The Uncomfortable Truth About Your Brain</h2> <p>For centuries, we’ve told ourselves a comforting story about human cognition: that we’re rational beings who occasionally get hijacked by primitive emotions. That the highest form of intelligence is cold, calculating logic. That the path to better decisions lies in suppressing our feelings and embracing pure reason.</p> <p>This story isn’t just wrong — it’s dangerous.</p> <p>Antonio Damasio, a Portuguese-American neuroscientist, spent decades studying patients with brain damage, and what he discovered should fundamentally change how you think about every decision you make. His findings, published in the groundbreaking 1994 book “Descartes’ Error,” revealed that emotion and reason aren’t competing systems. They’re so thoroughly integrated that damage to one cripples the other entirely.</p> <p>The implications are staggering: Your body has been making decisions for you your entire life, often with breathtaking accuracy, and you’ve been trained to ignore it.</p> <h2 id="the-lawyer-who-lost-his-soul">The Lawyer Who Lost His Soul</h2> <p>Meet Elliot — though that’s not his real name. Before his brain tumor, he was everything our society celebrates: a successful attorney, devoted father, pillar of his community. His IQ was superior, his memory photographic, his logical reasoning flawless. Then surgeons removed a tumor from his ventromedial prefrontal cortex, and everything changed.</p> <p>Not his intelligence — that remained pristine. Not his memory — every fact perfectly preserved. What vanished was something far more fundamental: his ability to feel the weight of his choices.</p> <p>Elliot could analyze the pros and cons of restaurant options for hours, constructing elaborate decision trees worthy of Harvard Business School. He could explain with clinical precision why his wife leaving him was problematic, but felt no grief. He could understand that losing his life savings was financially catastrophic, but experienced no regret, no anxiety, no motivation to prevent it from happening again.</p> <p>He became a human computer — brilliant at processing information, utterly hopeless at living life.</p> <p>The cruel irony? Elliot could pass every test of rationality while making one disastrous decision after another. He cycled through jobs, relationships, investments, each failure analyzed with perfect logic but no emotional learning. Watching him was like observing a chess grandmaster who had forgotten why winning mattered.</p> <h2 id="the-railroad-worker-who-rewrote-psychology">The Railroad Worker Who Rewrote Psychology</h2> <p>But Elliot wasn’t unique. In 1848, a railroad foreman named Phineas Gage was preparing a blasting charge when an iron rod three feet long and over an inch thick shot through his skull like a javelin. Miraculously, he survived. His intelligence remained intact, his memory perfect, his language fluent.</p> <p>Yet everyone who knew him said the same thing: “This is no longer Phineas Gage.”</p> <p>Before the accident, Gage was described as responsible, social, and well-liked — the kind of man you’d trust with your life. After? He became impulsive, irreverent, and socially inappropriate. He couldn’t hold a job, couldn’t maintain relationships, couldn’t make the kind of basic life decisions that any functioning adult manages automatically.</p> <p>What did Elliot and Gage share? Both had damage to the same brain region — the ventromedial prefrontal cortex. This wasn’t coincidence. It was the key to understanding something profound about human intelligence: we don’t just think with our brains. We think with our entire bodies.</p> <h2 id="the-card-game-that-changed-everything">The Card Game That Changed Everything</h2> <p>To test his revolutionary theory, Damasio created a deceptively simple experiment: the Iowa Gambling Task. Participants sit before four decks of cards, each containing different patterns of rewards and punishments. Two decks offer large immediate payoffs but devastating long-term losses — like a predatory loan with attractive terms. Two offer modest rewards but sustainable long-term gains — like a sensible investment strategy.</p> <p>The participants have no idea about these patterns. They’re told only to maximize their winnings.</p> <p>Here’s what happens with healthy participants, and it’s genuinely eerie:</p> <p><strong>Cards 1-10:</strong> Random exploration. They sample all decks, trying to figure out the rules.</p> <p><strong>Cards 10-50:</strong> Their skin conductance starts spiking when they reach for the “bad” decks. Their bodies are detecting danger before their minds understand what’s happening. They can’t explain why, but they start avoiding certain decks.</p> <p><strong>Cards 50-80:</strong> They develop strong preferences for the “good” decks, still unable to articulate the underlying patterns.</p> <p><strong>Cards 80+:</strong> Finally, conscious understanding emerges. They can explain the deck patterns and their strategy.</p> <p>The body knows. The mind follows. By several dozen cards.</p> <p>But here’s where it gets genuinely disturbing: patients with damage to the ventromedial prefrontal cortex, like Elliot, never develop those early warning signals. Their skin conductance stays flat. They keep choosing the devastating decks long after healthy participants have learned to avoid them.</p> <p>Even more chilling: they can intellectually understand the patterns when explained, but they can’t feel why they matter. It’s like explaining to a colorblind person that the red wire is dangerous — they can memorize the rule, but they can’t perceive the difference that makes it meaningful.</p> <h2 id="the-neurobiology-of-intuition">The Neurobiology of Intuition</h2> <p>So how does this actually work? How can a collection of cells in your gut possibly know things your conscious mind hasn’t figured out yet?</p> <p>The answer lies in what Damasio calls “somatic markers” — from the Greek word sōma, meaning body. These are subtle physiological signals that your nervous system generates based on past experiences. Every time you encounter a situation, your body runs a lightning-fast simulation: “Last time something like this happened, how did it feel?”</p> <p>The amygdala, often called the brain’s “fear center,” processes these patterns at speeds that make conscious thought look glacial. It’s constantly scanning for threats, opportunities, and familiar patterns, creating emotional associations that bypass your rational mind entirely.</p> <p>The vagus nerve — a massive information highway connecting your brain to your heart, lungs, and digestive system — transmits these signals throughout your body. When your stomach “drops” during a stressful presentation, when your heart races before a job interview, when you feel “butterflies” about a big decision — that’s your vagus nerve at work, transmitting crucial information about your internal state.</p> <p>The ventromedial prefrontal cortex then integrates these bodily signals with abstract reasoning, social context, and long-term planning. It’s where your gut feelings get translated into actionable insights.</p> <p>Damage any part of this system, and you lose access to decades of embodied wisdom. You become, in essence, a tourist in your own life — able to observe and analyze, but unable to feel the weight of consequences.</p> <h2 id="the-counterattack-when-science-gets-uncomfortable">The Counterattack: When Science Gets Uncomfortable</h2> <p>Damasio’s findings were so radical that they triggered a scientific backlash. Critics raised legitimate concerns:</p> <p><strong>The Consciousness Problem:</strong> Researchers like Maia and McClelland argued that participants in the Iowa Gambling Task actually had more conscious awareness than Damasio claimed. Maybe those “unconscious” somatic markers weren’t so unconscious after all.</p> <p><strong>The Specificity Problem:</strong> When exactly do somatic markers influence decisions? For all choices? Only complex ones? Only emotional ones? The theory seemed frustratingly vague about its own boundaries.</p> <p><strong>The Replication Problem:</strong> Multiple research groups found mixed results when trying to replicate the Iowa Gambling Task findings. Some studies showed that participants could use pure logic to crack the task, no somatic markers required.</p> <p>These criticisms revealed something important: the relationship between emotion and reason is more complex than any simple theory can capture. But they also missed something crucial about what Damasio was actually claiming.</p> <h2 id="the-integration-revolution">The Integration Revolution</h2> <p>Recent neuroscience suggests that the emotion-versus-reason debate has been asking the wrong question entirely. We’ve been treating them as separate systems that sometimes cooperate, like oil and water that occasionally mix. But that’s not how the brain actually works.</p> <p>Modern brain imaging reveals that emotional and rational processing aren’t competing systems — they’re deeply integrated networks that function as a unified whole. Your prefrontal cortex doesn’t just override your emotions; it incorporates them into a sophisticated decision-making process that includes:</p> <ul> <li>Pattern recognition from past experiences (amygdala and hippocampus)</li> <li>Physiological stress responses (vagus nerve and autonomic nervous system)</li> <li>Abstract reasoning about future consequences (prefrontal cortex)</li> <li>Social context about relationships and obligations (various cortical regions)</li> <li>Embodied simulation of potential outcomes (somatic markers throughout the body)</li> </ul> <p>The magic isn’t in choosing between emotion and reason — it’s in their seamless integration. When this integration breaks down, as it did with Elliot and Gage, you don’t become more rational. You become less human.</p> <h2 id="the-practical-implications-how-to-actually-use-this-knowledge">The Practical Implications: How to Actually Use This Knowledge</h2> <p>Understanding somatic markers isn’t just academic curiosity. It has profound implications for how you navigate everything from career choices to relationships to financial decisions. Here’s how to actually apply this knowledge:</p> <h3 id="trust-your-gut-but-verify">Trust Your Gut (But Verify)</h3> <p>Your initial emotional reaction to a situation often contains valuable information that your conscious mind hasn’t processed yet. That uneasy feeling about a business partner, that inexplicable excitement about a new opportunity — these aren’t random neural noise.</p> <p><strong>Practical technique:</strong> When facing a decision, pay attention to your body’s immediate response before your rational mind engages. Notice physical sensations: tension in your shoulders, flutter in your chest, knot in your stomach. These are data points, not just feelings.</p> <p>But emotions can also mislead. The same physiological arousal that signals danger can also signal excitement. Context matters enormously.</p> <p><strong>The verification step:</strong> After noting your somatic response, consciously analyze why you might be feeling this way. Are you responding to genuine patterns, or are you triggered by irrelevant factors like the person’s appearance or accent?</p> <h3 id="the-paradox-of-choice">The Paradox of Choice</h3> <p>When faced with complex decisions — especially those involving uncertain outcomes and emotional stakes — your somatic markers can help cut through analysis paralysis. They provide a rapid, unconscious filtering system that narrows your options to the most viable alternatives.</p> <p><strong>Practical technique:</strong> When overwhelmed by options, create a shortlist based on your immediate gut reactions. Then apply rational analysis only to your top choices. This leverages your body’s pattern recognition while preserving cognitive resources for detailed evaluation.</p> <h3 id="learning-from-experience">Learning from Experience</h3> <p>Every decision you make creates new somatic markers. The anxiety you felt after that impulsive purchase, the satisfaction from that well-researched investment, the regret from that hasty relationship decision — your body is literally learning from these experiences and encoding them for future reference.</p> <p><strong>Practical technique:</strong> After making significant decisions, consciously connect the outcome to your initial somatic response. Did your gut feeling prove accurate? What physical sensations preceded good or bad outcomes? This strengthens your body’s decision-making calibration over time.</p> <h3 id="the-stress-response-paradox">The Stress Response Paradox</h3> <p>Here’s something counterintuitive: mild stress often improves decision-making by activating your somatic marker system. But chronic stress or extreme anxiety can overwhelm it, leading to poor choices.</p> <p><strong>Practical technique:</strong> For important decisions, aim for a state of “aroused calm” — alert but not panicked. If you’re too stressed, your somatic markers become noise. If you’re too relaxed, you lose access to important emotional information.</p> <h2 id="the-modern-twist-ai-emotions-and-the-future-of-decision-making">The Modern Twist: AI, Emotions, and the Future of Decision-Making</h2> <p>In our age of artificial intelligence and algorithmic decision-making, Damasio’s insights take on new urgency. We’re building systems that can process vast amounts of data and optimize for specific outcomes, but they lack the embodied wisdom that comes from having a body that can feel consequences.</p> <p>This creates a fascinating paradox: AI systems are already superior to humans at many types of decisions — medical diagnosis, financial analysis, strategic planning. But they fail catastrophically at the kinds of decisions that require integrating emotional intelligence with rational analysis.</p> <p>When you’re deciding whether to trust someone, whether to take a creative risk, whether to commit to a relationship — you’re not just processing information. You’re integrating decades of embodied experience, physiological responses, and contextual awareness in ways that no algorithm can yet replicate.</p> <p>This isn’t necessarily a limitation of AI — it’s a reminder of something unique about human intelligence. Our emotions aren’t a bug in the system. They’re a feature.</p> <h2 id="the-deeper-revolution">The Deeper Revolution</h2> <p>Damasio’s work suggests something profound about the nature of human experience: we don’t just think with our brains. We think with our entire bodies. This has implications that extend far beyond neuroscience:</p> <p><strong>For Education:</strong> Maybe we need to teach decision-making skills that incorporate both analytical thinking and emotional intelligence. Traditional education focuses on suppressing emotions to think clearly, but this might be exactly backward.</p> <p><strong>For Leadership:</strong> The best leaders might be those who can integrate rational analysis with somatic wisdom. They can feel the emotional climate of their organization while maintaining strategic clarity.</p> <p><strong>For Mental Health:</strong> Understanding how trauma affects the body’s decision-making systems opens new therapeutic possibilities. PTSD, anxiety, and depression all involve disruptions to the somatic marker system.</p> <p><strong>For Philosophy:</strong> The ancient mind-body problem gets reframed entirely when we realize there was never a real separation to begin with. We’re not minds trapped in bodies or bodies controlled by minds. We’re integrated beings whose intelligence emerges from their unity.</p> <h2 id="the-bottom-line-your-body-has-been-keeping-score-all-along">The Bottom Line: Your Body Has Been Keeping Score All Along</h2> <p>Antonio Damasio didn’t just propose a new theory of decision-making. He revealed something fundamental about what it means to be human. We’re not rational machines occasionally disrupted by emotions. We’re not emotional creatures occasionally guided by reason.</p> <p>We’re integrated beings whose intelligence emerges from the complex interplay between our bodies, our emotions, and our conscious minds. The artificial separation between “thinking” and “feeling” that has dominated Western thought for centuries isn’t just wrong — it’s counterproductive.</p> <p>The next time you’re facing a difficult decision, pay attention to what your body is telling you. That flutter in your chest, that knot in your stomach, that sense of lightness or heaviness — these aren’t distractions from good decision-making. They’re part of the most sophisticated decision-making system ever evolved.</p> <p>Your body has been keeping score all along. Maybe it’s time to start listening.</p> <p>After all, Elliot could analyze every angle of his life’s collapse with perfect logic, but he couldn’t feel why any of it mattered. Don’t make his mistake. Your emotions aren’t the enemy of good judgment — they’re its most essential ally.</p> <p>The next time someone tells you to “think, don’t feel,” remember: you can’t actually do one without the other. And that’s not a limitation of human intelligence — it’s its greatest strength.</p>]]></content><author><name></name></author><category term="cognitive-science"/><category term="neuroscience,"/><category term="decision-making,"/><category term="emotions,"/><category term="cognition,"/><category term="psychology"/><summary type="html"><![CDATA[Why your gut feelings aren't just superstition — they're sophisticated neural machinery that's been running your life all along]]></summary></entry><entry><title type="html">LLM Inference Optimization</title><link href="https://xmarva.github.io/blog/2025/inference-optimization/" rel="alternate" type="text/html" title="LLM Inference Optimization"/><published>2025-06-19T10:00:00+00:00</published><updated>2025-06-19T10:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/inference-optimization</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/inference-optimization/"><![CDATA[<p>You’re not an ML Engineer if you can’t deploy and optimize inference. This statement might sound harsh, but it reflects a fundamental shift in what the industry expects from machine learning practitioners today. Building impressive models in research environments is one thing; keeping them running efficiently in production under real-world constraints is an entirely different challenge.</p> <p>Large transformer models have become the backbone of modern AI applications, delivering state-of-the-art results across numerous tasks. However, their power comes at a significant cost. The computational and memory requirements for inference can be prohibitive, with some models requiring multiple high-end GPUs and generating response times measured in seconds rather than milliseconds.</p> <p>When your model hits production and users start complaining about 7-second latencies while your GPU utilization sits at 99% and costs spiral out of control, the gap between research and production becomes painfully clear.</p> <p>The challenge isn’t just about making models work; it’s about making them work efficiently at scale. Modern production environments demand systems that can handle thousands of concurrent requests, maintain sub-second response times, and operate within reasonable cost budgets. This requires a deep understanding of inference optimization techniques that go far beyond basic model compression or quantization.</p> <h2 id="key-value-caching-with-paged-attention">Key-Value Caching with Paged Attention</h2> <p>The transformer architecture’s self-attention mechanism creates a fundamental computational bottleneck during inference. By default, when generating each new token, the model recomputes attention weights over the entire sequence history. For a sequence of length \(n\), this results in \(O(n^2)\) computational complexity that grows quadratically with context length. This becomes particularly problematic for applications requiring long context windows, such as document analysis or extended conversations.</p> <p>In practice, this bottleneck manifests differently depending on your hardware setup. On a single A100 with 80GB memory, you might handle 32K context length with batch size 1, but try to increase the batch size to 4 and you’ll immediately hit out-of-memory errors. The memory consumption follows the pattern:</p> \[\text{Memory} = \text{batch\_size} \times \text{seq\_length} \times \text{hidden\_size} \times \text{num\_layers} \times 2 \times \text{precision\_bytes}\] <p>where the factor of 2 accounts for both keys and values.</p> <p>Key-Value (KV) caching addresses this inefficiency by storing the computed key and value matrices from previous tokens, allowing the model to only compute attention for the new token against the cached states. The attention computation becomes:</p> \[\text{Attention}(Q_{\text{new}}, K_{\text{cached}}, V_{\text{cached}}) = \text{softmax}\left(\frac{Q_{\text{new}} K_{\text{cached}}^T}{\sqrt{d_k}}\right) V_{\text{cached}}\] <p>However, naive KV caching implementations create their own problems. A typical production scenario involves serving hundreds of concurrent users with varying conversation lengths. Without proper memory management, you’ll see memory fragmentation where 40GB of your 80GB GPU memory shows as “allocated” but only 20GB is actually being used for active computations. The remaining 20GB is trapped in unusable fragments between completed conversations.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-06-19-inference-optimization/0-480.webp 480w,/assets/img/posts/2025-06-19-inference-optimization/0-800.webp 800w,/assets/img/posts/2025-06-19-inference-optimization/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-06-19-inference-optimization/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Paged Attention in vLLM system overview" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Paged Attention in vLLM system overview </div> <p>This fragmentation issue becomes particularly acute during peak usage periods. You might start the day serving 200 concurrent users smoothly, but by afternoon, after thousands of conversations have started and completed, you can only handle 100 concurrent users due to fragmented memory. The only solution is often a complete service restart, which is unacceptable in production environments.</p> <p>Paged attention, implemented in systems like vLLM, revolutionizes KV cache management by treating memory like an operating system manages virtual memory. Instead of allocating contiguous memory blocks for each sequence, paged attention divides the KV cache into fixed-size pages that can be dynamically allocated and deallocated. In practice, this means using page sizes of 16 or 32 tokens, creating a balance between memory efficiency and computational overhead.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-06-19-inference-optimization/1-480.webp 480w,/assets/img/posts/2025-06-19-inference-optimization/1-800.webp 800w,/assets/img/posts/2025-06-19-inference-optimization/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-06-19-inference-optimization/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Storing the KV cache of two requests at the same time in vLLM" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Storing the KV cache of two requests at the same time in vLLM </div> <p>The real-world impact is dramatic. In production deployments, paged attention can improve memory utilization from 60-70% to 90-95%, allowing you to serve 2-3x more concurrent users on the same hardware. The page table overhead is minimal, typically consuming less than 1% of total memory, while the flexibility gains are substantial. When a user ends their conversation, their memory pages are immediately available for new conversations, eliminating the gradual memory fragmentation that plagues traditional implementations.</p> <h2 id="continuous-batching-for-dynamic-workloads">Continuous Batching for Dynamic Workloads</h2> <p>Traditional batching approaches assume that requests arrive in synchronized groups, but production workloads are inherently streaming and asynchronous. Users don’t coordinate their requests, leading to irregular arrival patterns that can severely underutilize available compute resources if handled naively.</p> <p>The reality of production traffic is far messier than research benchmarks suggest. During typical business hours, you might see 50 requests per minute with response lengths varying from 10 tokens to 2000 tokens. Traditional static batching with a batch size of 8 would either waste GPU cycles waiting for enough requests to fill a batch, or force you to use smaller batch sizes that underutilize the hardware. The utilization often drops to 30-40% during off-peak hours when requests are sparse.</p> <p>Continuous batching solves this problem by dynamically combining requests as they arrive, creating batches on-the-fly rather than waiting for predetermined batch sizes. The system maintains a queue of pending requests and continuously forms new batches based on available computational capacity and memory constraints.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-06-19-inference-optimization/3-480.webp 480w,/assets/img/posts/2025-06-19-inference-optimization/3-800.webp 800w,/assets/img/posts/2025-06-19-inference-optimization/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-06-19-inference-optimization/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Static batching" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Completing four sequences using static batching. On the first iteration (left), each sequence generates one token (blue) from the prompt tokens (yellow). After several iterations (right), the completed sequences each have different sizes because each emits their end-of-sequence-token (red) at different iterations. Even though sequence 3 finished after two iterations, static batching means that the GPU will be underutilized until the last sequence in the batch finishes generation (in this example, sequence 2 after six iterations). </div> <p>The implementation complexity is significant. You need to track not just the current batch composition, but also predict memory requirements for each request addition. A naive approach might add requests to a batch until GPU memory is exhausted, but this leads to out-of-memory crashes when the prediction is wrong. A more robust approach maintains a safety margin of 10-15% and uses conservative estimates for memory consumption.</p> <p>The key insight is that different requests within a batch can be at different stages of generation. While one request might be generating its 50th token, another could be starting its first. The system tracks the state of each request independently while still benefiting from the computational efficiency of batched operations. This requires sophisticated tensor manipulation where each row in the batch might have different sequence lengths and different remaining generation requirements.</p> <p>In practice, continuous batching introduces latency tradeoffs that must be carefully managed. A request that arrives when the GPU is processing a large batch might wait 200-500ms before being included in the next batch. For interactive applications, this delay can be noticeable. Advanced implementations use preemption strategies where long-running requests can be temporarily paused to accommodate new, potentially shorter requests.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-06-19-inference-optimization/4-480.webp 480w,/assets/img/posts/2025-06-19-inference-optimization/4-800.webp 800w,/assets/img/posts/2025-06-19-inference-optimization/4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-06-19-inference-optimization/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Continuous batching" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Completing seven sequences using continuous batching. Left shows the batch after a single iteration, right shows the batch after several iterations. Once a sequence emits an end-of-sequence token, we insert a new sequence in its place (i.e. sequences S5, S6, and S7). This achieves higher GPU utilization since the GPU does not wait for all sequences to complete before starting a new one. </div> <p>The scheduling algorithm becomes the critical component. A simple first-come-first-served approach can lead to head-of-line blocking where a single long request delays many shorter ones. Production systems typically implement weighted fair queuing or shortest-job-first strategies, but these require accurate prediction of request completion times, which is challenging for generative models where output length isn’t known in advance.</p> <p>Real-world measurements show that continuous batching can achieve throughput improvements of 3-5x compared to traditional static batching in production environments with variable request patterns. However, the improvement depends heavily on request size distribution and arrival patterns. Workloads with many short requests see larger benefits than those dominated by long document processing tasks.</p> <h2 id="multi-gpu-orchestration-and-model-sharding">Multi-GPU Orchestration and Model Sharding</h2> <p>As models grow beyond the memory capacity of single GPUs, sophisticated distribution strategies become essential. Modern large language models often require multiple GPUs working in coordination, either through model parallelism (splitting the model across devices) or through replica serving (running multiple copies for higher throughput).</p> <p>The transition from single-GPU to multi-GPU serving introduces a host of practical challenges that aren’t immediately obvious. A 70B parameter model like Llama-2-70B requires approximately 140GB of memory in FP16 precision, necessitating distribution across at least two A100 GPUs. However, simply splitting the model layers across GPUs introduces significant communication overhead that can reduce effective throughput by 40-60% compared to theoretical maximum.</p> <p>Model parallelism involves partitioning the model’s layers or parameters across multiple GPUs. For transformer models, this typically means distributing attention heads across devices or splitting the feed-forward network components. The challenge lies in minimizing communication overhead between GPUs while maintaining computational efficiency.</p> <p>In practice, tensor parallelism often works better than pipeline parallelism for transformer models. With tensor parallelism, each attention head computation \(\text{head}_i = \text{Attention}(Q_i, K_i, V_i)\) is distributed across GPUs, where each GPU handles a subset of heads. This requires all-gather operations after each layer, but the communication pattern is more predictable and the memory usage is more balanced.</p> <p>Pipeline parallelism, where different layers run on different GPUs, seems attractive but introduces bubble time where GPUs wait for data from previous stages. In production, pipeline parallelism works well only with large batch sizes (32 or higher) that can keep all pipeline stages busy. For interactive workloads with small batch sizes, the pipeline bubbles can waste 50-70% of GPU cycles.</p> <p>The networking infrastructure becomes critical for multi-GPU deployments. NVLink connections between GPUs on the same node provide 50-100 GB/s bandwidth, but inter-node communication over InfiniBand typically provides only 25-50 GB/s. This bandwidth difference means that placing communicating model components on the same node is crucial for performance.</p> <p>Effective multi-GPU serving also requires intelligent request routing that considers the current load on each device, the memory requirements of incoming requests, and the communication costs of different placement strategies. Advanced systems can dynamically migrate requests between GPUs or adjust the parallelization strategy based on real-time performance metrics.</p> <p>In production environments, GPU failures are not uncommon. A single GPU failure in a tensor-parallel setup brings down the entire model instance. Robust systems implement health monitoring that can detect GPU degradation before complete failure and gracefully migrate workloads to healthy replicas. The mean time to recovery (MTTR) for GPU failures can be 5-15 minutes, during which alternative serving capacity must handle the load.</p> <p>Load balancing across GPU replicas requires more sophistication than traditional web service load balancing. GPU warmup time after a cold start can be 30-120 seconds depending on model size, and the memory state of each replica affects its capacity to accept new requests. Simple round-robin load balancing often leads to uneven utilization where some GPUs are overwhelmed while others are underutilized.</p> <h2 id="token-streaming-and-progressive-response-generation">Token Streaming and Progressive Response Generation</h2> <p>User experience in interactive applications depends heavily on perceived responsiveness rather than just raw throughput. Even if generating a complete response takes several seconds, users often prefer to see partial results immediately rather than waiting for the entire output.</p> <p>The psychological impact of streaming is significant and measurable. User studies show that perceived response time for a 3-second generation with immediate streaming feels similar to a 1.5-second generation without streaming. However, implementing streaming correctly is more complex than simply sending tokens as they arrive.</p> <p>Token streaming addresses this by sending generated tokens to the client as soon as they’re produced, creating a typewriter effect that provides immediate feedback. This approach requires careful implementation to handle network buffering, error conditions, and client-side rendering efficiently.</p> <p>The implementation typically uses Server-Sent Events (SSE) or WebSocket connections to maintain a persistent communication channel between the server and client. Each generated token is immediately serialized and transmitted, along with metadata indicating the current generation state and any confidence scores or alternative candidates.</p> <p>Network buffering can sabotage streaming implementations. TCP’s Nagle algorithm and various proxy servers often buffer small packets, introducing 40-200ms delays that defeat the purpose of streaming. Production systems must explicitly disable Nagle’s algorithm and use HTTP/2 or HTTP/3 where possible to minimize buffering delays. Additionally, reverse proxies like nginx require specific configuration (proxy_buffering off) to avoid accumulating streamed responses.</p> <p>Browser compatibility introduces another layer of complexity. While modern browsers handle SSE well, older versions or certain corporate firewalls can interfere with streaming connections. Production systems typically implement fallback mechanisms that detect when streaming fails and switch to traditional request-response patterns.</p> <p>Streaming becomes more complex when dealing with constrained generation scenarios, such as JSON output or structured data formats. The system must buffer tokens until it can verify that the partial output remains valid according to the specified constraints. This might require implementing streaming parsers that can validate partial structures and provide early termination if the generation goes off-track.</p> <p>For JSON generation, you can’t stream individual characters because partial JSON is invalid. Instead, the system must buffer until complete JSON objects or arrays are formed. This reduces the streaming benefit but still provides better user experience than waiting for complete responses. The buffering strategy might hold tokens until reaching closing braces or brackets that complete valid JSON fragments.</p> <p>Error handling in streaming scenarios requires special consideration. If generation fails midway through a response, the client needs to be notified appropriately, and any partial results must be clearly marked as incomplete. Recovery strategies might include automatic retry with different sampling parameters or fallback to alternative models.</p> <p>The infrastructure costs of streaming are also significant. Each streaming connection maintains server resources for the entire generation duration, typically 2-10 seconds per request. For high-volume applications, this can require 5-10x more connection handling capacity compared to traditional request-response patterns. Load balancers must be configured to handle long-lived connections, and connection pooling strategies need adjustment.</p> <h2 id="asynchronous-queue-management-and-backpressure-control">Asynchronous Queue Management and Backpressure Control</h2> <p>Production inference systems must handle highly variable request loads without degrading performance or dropping requests. A robust queueing system serves as the buffer between unpredictable user demand and the fixed computational capacity of the inference hardware.</p> <p>The challenge becomes apparent during traffic spikes. A typical production system might handle 100 requests per minute during normal operation, but see spikes to 1000 requests per minute during peak usage or viral events. Without proper queueing, these spikes either crash the system or force you to overprovision hardware by 10x to handle rare peaks.</p> <p>Asynchronous queues decouple request reception from processing, allowing the system to accept requests even when all compute resources are busy. The queue management system must implement multiple priority levels, timeout handling, and fair scheduling policies to ensure good user experience across different usage patterns.</p> <p>The queue depth becomes a critical operational metric. In practice, queue depths of 50-100 requests are manageable, but beyond 200-300 requests, user experience degrades significantly. Users expect responses within 10-15 seconds for interactive applications, so queue depths translate directly to response time guarantees. A queue processing 10 requests per minute with 200 pending requests means 20-minute response times, which is unacceptable.</p> <p>Backpressure mechanisms prevent the system from accepting more work than it can handle, which could lead to cascading failures or excessive latency. When queue depth exceeds predetermined thresholds, the system can implement various strategies: rejecting new requests with appropriate error codes, upgrading to faster but less accurate models, or redirecting traffic to alternative endpoints.</p> <p>The implementation often involves multiple queue tiers with different service level agreements. Premium users might have access to a high-priority queue with guaranteed sub-5-second response times, while free tier users accept longer delays. The scheduler must balance these priorities while avoiding starvation of lower-priority requests.</p> <p>Redis Streams or Apache Kafka provide robust foundations for distributed queueing, but the integration with GPU inference workers requires careful consideration. GPU workers can’t efficiently handle traditional message acknowledgment patterns because they need to commit to processing requests for several seconds. Dead letter queues become essential for handling requests that fail processing or exceed timeout limits.</p> <p>The queue management system should implement sophisticated timeout handling that considers both user expectations and resource utilization. Short requests might have tight timeout requirements, while complex analytical tasks might warrant longer processing windows. The system must track per-request timeouts and proactively cancel work that can no longer meet its deadline.</p> <p>Circuit breaker patterns add another layer of protection. If the inference service starts returning errors at rates above 5-10%, the circuit breaker can redirect traffic to fallback responses or cached results rather than continuing to queue requests that will likely fail. The circuit breaker must distinguish between temporary GPU memory pressure and permanent model failures to avoid unnecessary service degradation.</p> <p>Priority-based scheduling allows the system to differentiate between different types of requests. Interactive user queries might receive higher priority than batch processing tasks, or premium users might get preferential treatment during high-load periods. However, implementing fairness across priority levels requires careful tuning to prevent lower-priority requests from being starved indefinitely.</p> <h2 id="hardware-aware-runtime-optimization">Hardware-Aware Runtime Optimization</h2> <p>Modern inference acceleration relies heavily on specialized hardware features and optimized software stacks that can take advantage of these capabilities. Graphics processors offer tensor cores designed specifically for mixed-precision arithmetic, while newer architectures provide dedicated units for sparse computations and quantized operations.</p> <p>The gap between theoretical performance and actual throughput can be startling. An A100 GPU advertises 312 TFLOPS of mixed-precision performance, but naive PyTorch implementations often achieve only 20-40 TFLOPS in practice. The difference lies in memory bandwidth utilization, kernel fusion, and precision management that specialized runtimes handle automatically.</p> <p>TensorRT, NVIDIA’s inference optimization library, provides automatic kernel fusion, precision calibration, and memory layout optimization specifically tuned for inference workloads. The system analyzes the computational graph and applies various optimizations: fusing consecutive operations to reduce memory bandwidth requirements, selecting optimal precision levels for different operations, and choosing memory layouts that maximize cache efficiency.</p> <p>In production deployments, TensorRT can improve inference speed by 2-5x compared to standard PyTorch implementations, but the optimization process itself can take 10-30 minutes depending on model size. This optimization time must be factored into deployment pipelines, and the optimized models are hardware-specific and can’t be easily moved between different GPU architectures.</p> <p>The precision calibration process requires representative data samples to determine optimal quantization scales. Using inappropriate calibration data can result in accuracy degradation that might not be immediately apparent in automated tests but becomes obvious in production usage. The calibration dataset should represent real user inputs, not just validation sets from model training.</p> <p>ONNX Runtime offers cross-platform optimization capabilities that can target different hardware backends while providing a unified interface. The runtime includes optimization passes that eliminate redundant operations, fold constants, and apply algebraic simplifications that reduce computational requirements without affecting model accuracy.</p> <p>However, ONNX conversion introduces its own challenges. Complex PyTorch operations might not have direct ONNX equivalents, requiring manual implementation of custom operators. The conversion process can also introduce subtle numerical differences that accumulate over long generation sequences, leading to different outputs compared to the original PyTorch model.</p> <p>For CPU-based deployment, specialized runtimes like llama.cpp implement highly optimized kernels that take advantage of specific instruction sets such as AVX-512 or ARM NEON. These implementations often outperform general-purpose frameworks by significant margins, especially for quantized models running on consumer hardware.</p> <p>The performance characteristics vary dramatically across hardware. A quantized INT8 model might run 3x faster on modern Intel CPUs with VNNI instructions, but show minimal improvement on older architectures lacking these specialized units. Apple’s M-series processors with their unified memory architecture demonstrate different optimization patterns entirely, often favoring larger batch sizes than traditional GPU deployments.</p> <p>The choice of runtime significantly impacts performance, and the optimal selection depends on the specific model architecture, target hardware, and precision requirements. A quantized INT8 model running on a runtime optimized for full-precision operations might perform worse than the original FP32 model on a properly optimized stack.</p> <p>Framework overhead becomes particularly visible in high-throughput scenarios. Python’s Global Interpreter Lock (GIL) can become a bottleneck when handling many concurrent requests, even if the actual model inference happens in optimized C++ or CUDA code. Production systems often use multiple worker processes or specialized serving frameworks like Triton Inference Server to bypass these limitations.</p> <h2 id="fault-isolation-and-system-resilience">Fault Isolation and System Resilience</h2> <p>Large language models are complex systems that can fail in numerous ways: memory leaks that gradually degrade performance, infinite loops in generation that consume resources indefinitely, or edge cases in input processing that cause crashes. Production systems must be designed to contain these failures and maintain overall service availability.</p> <p>The failure modes are often subtle and difficult to detect in development environments. Memory leaks might only become apparent after processing thousands of requests over several hours. A model might work perfectly for 99% of inputs but crash on specific Unicode characters or extremely long input sequences. These edge cases are particularly problematic because they can bring down entire model instances, affecting many concurrent users.</p> <p>Process isolation provides the fundamental building block for fault tolerance by running each model instance in a separate process or container. When a model process crashes or becomes unresponsive, only that specific instance is affected, and the orchestration system can restart it without impacting other running models or user sessions.</p> <p>In practice, containerization with Docker or similar technologies provides the necessary isolation, but the configuration details matter significantly. Memory limits must be set appropriately to prevent one model from consuming all available system memory, but too-restrictive limits can cause legitimate requests to fail. GPU memory isolation is particularly challenging because CUDA contexts are typically shared across processes on the same device.</p> <p>Kubernetes provides excellent orchestration capabilities for containerized model serving, but the default restart policies aren’t always appropriate for GPU workloads. A crashed model container might restart on a different node that doesn’t have the required GPU resources, leading to extended downtime while the scheduler finds appropriate placement.</p> <p>Health monitoring systems continuously probe model instances to detect degraded performance or unresponsive behavior. These checks might include periodic test queries with known expected outputs, memory usage monitoring, and response time tracking. When an instance fails health checks, it can be automatically removed from the serving pool and restarted.</p> <p>The health check implementation requires careful consideration of what constitutes a failure. A model that takes 30 seconds to respond to a complex query might be functioning normally, but a model that takes 30 seconds for a simple “hello world” request is clearly degraded. The health checks must distinguish between temporary load-induced slowdowns and permanent failures.</p> <p>Memory leak detection becomes crucial for long-running model services. GPU memory leaks are particularly insidious because they might not be immediately apparent and can gradually degrade performance over hours or days. Monitoring GPU memory usage patterns and implementing automatic restarts when memory usage exceeds expected thresholds helps maintain system stability.</p> <p>Hot restart capabilities allow the system to replace failed model instances without service interruption. This requires maintaining warm standby processes that are ready to accept traffic immediately when a primary instance fails. The standby processes must have the model already loaded in memory and be prepared to handle the specific configuration and routing requirements of the failed instance.</p> <p>The cost of maintaining hot standbys is significant, potentially doubling infrastructure costs. More economical approaches might maintain cold standbys that can be activated within 30-60 seconds, accepting brief service degradation in exchange for lower operational costs.</p> <p>Circuit breaker patterns protect the overall system from cascading failures when individual components become unreliable. If a particular model endpoint starts returning errors at a high rate, the circuit breaker can temporarily stop routing requests to that endpoint, allowing it time to recover while protecting users from experiencing failures.</p> <p>The circuit breaker configuration requires tuning based on expected failure patterns. A threshold of 10% error rate might be appropriate for experimental models, while production-critical services might use 1-2% thresholds. The recovery time must balance system stability with service availability, typically using exponential backoff to gradually reintroduce traffic to recovered endpoints.</p> <h2 id="gpu-aware-autoscaling-strategies">GPU-Aware Autoscaling Strategies</h2> <p>Traditional autoscaling approaches based on CPU metrics are inadequate for GPU-accelerated inference workloads where the primary constraint is specialized compute resources rather than general processing capacity. Effective autoscaling for language model inference requires metrics that reflect GPU utilization, memory pressure, and queue dynamics.</p> <p>The challenge is that GPU metrics are fundamentally different from CPU metrics. While CPU utilization is relatively uniform and predictable, GPU utilization can spike from 0% to 100% within milliseconds as batch processing begins and ends. A GPU showing 30% average utilization might actually be alternating between 0% idle and 100% busy states, making traditional threshold-based autoscaling ineffective.</p> <p>GPU memory utilization provides a critical signal for scaling decisions, but it must be interpreted carefully. Unlike CPU memory, GPU memory is typically allocated in large chunks and managed differently by various frameworks. The autoscaler must distinguish between memory that’s allocated but idle and memory that’s actively being used for computation.</p> <p>In practice, monitoring GPU memory fragmentation becomes as important as monitoring total usage. A GPU showing 60% memory utilization might actually be unable to accept new requests due to fragmentation, necessitating scale-out decisions based on memory fragmentation metrics rather than just total usage.</p> <p>Request queue length and processing latency offer additional signals that reflect user-visible performance degradation. When average response times start increasing or queue depths grow beyond acceptable thresholds, the system should proactively scale out additional capacity rather than waiting for resource exhaustion.</p> <p>Cold start considerations become particularly important for GPU workloads where model loading can take 30-60 seconds or more. The autoscaler must anticipate demand increases and begin warming new instances before they’re critically needed. This might involve maintaining a small pool of pre-warmed instances or implementing predictive scaling based on historical usage patterns.</p> <p>The autoscaling system must also consider the heterogeneous nature of GPU resources. Different instance types offer different capabilities, and the system might need to choose between adding high-memory instances for long-context workloads versus high-throughput instances for many short requests.</p> <h2 id="token-budget-management-and-output-control">Token Budget Management and Output Control</h2> <p>Inference costs scale directly with the number of tokens processed, including both input context and generated output. Production systems require sophisticated controls to manage these costs while maintaining good user experience across different usage tiers and application requirements.</p> <p>Token budgeting operates at multiple levels: per-request limits that prevent individual queries from consuming excessive resources, per-user quotas that enforce fair usage policies, and system-wide controls that protect against resource exhaustion during traffic spikes.</p> <p>The implementation must distinguish between different types of token usage. Input tokens consumed by long documents or conversation history might be treated differently from output tokens generated in response to user queries. Some applications might allow unlimited input processing but strictly control output generation, while others might implement combined budgets.</p> <p>Early stopping mechanisms provide additional cost control by terminating generation when certain conditions are met. This might include stopping at natural sentence boundaries, detecting repetitive outputs, or identifying when the model’s confidence in subsequent tokens drops below acceptable thresholds.</p> <p>Rate limiting complements token budgeting by controlling the temporal distribution of resource usage. A user might have a large token budget but be limited in how quickly they can consume it, preventing burst usage patterns that could impact other users or destabilize the system.</p> <h2 id="comprehensive-observability-and-performance-monitoring">Comprehensive Observability and Performance Monitoring</h2> <p>Effective optimization requires detailed visibility into system behavior at multiple levels: individual request performance, model-level metrics, hardware utilization, and overall system health. The monitoring system must capture both real-time operational data and longer-term trends that inform capacity planning and optimization priorities.</p> <p>Request-level telemetry tracks the complete lifecycle of each inference request: queuing time, model loading time, actual inference duration, and response transmission time. This granular data enables identification of bottlenecks and optimization opportunities that might not be visible in aggregate metrics.</p> <p>GPU profiling provides insights into hardware utilization patterns, memory access efficiency, and kernel performance. Tools like NVIDIA Nsight or PyTorch Profiler can reveal whether computational kernels are memory-bound or compute-bound, informing decisions about model architecture changes or runtime optimizations.</p> <p>Cache performance metrics become particularly important for systems using KV caching or other memory optimization techniques. Cache hit rates, memory fragmentation levels, and eviction patterns provide insights into how well the caching strategy matches actual usage patterns.</p> <p>Distributed tracing becomes essential for complex multi-GPU or multi-model deployments where a single user request might involve multiple service components. The tracing system must correlate activities across different processes and devices to provide a complete picture of request processing.</p> <p>The monitoring system should implement automated alerting that can detect both immediate failures and gradual performance degradation. This includes not just binary up/down checks, but also trend analysis that can identify slowly developing problems before they impact users.</p>]]></content><author><name></name></author><category term="system-design"/><category term="machine-learning,"/><category term="transformers,"/><category term="production,"/><category term="optimization,"/><category term="high-load"/><summary type="html"><![CDATA[Production-Ready Techniques for Real-World Deployment]]></summary></entry><entry><title type="html">PEFT Method Overview [implementing Adapters in PyTorch]</title><link href="https://xmarva.github.io/blog/2025/adapters/" rel="alternate" type="text/html" title="PEFT Method Overview [implementing Adapters in PyTorch]"/><published>2025-05-11T10:00:00+00:00</published><updated>2025-05-11T10:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/adapters</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/adapters/"><![CDATA[<p>In the rapidly evolving landscape <a href="https://xmarva.github.io/blog/2025/building-a-transformer/">transformer-based architectures</a>, a significant challenge has emerged: how do we customize these increasingly massive models for specific tasks without breaking the bank on computational resources?</p> <p>Enter Parameter-Efficient Fine-Tuning (PEFT), a family of techniques that has revolutionized how we adapt pre-trained models to downstream tasks.</p> <h2 id="the-fine-tuning-dilemma">The Fine-Tuning Dilemma</h2> <p>So, you’ve got access to a SoTA LM with billions of parameters.</p> <p>Perhaps it’s GPT-4, LLaMA 3, Mistral or Qwen. You want to adapt this model to a specialized domain like medical text analysis or legal document processing.</p> <p>The traditional approach would involve fine-tuning the entire model on your domain-specific data.</p> <p>Full fine-tuning comes with substantial costs:</p> <ol> <li><strong>Computational Expense</strong>: Training billions of parameters requires significant GPU resources.</li> <li><strong>Storage Overhead</strong>: Each fine-tuned version requires storing a complete copy of the model</li> <li><strong>Catastrophic Forgetting</strong>: Aggressive fine-tuning might cause the model to lose its general capabilities</li> <li><strong>Limited Scalability</strong>: Maintaining multiple specialized versions becomes unmanageable</li> </ol> <p>This is where PEFT techniques come to the rescue. Rather than updating all parameters, PEFT methods focus on adding and training a small number of parameters while keeping most of the pre-trained model frozen. This approach typically requires updating less than 1% of the parameters compared to full fine-tuning, while achieving comparable performance.</p> <p>Let’s understand most significant PEFT methods, their core principles, and implement them using PyTorch for better understanding. Then we’ll explore how to use these techniques with the Hugging Face <code class="language-plaintext highlighter-rouge">peft</code> library for practical applications.</p> <h2 id="general-overview-of-peft-methods">General Overview of PEFT methods</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/method-overview-480.webp 480w,/assets/img/posts/2025-05-11-adapters/method-overview-800.webp 800w,/assets/img/posts/2025-05-11-adapters/method-overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/method-overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Parameter-efficient fine-tuning methods taxonomy." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Parameter-efficient fine-tuning methods taxonomy. Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning.s </div> <hr/> <h3 id="1-addition-based-methods"><strong>1. Addition-Based Methods</strong></h3> <p>These approaches add new, lightweight modules to a pre-trained model while keeping the original weights frozen. This is the most widely explored category and includes two main subtypes: <strong>adapters</strong> and <strong>soft prompts</strong>.</p> <ul> <li><strong>Adapters</strong> (e.g., Bottleneck Adapters, Parallel Adapters): Small neural layers inserted within Transformer blocks that are trained while the rest of the model remains unchanged. Variants differ in placement, structure, and compression strategies.</li> <li><strong>LoRA (Low-Rank Adaptation)</strong>: Instead of fine-tuning full weight matrices, LoRA introduces low-rank decompositions for weight updates (e.g., replacing a full-rank update with <code class="language-plaintext highlighter-rouge">W_down * W_up</code>).</li> <li><strong>Prefix Tuning / Prompt Tuning</strong>: Add trainable vectors (prefixes or prompts) to the model’s input or internal layers. These methods steer model behavior without changing its core parameters.</li> <li><strong>Soft Prompts</strong>: Instead of using discrete tokens, these train continuous embeddings that are prepended to the input. Can be applied to input embeddings or even across all Transformer layers.</li> </ul> <p>Despite adding new parameters, these methods often use significantly less memory and are more computationally efficient due to fewer gradients and optimizer states being updated.</p> <hr/> <h3 id="2-selection-based-methods"><strong>2. Selection-Based Methods</strong></h3> <p>Selective approaches involve fine-tuning only a specific subset of the model’s original parameters, chosen either manually or via structural criteria.</p> <ul> <li><strong>BitFit</strong>: Fine-tunes only the bias terms of the model, drastically reducing the number of parameters involved.</li> <li><strong>IA³ (Infused Adapter by Inhibiting and Amplifying Activations)</strong>: Adds scalar gating parameters to control the flow of information through the attention and feed-forward layers.</li> <li><strong>Layer-wise Selection</strong>: Fine-tunes only the top or bottom layers of the model, or focuses on specific components (e.g., attention vs. FFN).</li> <li><strong>Sparse Fine-Tuning</strong>: Selects parameters to update based on certain criteria (e.g., magnitude or gradients), ignoring model structure. However, this poses practical challenges for current hardware.</li> </ul> <p>These methods are particularly useful when model updates must be extremely lightweight or constrained due to storage, bandwidth, or privacy concerns.</p> <hr/> <h3 id="3-reparameterization-based-methods"><strong>3. Reparameterization-Based Methods</strong></h3> <p>These techniques re-structure the parameter space to enable efficient updates with fewer trainable weights.</p> <ul> <li><strong>LoRA</strong> (also fits here): Uses low-rank matrices to model weight updates, greatly reducing parameter count.</li> <li><strong>Compacter</strong>: Builds on adapters but compresses them using low-rank decomposition and parameter sharing.</li> <li><strong>(IA)³</strong>: Combines gating and reparameterization ideas to modulate specific subcomponents of the model.</li> <li><strong>KronA / Kron Adapter</strong>: Uses Kronecker product decomposition to represent weight updates with a favorable trade-off between expressiveness and size.</li> <li><strong>Intrinsic SAID</strong>: Employs the Fastfood transform to apply updates within a low-rank subspace, based on the theory that fine-tuning operates within a lower-dimensional manifold.</li> </ul> <p>These methods often target attention-related weights like <code class="language-plaintext highlighter-rouge">W_Q</code>, <code class="language-plaintext highlighter-rouge">W_K</code>, <code class="language-plaintext highlighter-rouge">W_V</code>, where much of the model’s representational power lies.</p> <hr/> <h3 id="4-hybrid-methods"><strong>4. Hybrid Methods</strong></h3> <p>Hybrid approaches combine strategies from multiple categories to balance trade-offs in memory, compute, and performance.</p> <ul> <li><strong>MAM Adapter</strong>: Combines Adapters with Prompt Tuning for better modularity.</li> <li><strong>UniPELT</strong>: Merges LoRA with Adapters and Prompts into a unified framework.</li> <li><strong>Compacter++ / Kron Adapter</strong>: Combines adapter-based methods with Kronecker reparameterization to reduce the number of trainable parameters further.</li> </ul> <p>These methods allow researchers to adapt fine-tuning strategies to specific deployment constraints, whether that be edge devices, multi-task learning, or multilingual models.</p> <h2 id="bottleneck-adapters">Bottleneck Adapters</h2> <p>Adapters were among the first successful PEFT approaches, introduced in <a href="https://arxiv.org/abs/1902.00751">Parameter-Efficient Transfer Learning for NLP</a> by Houlsby et al. in 2019.</p> <p>The core idea is elegantly simple: insert small trainable modules into each layer of a pre-trained network while keeping the original parameters frozen.</p> <p><strong>Bottleneck adapters</strong> add lightweight feed-forward layers into each Transformer block. These adapter layers typically include:</p> <ul> <li>a <strong>down-projection matrix</strong> that reduces the hidden state dimension from \(d\) to a smaller dimension \(b\),</li> <li>a <strong>non-linear activation</strong> \(\sigma\),</li> <li>an <strong>up-projection matrix</strong> that expands the representation back to the original size \(d\), and</li> <li>a <strong>residual connection</strong>, so the original input is added back after transformation:</li> </ul> \[\text{Adapter}(x) = x + W_{\text{up}} \, \sigma(W_{\text{down}} x)\] <p>Depending on the specific configuration, these adapter layers can be placed at various points inside the Transformer block. Other components like residual connections, layer normalizations, activation functions, and the size of the bottleneck layer can also be customized.</p> <p>The <strong>most important hyperparameter</strong> in this setup is the <strong>bottleneck dimension</strong> \(b\). Rather than setting \(b\) directly, it’s usually defined through a parameter called <code class="language-plaintext highlighter-rouge">reduction_factor</code>. This factor represents the ratio between the hidden layer size \(d\) and the bottleneck size \(b\), given by:</p> \[b = \frac{d}{\text{reduction\_factor}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        A bottleneck adapter module that can be inserted into a transformer.
        
        It projects hidden states down to a lower-dimensional space and then 
        back up again, with non-linearity and dropout in between. This helps 
        the model adapt to new tasks without updating the original transformer.
        
        Args:
            hidden_size: The dimension of the model</span><span class="sh">'</span><span class="s">s hidden states (e.g., 768 for BERT-base)
            adapter_size: The smaller bottleneck dimension (e.g., 64)
            dropout_rate: Regularization to improve generalization
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">down_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>  <span class="c1"># d -&gt; b
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>  <span class="c1"># non-linearity
</span>        <span class="n">self</span><span class="p">.</span><span class="n">up_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>    <span class="c1"># b -&gt; d
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Initialize adapter weights — not learned from pretraining, so good init is important!
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="c1"># Store original input for residual connection
</span>        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># Apply adapter: down-project -&gt; non-linear -&gt; up-project -&gt; dropout
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_project</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_project</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add residual and normalize
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>But how do we integrate adapters into a pre-trained model?</p> <p>Let’s see how to modify a standard transformer layer to include our bottleneck adapter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdapterTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        A wrapper around an existing transformer layer that adds adapters after
        attention and after the feed-forward sublayers.

        Args:
            transformer_layer: One layer from a pre-trained transformer (e.g., BERTLayer)
            adapter_size: Bottleneck size for the adapters
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">transformer_layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span><span class="p">.</span><span class="n">all_head_size</span>  <span class="c1"># Model-specific
</span>
        <span class="c1"># Freeze all transformer weights (we don’t train them)
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="c1"># Add bottleneck adapters at two key places:
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_adapter</span> <span class="o">=</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn_adapter</span> <span class="o">=</span> <span class="nc">BottleneckAdapter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Standard attention (output of frozen pre-trained layer)
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Inject adapter after attention
</span>        <span class="n">adapted_attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_adapter</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>

        <span class="c1"># Apply frozen feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">adapted_attention</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">adapted_attention</span><span class="p">)</span>

        <span class="c1"># Inject second adapter after feed-forward
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn_adapter</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>All set, and we need to load pre-trained model and wrap it’s target layers with this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define adapter size
</span><span class="n">adapter_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Wrap all encoder layers with adapter-enabled versions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">)):</span>
    <span class="n">original_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nc">AdapterTransformerLayer</span><span class="p">(</span><span class="n">original_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
<span class="c1"># Check that only adapters will be trained
</span><span class="n">trainable_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trainable parameters: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s"> / </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Now you can tokenize input and train like usual.
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are lightweight and powerful.</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></div> <p>Now, I’m gonna show how to use adapters in transformers library. It’s faster, easier, and production-tested.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pip install adapter-transformers
</span>
<span class="c1"># `BertAdapterModel` = a special version of BERT that allows adapter injection.
</span><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertAdapterModel</span>
<span class="kn">from</span> <span class="n">transformers.adapters</span> <span class="kn">import</span> <span class="n">AdapterConfig</span>

<span class="c1"># Load model and tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertAdapterModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define adapter configuration
</span><span class="n">config</span> <span class="o">=</span> <span class="n">AdapterConfig</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">pfeiffer</span><span class="sh">"</span><span class="p">,</span>                    <span class="c1"># Adapter type: "pfeiffer", "houlsby", etc.
</span>    <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>          <span class="c1"># Bottleneck size (768 / 16 = 48)
</span>    <span class="n">leave_out</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>            <span class="c1"># Skip layer 0 and 11 (i.e., don't inject there)
</span>    <span class="n">non_linearity</span><span class="o">=</span><span class="sh">"</span><span class="s">gelu</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add adapter with a custom name
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Activate + train this adapter
</span><span class="n">model</span><span class="p">.</span><span class="nf">train_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#Tokenize Input and Forward Pass
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are efficient!</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Last hidden state (batch_size, seq_len, hidden_dim)
</span><span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Add a Classification Head (for downstream tasks)
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_classification_head</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Switch to training mode
</span><span class="n">model</span><span class="p">.</span><span class="nf">train_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Forward pass for classification
</span><span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">Adapters are awesome!</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span>

<span class="c1"># Training Only Adapter Parameters
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>

<span class="c1"># Save / Load Adapters Separately
</span>
<span class="c1"># Save adapter after training
</span><span class="n">model</span><span class="p">.</span><span class="nf">save_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">saved/my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Load later into another model
</span><span class="n">model</span><span class="p">.</span><span class="nf">load_adapter</span><span class="p">(</span><span class="sh">"</span><span class="s">saved/my_task_adapter</span><span class="sh">"</span><span class="p">,</span> <span class="n">load_as</span><span class="o">=</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">set_active_adapters</span><span class="p">(</span><span class="sh">"</span><span class="s">my_task_adapter</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="parallel-adapters">Parallel Adapters</h2> <p>While bottleneck adapters are inserted sequentially in the model’s architecture, <strong>parallel Adapters</strong> inject adapter modules in parallel with the main feed-forward layers in each Transformer block, instead of sequentially. This means that the output of the adapter is added to the output of the feed-forward network, not to its input.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/parallel-adapter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/parallel-adapter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/parallel-adapter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/parallel-adapter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Parallel adapter." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Parallel Adapter </div> <p>Let \(x\) be the input to the Transformer block. The original feed-forward output is \(\mathrm{FFN}(x)\), and the adapter path is:</p> \[\mathrm{Adapter}(x) = W_\text{up} \, \sigma(W_\text{down} \, x)\] <p>The final output becomes:</p> \[y = \mathrm{FFN}(x) + \mathrm{Adapter}(x)\] <p>This allows the adapter to independently learn task-specific modifications without disrupting the main path.</p> <p>The parallel design has a slight computational overhead but can better preserve the pre-trained representations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ParallelAdapter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up_project</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        
        <span class="c1"># Initialize weights
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">down_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">xavier_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">up_project</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        
        <span class="c1"># Scale factor - can be trained or fixed
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_project</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_project</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Scale the adapter output and add to original
</span>        <span class="k">return</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">x</span>
</code></pre></div></div> <p>The integration into a transformer layer would be similar to the bottleneck adapter, but the adapter would be applied in parallel rather than sequentially.</p> <h2 id="low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</h2> <p><a href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><strong>LoRA (Low-Rank Adaptation)</strong></a> introduced by <a href="https://arxiv.org/abs/2106.09685">Hu et al. (2021)</a> replaces or augments weight matrices with low-rank decompositions. Instead of fine-tuning a full matrix \(W \in \mathbb{R}^{d \times d}\), LoRA learns two smaller matrices:</p> \[W' = W + A B \quad \text{with} \quad A \in \mathbb{R}^{d \times r}, \; B \in \mathbb{R}^{r \times d}\] <p>Where \(r \ll d\) (typically \(r = 8\) or \(4\)). This drastically reduces the number of trainable parameters. LoRA is usually applied to the attention projection layers (query/key/value/output).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/lora-adapter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/lora-adapter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/lora-adapter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/lora-adapter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="LoRA Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> LoRA Adapter </div> <p>Intuitively, LoRA adds a “low-rank path” through which task-specific information can flow, while keeping the rest of the model fixed.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        LoRA implementation for linear layers.
        
        Args:
            in_features: Input dimension
            out_features: Output dimension
            rank: Rank of the low-rank decomposition
            alpha: Scaling factor for the LoRA contribution
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">rank</span>
        
        <span class="c1"># LoRA weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        
        <span class="c1"># Initialize weights
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_uniform_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lora_B</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># LoRA contribution: scaling * (x @ A) @ B
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_A</span><span class="p">)</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">lora_B</span>
</code></pre></div></div> <p>Now, let’s apply LoRA to a pre-trained linear layer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">linear_layer</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Wraps a pre-trained linear layer with LoRA functionality.
        
        Args:
            linear_layer: The pre-trained nn.Linear module to adapt
            rank: Rank of the low-rank decomposition
            alpha: Scaling factor
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear_layer</span>
        
        <span class="c1"># Freeze original weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="c1"># Add LoRA components
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lora</span> <span class="o">=</span> <span class="nc">LoRALayer</span><span class="p">(</span>
            <span class="n">linear_layer</span><span class="p">.</span><span class="n">in_features</span><span class="p">,</span> 
            <span class="n">linear_layer</span><span class="p">.</span><span class="n">out_features</span><span class="p">,</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Combine original output with LoRA contribution
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>The genius of LoRA is in its efficiency.</p> <p>If the original weight matrix has dimensions n×m, full fine-tuning would require updating n×m parameters. With LoRA, using a rank r, we only need to update r×(n+m) parameters. For large matrices where r « min(n,m), this represents a massive reduction in trainable parameters.</p> <h3 id="applying-lora-to-a-transformer">Applying LoRA to a Transformer</h3> <p>In practice, LoRA is typically applied to specific weight matrices within a transformer, most commonly the query and value projection matrices in attention layers. Here’s how to adapt a transformer’s attention mechanism with LoRA:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>

<span class="k">def</span> <span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">q_proj</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">v_proj</span><span class="sh">"</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">
    Apply LoRA to specific modules in a transformer model.
    
    Args:
        model: A Hugging Face transformer model
        rank: Rank for LoRA decomposition
        alpha: Scaling factor
        target_modules: List of module names to apply LoRA to
    </span><span class="sh">"""</span>
    <span class="c1"># First, freeze all parameters
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="c1"># Then apply LoRA to target modules
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">any</span><span class="p">(</span><span class="n">target_name</span> <span class="ow">in</span> <span class="n">name</span> <span class="k">for</span> <span class="n">target_name</span> <span class="ow">in</span> <span class="n">target_modules</span><span class="p">):</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="c1"># Get the parent module
</span>                <span class="n">parent_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">child_name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">parent_module</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_submodule</span><span class="p">(</span><span class="n">parent_name</span><span class="p">)</span>
                
                <span class="c1"># Replace with LoRA version
</span>                <span class="n">lora_layer</span> <span class="o">=</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
                <span class="nf">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">child_name</span><span class="p">,</span> <span class="n">lora_layer</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">bert-base-uncased</span><span class="sh">"</span><span class="p">)</span>
<span class="n">lora_model</span> <span class="o">=</span> <span class="nf">apply_lora_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div> <h3 id="quantized-lora-qlora">Quantized LoRA (QLoRA)</h3> <p><a href="https://arxiv.org/abs/2305.14314">QLoRA</a>, takes LoRA’s efficiency to the next level by combining it with quantization techniques. The key insight is to keep the base model in a quantized format (typically 4-bit precision) while applying LoRA adapters in full precision.</p> <p>QLoRA has been a game-changer for democratizing LLM fine-tuning, enabling the adaptation of models with over 70 billion parameters on a single consumer GPU.</p> <h2 id="prefix-tuning-virtual-tokens-in-hidden-space">Prefix Tuning: Virtual Tokens in Hidden Space</h2> <p>Now let’s shift our focus to another family of PEFT methods that operate by introducing trainable tokens to the input sequence or hidden states: Prefix Tuning and Prompt Tuning.</p> <p>Prefix Tuning, introduced by <a href="https://arxiv.org/abs/2101.00190">Li and Liang (2021)</a>, prepends a small number of learned key-value vectors (“prefixes”) to the attention mechanism.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/prefix-tuning-480.webp 480w,/assets/img/posts/2025-05-11-adapters/prefix-tuning-800.webp 800w,/assets/img/posts/2025-05-11-adapters/prefix-tuning-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/prefix-tuning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Prefix Tuning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Prefix Tuning </div> <p>Instead of modifying weights, it expands the input to attention as:</p> \[\text{Attention}(\text{prefix} + x)\] <p>This means the model sees the learned prefix as a pseudo-context for every input, influencing the attention output without changing the underlying Transformer parameters.</p> <p>Prefix tuning is powerful for generation tasks like summarization or translation where modifying the attention context is sufficient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrefixTuningModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">prefix_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of Prefix Tuning.
        
        Args:
            hidden_size: Model</span><span class="sh">'</span><span class="s">s hidden size
            prefix_length: Number of virtual tokens to add
            num_layers: Number of transformer layers
            num_heads: Number of attention heads
            head_dim: Dimension of each attention head
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prefix_length</span> <span class="o">=</span> <span class="n">prefix_length</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        
        <span class="c1"># Create a prefix for each layer for both key and value states
</span>        <span class="c1"># Shape: [num_layers, 2, prefix_length, num_heads, head_dim]
</span>        <span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">prefix_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Initialize with a small standard deviation
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key_value_states</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Prepend prefix to key and value states for a specific layer.
        
        Args:
            key_value_states: Tuple of (key, value) states from the model
            layer_idx: Current transformer layer index
        </span><span class="sh">"""</span>
        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">key_value_states</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get the prefix for the current layer
</span>        <span class="c1"># Shape: [2, prefix_length, num_heads, head_dim]
</span>        <span class="n">prefix</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">prefix_tokens</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
        
        <span class="c1"># Extract key and value prefixes
</span>        <span class="n">key_prefix</span> <span class="o">=</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_prefix</span> <span class="o">=</span> <span class="n">prefix</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Reshape to match model's key and value shapes
</span>        <span class="c1"># From: [batch_size, prefix_length, num_heads, head_dim]
</span>        <span class="c1"># To: [batch_size, num_heads, prefix_length, head_dim]
</span>        <span class="n">key_prefix</span> <span class="o">=</span> <span class="n">key_prefix</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value_prefix</span> <span class="o">=</span> <span class="n">value_prefix</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="c1"># Concatenate with original states
</span>        <span class="c1"># Original shape: [batch_size, num_heads, seq_length, head_dim]
</span>        <span class="n">new_key_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">key_prefix</span><span class="p">,</span> <span class="n">key_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">new_value_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">value_prefix</span><span class="p">,</span> <span class="n">value_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="nf">return </span><span class="p">(</span><span class="n">new_key_states</span><span class="p">,</span> <span class="n">new_value_states</span><span class="p">)</span>
</code></pre></div></div> <p>To integrate this with a transformer model, we need to modify each attention layer to incorporate the prefixes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PrefixTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">prefix_module</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prefix_module</span> <span class="o">=</span> <span class="n">prefix_module</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        
        <span class="c1"># Freeze the original layer
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Extract the attention module (implementation depends on model architecture)
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span>
        
        <span class="c1"># Prepare key, query, value states as in the original attention
</span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        
        <span class="c1"># Reshape for multi-head attention
</span>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="n">head_size</span>
        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="n">num_attention_heads</span>
        
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Apply prefix
</span>        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">prefix_module</span><span class="p">((</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_idx</span><span class="p">)</span>
        
        <span class="c1"># Update attention mask for the additional prefix tokens
</span>        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prefix_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> 
                <span class="mi">1</span><span class="p">,</span> 
                <span class="mi">1</span><span class="p">,</span> 
                <span class="n">self</span><span class="p">.</span><span class="n">prefix_module</span><span class="p">.</span><span class="n">prefix_length</span><span class="p">,</span> 
                <span class="n">device</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prefix_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="bp">None</span>
        
        <span class="c1"># Calculate attention scores and outputs
</span>        <span class="c1"># (Implementation depends on the specific attention mechanism)
</span>        <span class="c1"># ...
</span>        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>The above implementation is conceptual and would need to be adapted based on the specific transformer architecture you’re working with.</p> <h3 id="prompt-tuning">Prompt Tuning</h3> <p><a href="https://arxiv.org/abs/2104.08691">Prompt Tuning</a>, can be seen as a simplified version of Prefix Tuning. Rather than adding virtual tokens at every layer, Prompt Tuning only prepends trainable embeddings to the input sequence embeddings at the first layer.</p> <p>Here’s a straightforward implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PromptTuning</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">prompt_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of Prompt Tuning.
        
        Args:
            model: The pre-trained transformer model
            prompt_length: Number of virtual tokens to add
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt_length</span> <span class="o">=</span> <span class="n">prompt_length</span>
        
        <span class="c1"># Freeze model parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Get embedding dimension from the model
</span>        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Create soft prompt embeddings
</span>        <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
        
        <span class="c1"># Initialize with embeddings of random tokens from the vocabulary
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">random_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">prompt_length</span><span class="p">,))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">[</span><span class="n">random_indices</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">attention_mask</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Get input embeddings
</span>        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">()(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="sh">"</span><span class="s">inputs_embeds</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Expand soft prompts for batch size and prepend to input embeddings
</span>        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">soft_prompts</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Adjust attention mask for the added prompt tokens
</span>        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prompt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">prompt_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Forward pass through the model without input_ids
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
</code></pre></div></div> <h2 id="bitfit-adapter">BitFit Adapter</h2> <p>BitFit, proposed by <a href="https://arxiv.org/abs/2106.10199">Zaken et al. (2021)</a>, takes a radically different approach from the methods we’ve discussed so far. Instead of adding new parameters, BitFit selectively trains only the bias terms in the pre-trained model, leaving all other parameters frozen.</p> <p>Despite its extreme parameter efficiency, BitFit has shown good performance across various tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_bitfit_to_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Apply BitFit to a transformer model by only training bias terms.
    
    Args:
        model: A PyTorch model
    </span><span class="sh">"""</span>
    <span class="c1"># First, freeze all parameters
</span>    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="c1"># Then unfreeze only bias parameters
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
    
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <p>The implementation is remarkably simple, yet BitFit can achieve competitive performance while training less than 0.1% of the original model parameters in many cases.</p> <h2 id="ia-infused-adapter-by-inhibiting-and-amplifying-inner-activations">IA³: Infused Adapter by Inhibiting and Amplifying Inner Activations</h2> <p><strong>IA³ (Input-Aware Activation Adjustment)</strong> by <a href="https://arxiv.org/abs/2205.05638">Liu et al. (2022)</a>, modifies the element-wise activation scale and bias <em>after</em> each linear transformation. For a layer with output \(x\), IA³ computes:</p> \[x' = \alpha \cdot x + \beta\] <p>Here, \(\alpha\) and \(\beta\) are trainable parameters. This is similar to fine-tuning just the scale and shift of activations and can be extremely efficient.</p> <p>IA³ is useful when slight shifts in activation distributions are enough to steer the model to the new task.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/ia3-480.webp 480w,/assets/img/posts/2025-05-11-adapters/ia3-800.webp 800w,/assets/img/posts/2025-05-11-adapters/ia3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/ia3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="IA3 Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> IA3 Adapter </div> <p>Let’s check how it looks like in the code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Implementation of IA³ scaling vectors.
        
        Args:
            hidden_size: Dimension to scale
            ia3_type: Where to apply IA³ (</span><span class="sh">'</span><span class="s">feed_forward</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">attention_output</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">attention_value</span><span class="sh">'</span><span class="s">)
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ia3_type</span> <span class="o">=</span> <span class="n">ia3_type</span>
        
        <span class="c1"># Create scaling vectors initialized to ones
</span>        <span class="k">if</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For the output of the feed-forward layer
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_output</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For scaling attention outputs
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For scaling value vectors in attention
</span>            <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Apply scaling to input tensor.
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ia3_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">:</span>
            <span class="c1"># For attention values, we reshape for broadcasting across batch and seq dimensions
</span>            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For feed-forward and attention outputs
</span>            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_vector</span>
</code></pre></div></div> <p>Integrating IA³ with a transformer model requires injecting the scaling at specific points:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IA3TransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        
        <span class="c1"># Freeze original parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Add IA³ modules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_value_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_value</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention_output_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">attention_output</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feed_forward_ia3</span> <span class="o">=</span> <span class="nc">IA3Module</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ia3_type</span><span class="o">=</span><span class="sh">"</span><span class="s">feed_forward</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Extract components (implementation is model-specific)
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span>
        
        <span class="c1"># Compute query, key, value projections
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attention</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        
        <span class="c1"># Apply IA³ to value projections
</span>        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_value_ia3</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        
        <span class="c1"># Compute attention
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Apply IA³ to attention output
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_output_ia3</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Apply IA³ to feed-forward output
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feed_forward_ia3</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>IA³’s efficiency is remarkable: for a model with hidden size h, it adds only 3h parameters per layer, compared to the millions in the original layer.</p> <h2 id="compacter-kronecker-products-for-ultimate-efficiency">Compacter: Kronecker Products for Ultimate Efficiency</h2> <p><strong>Compacter</strong>, proposed by <a href="https://arxiv.org/abs/2106.04647">Mahabadi et al. (2021)</a>, builds on the adapter idea, but instead of learning full matrices for down/up projection, it composes them from Kronecker products of smaller matrices.</p> \[W = W_1 \otimes W_2\] <p>This gives an expressive yet parameter-efficient formulation. Compacter adapters can learn more complex transformations than simple low-rank matrices without adding much overhead.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-05-11-adapters/compacter-480.webp 480w,/assets/img/posts/2025-05-11-adapters/compacter-800.webp 800w,/assets/img/posts/2025-05-11-adapters/compacter-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-05-11-adapters/compacter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Compacter Adapter" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Compacter Adapter </div> <p>Let’s implement Compacter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Parameterized Hypercomplex Multiplication using Kronecker products.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span> <span class="o">=</span> <span class="n">factorized_phm</span>
        
        <span class="c1"># Calculate dimensions for the factors
</span>        <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">in_features</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>
        
        <span class="c1"># Ensure dimensions are compatible with factorization
</span>        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span> <span class="o">==</span> <span class="n">in_features</span><span class="p">,</span> \
            <span class="sh">"</span><span class="s">Input features must be a perfect square for factorization</span><span class="sh">"</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span> <span class="o">==</span> <span class="n">out_features</span><span class="p">,</span> \
            <span class="sh">"</span><span class="s">Output features must be a perfect square for factorization</span><span class="sh">"</span>
        
        <span class="k">if</span> <span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># Factorized representation using shared factors
</span>            <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">in_factor_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">out_factor_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Full Kronecker factors
</span>            <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">))</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">rank</span><span class="p">))</span>
        
        <span class="c1"># Initialize parameters
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">_init_parameters</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">_init_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Initialize the parameters with small random values.</span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">ones_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">kronecker_product</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Compute the Kronecker product of matrices A and B.
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">A</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">s3</span><span class="p">,</span> <span class="n">s4</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">B</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Reshape for matrix multiplication
</span>        <span class="n">A_reshaped</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span> <span class="o">*</span> <span class="n">s2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">B_reshaped</span> <span class="o">=</span> <span class="n">B</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s3</span> <span class="o">*</span> <span class="n">s4</span><span class="p">)</span>
        
        <span class="c1"># Perform outer product
</span>        <span class="n">kron_prod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">A_reshaped</span><span class="p">,</span> <span class="n">B_reshaped</span><span class="p">)</span>
        
        <span class="c1"># Reshape to get the final Kronecker product
</span>        <span class="k">return</span> <span class="n">kron_prod</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">s3</span><span class="p">,</span> <span class="n">s4</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">s1</span> <span class="o">*</span> <span class="n">s3</span><span class="p">,</span> <span class="n">s2</span> <span class="o">*</span> <span class="n">s4</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass using PHM.
        x: Input tensor of shape [batch_size, in_features]
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Compute the weight matrix using PHM
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># Using factorized representation
</span>            <span class="n">weight</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rank</span><span class="p">):</span>
                <span class="c1"># Apply scaling factor
</span>                <span class="n">kronecker_factor</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">kronecker_product</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">B</span><span class="p">[</span><span class="n">r</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">weight</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">[</span><span class="n">r</span><span class="p">]</span> <span class="o">*</span> <span class="n">kronecker_factor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Using full representation
</span>            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">s</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">rank</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Apply the weight matrix - handle factorized vs full differently
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">factorized_phm</span><span class="p">:</span>
            <span class="c1"># For factorized version, we already have batch-specific weights
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">weight</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For the full version, we use simple matrix multiply
</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">weight</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Compacter adapter implementation using PHM for weight parameterization.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adapter_size</span> <span class="o">=</span> <span class="n">adapter_size</span>
        
        <span class="c1"># Down projection using PHM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="n">factorized_phm</span><span class="p">)</span>
        
        <span class="c1"># Activation function
</span>        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        
        <span class="c1"># Up projection using PHM
</span>        <span class="n">self</span><span class="p">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="nc">PHM</span><span class="p">(</span><span class="n">adapter_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">factorized_phm</span><span class="o">=</span><span class="n">factorized_phm</span><span class="p">)</span>
        
        <span class="c1"># Additional components
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        
        <span class="c1"># Scaling factor for the adapter output
</span>        <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Forward pass through the Compacter adapter.
        </span><span class="sh">"""</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        
        <span class="c1"># Down projection with PHM
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Up projection with PHM
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Apply scaling and add residual
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># Integrating Compacter with a transformer layer would be similar to the adapter implementation
</span><span class="k">class</span> <span class="nc">CompacterTransformerLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">transformer_layer</span><span class="p">,</span> <span class="n">adapter_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">transformer_layer</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">transformer_layer</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">self</span><span class="p">.</span><span class="n">all_head_size</span>  <span class="c1"># Model specific
</span>        
        <span class="c1"># Freeze original parameters
</span>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="c1"># Add Compacter adapters
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attention_adapter</span> <span class="o">=</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn_adapter</span> <span class="o">=</span> <span class="nc">CompacterLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">adapter_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Original attention mechanism
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Apply attention adapter
</span>        <span class="n">adapted_attention</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention_adapter</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        
        <span class="c1"># Original feed-forward network
</span>        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">intermediate</span><span class="p">(</span><span class="n">adapted_attention</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">adapted_attention</span><span class="p">)</span>
        
        <span class="c1"># Apply ffn adapter
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn_adapter</span><span class="p">(</span><span class="n">layer_output</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine-learning"/><category term="machine-learning,"/><category term="transformers,"/><category term="peft,"/><category term="lora,"/><category term="adapters,"/><category term="fine-tuning"/><summary type="html"><![CDATA[Exploration of modern parameter-efficient fine-tuning techniques with PyTorch implementations and practical insights.]]></summary></entry><entry><title type="html">Physical Symbol Systems and the Language of Thought</title><link href="https://xmarva.github.io/blog/2025/minds-as-computers/" rel="alternate" type="text/html" title="Physical Symbol Systems and the Language of Thought"/><published>2025-05-01T15:00:00+00:00</published><updated>2025-05-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/minds-as-computers</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/minds-as-computers/"><![CDATA[<h2 id="the-physical-symbol-system-hypothesis">The Physical Symbol System Hypothesis</h2> <p>Imagine your brain as a sophisticated computer processing symbols. Not metaphorically, but literally—manipulating physical structures that represent the world around you. This is the essence of the <em>physical symbol system hypothesis</em>, an influential theory proposed by cognitive scientists Allen Newell and Herbert Simon as a fundamental framework for understanding intelligence.</p> <p>Much like how biologists rely on the cell doctrine or geologists on plate tectonics, cognitive scientists have used this hypothesis as their north star. It’s a starting point that frames how we think about thinking itself. Newell and Simon’s hypothesis states that “a physical symbol system has the necessary and sufficient means for general intelligent action.” In other words, if you want intelligence, you need a system that can process symbols—and if you have a system that can process symbols properly, you’ll get intelligence.</p> <p>But what exactly constitutes a physical symbol system? According to Newell and Simon, such a system needs:</p> <ol> <li>Symbols that can be physically instantiated</li> <li>Symbol structures composed of these basic symbols</li> <li>Processes for manipulating these symbols and structures</li> <li>The ability to interpret and produce new symbols</li> </ol> <p>Consider a simple example: the problem of getting foxes, chickens, and grain across a river in a boat that can only carry one item at a time (without leaving foxes alone with chickens or chickens alone with grain). To solve this puzzle, you need to mentally represent the objects and their relationships, consider possible moves, and evaluate potential outcomes. According to the physical symbol system hypothesis, your brain accomplishes this by physically manipulating symbols that represent foxes, chickens, boats, and so on.</p> <p>At its core, this hypothesis is about taking information, encoding it into symbols, and transforming those symbols according to specific rules. That spreadsheet calculation you just ran? That’s a simple example of symbol manipulation. Your morning deliberation about whether to have coffee or tea? According to this hypothesis, that too is symbol manipulation—just happening in the wetware of your brain rather than silicon.</p> <h2 id="the-language-of-thought-hypothesis">The Language of Thought Hypothesis</h2> <p>Philosopher and cognitive scientist Jerry Fodor took the physical symbol system hypothesis further with his <em>language of thought hypothesis</em>. His proposal is both elegant and radical: we think in sentences—not English or Mandarin sentences, but sentences in a special mental language he sometimes called “Mentalese.”</p> <p>What makes this mental language special? Unlike natural languages with their ambiguities and inconsistencies, the language of thought is supposed to be precise and logical—more like the formal languages used in mathematics and logic than the messy languages we speak. This language doesn’t need to be learned; Fodor argues it’s innate, built into our cognitive architecture.</p> <p>Fodor’s argument begins with an observation about how we explain human behavior. We routinely explain and predict what people do by attributing beliefs and desires to them. When I say you rushed into the water because you believed someone was drowning and wanted to save them, I’m describing internal mental states that caused your behavior. Fodor claims this “belief-desire psychology” works so well because it’s actually true—we really do have beliefs and desires that cause our actions.</p> <p>But this raises a puzzle: how can mental states cause physical actions? How can the <em>content</em> or meaning of your beliefs affect anything physical? If I believe “the door is open” and desire “to close the door,” how does the meaning of these thoughts actually move my muscles?</p> <p>This is what philosophers call the problem of “causation by content,” and it’s at the heart of Fodor’s language of thought hypothesis.</p> <h2 id="syntax-semantics-and-the-computer-model">Syntax, Semantics, and the Computer Model</h2> <p>Fodor’s solution draws on an analogy with computers. When you type “2+3” into a calculator, the machine doesn’t understand addition or numbers. It simply follows rules for manipulating symbols. Yet somehow, it consistently produces “5” as an output.</p> <p>This works because computers are designed to manipulate symbols based on their “formal properties” (their shape or syntax) in ways that respect their “semantic properties” (their meaning). The computer doesn’t know what “2” means, but it’s programmed to manipulate the symbol “2” according to rules that consistently produce the right results.</p> <p>Fodor argues that our brains work in a similar way. Sentences in the language of thought are physical symbol structures that can be viewed either syntactically (in terms of their physical form) or semantically (in terms of what they represent). The brain processes these structures according to their syntax, while remaining blind to their semantics. Yet because the system is properly designed, these syntactic manipulations respect semantic relationships.</p> <p>Consider a more detailed example:</p> <ol> <li>We start with two basic symbols in our language of thought: “Ga” (meaning “Georgina is tall”) and “Fa” (meaning “Georgina has red hair”).</li> <li>Our brain contains rules for transforming these symbols. One rule might be: if you have two symbols “S” and “T,” you can form a new symbol “(S &amp; T).”</li> <li>Applying this rule to our initial symbols gives us “(Ga &amp; Fa)” (meaning “Georgina is tall and has red hair”).</li> <li>Another rule might be: if you have a symbol containing a name, you can replace the name with a variable “x” and add “∃x” (meaning “there exists an x such that…”) at the beginning.</li> <li>Applying this to “(Ga &amp; Fa)” gives us “∃x (Gx &amp; Fx)” (meaning “There exists someone who is tall and has red hair”).</li> </ol> <p>According to Fodor, your brain processes these as physical symbol structures, applying transformation rules without directly “seeing” their meaning. Yet the transformations reliably preserve truth, allowing you to draw valid conclusions about the world. This is possible because the language of thought is a formal system where syntax tracks semantics, much like in formal logic.</p> <p>Fodor likens this to how in formal logic, syntactic deducibility (having a formal proof) corresponds to semantic entailment (truth preservation). His key insight is that this correspondence allows purely physical systems to implement what appears to be reasoning about meanings.</p> <h2 id="the-chinese-room-argument">The Chinese Room Argument</h2> <p>But can symbol manipulation alone really produce genuine understanding and intelligence? Philosopher John Searle famously challenged this idea with his “Chinese Room” thought experiment.</p> <p>Imagine yourself locked in a room with nothing but an enormous instruction manual written in English. Through one window, you receive papers with Chinese symbols. You consult your manual, which tells you which Chinese symbols to send back through another window based solely on the shapes of the symbols you received. Despite knowing no Chinese whatsoever, you could theoretically provide appropriate responses to any Chinese question.</p> <p>To outside observers, the room appears to understand Chinese perfectly—it passes what Alan Turing proposed as the “Turing Test” for machine intelligence. If you ask a question in Chinese, you get an appropriate answer in Chinese. Yet you, inside the room, understand nothing about the meaning of these symbols. You’re just following rules for matching patterns.</p> <p>Searle’s argument cuts to the heart of computational theories of mind: if you don’t understand Chinese despite implementing the program, how could a computer understand anything by implementing essentially the same program? The Chinese Room seems to satisfy the conditions of the physical symbol system hypothesis—it manipulates symbols according to rules—yet it lacks understanding. Therefore, Searle concludes, the physical symbol system hypothesis must be wrong.</p> <p>Searle’s challenge is particularly pointed because it grants the physical symbol system hypothesis everything it seems to want. The Chinese Room manipulates symbols perfectly—it gives all the right outputs for the inputs it receives. It’s just that this manipulation doesn’t seem to produce understanding. The person in the room is just “pushing symbols around” without comprehending what they mean.</p> <h2 id="responses-to-the-chinese-room">Responses to the Chinese Room</h2> <p>Defenders of computational approaches have offered various responses to Searle’s challenge:</p> <p>The <strong>systems reply</strong> argues that while you as the person in the room don’t understand Chinese, the system as a whole (you plus the instruction manual plus the room) does understand. Understanding emerges at the system level, not the component level. This is analogous to how neurons individually don’t understand language, yet the brain as a whole does. The individual components of a system need not possess the properties of the system itself.</p> <p>Searle counters this by suggesting we internalize the entire system—imagine memorizing the entire instruction manual so that it’s all in your head. You’re now the entire system, yet you still don’t understand Chinese. You’re just following memorized rules for symbol manipulation.</p> <p>The <strong>robot reply</strong> suggests that understanding requires embodiment and causal connections to the world. A Chinese Room embedded in a robot that can interact with the world—seeing Chinese characters, handling Chinese objects, speaking with Chinese speakers—would genuinely understand Chinese through these grounded interactions.</p> <p>Searle remains unconvinced. He argues that adding sensors and motors doesn’t bridge the fundamental gap between syntax and semantics. A robot could be programmed to stop when it “sees” the Chinese character for “stop,” but this wouldn’t mean it understands what “stop” means any more than a trained pigeon understands a stop sign.</p> <p>Some philosophers and cognitive scientists have proposed other responses. The <strong>biological reply</strong> suggests that perhaps understanding requires specific biological processes that non-biological systems cannot replicate. The <strong>learning reply</strong> argues that a system that developed its symbol-processing capacities through learning (rather than being hand-programmed) might genuinely understand.</p> <h2 id="the-symbol-grounding-problem">The Symbol-Grounding Problem</h2> <p>This debate leads us to what cognitive scientists call the <em>symbol-grounding problem</em>: How do symbols become meaningful in the first place? How does the symbol “cat” connect to actual cats in the world?</p> <p>In a computer program, symbols get their meaning from the programmers who create them. The binary code “01100011 01100001 01110100” represents “cat” because programmers decided it should. But if our minds are symbol systems, where do our symbols get their meaning? There’s no programmer assigning meanings to our mental symbols.</p> <p>One approach, associated with philosophers like Fred Dretske and Ruth Millikan, suggests that meaning comes from causal connections between symbols and the world. Your mental symbol for “cat” means cat because it’s reliably caused by encounters with cats. Another approach, championed by philosopher Wilfrid Sellars, argues that symbols get their meaning from their role in a network of inferential relationships with other symbols.</p> <p>The symbol-grounding problem isn’t unique to artificial systems—it’s equally mysterious how our own thoughts connect to reality. When you think about cats, what exactly makes that thought <em>about</em> cats rather than dogs or airplanes? This problem of “intentionality” or “aboutness” has puzzled philosophers for centuries.</p> <h2 id="implications-for-artificial-intelligence">Implications for Artificial Intelligence</h2> <p>The debate between Searle and proponents of the physical symbol system hypothesis has profound implications for artificial intelligence. If Searle is right, then no amount of symbol manipulation, no matter how sophisticated, will ever produce genuine understanding or consciousness. AI systems might simulate intelligence, but they would be what Searle calls “weak AI”—useful tools that mimic intelligence without possessing it.</p> <p>On the other hand, if the physical symbol system hypothesis is correct, then there is no in-principle barrier to creating “strong AI”—artificial systems that genuinely understand and are conscious in the same way humans are. The challenge would be merely technical: figuring out the right symbols and rules for manipulating them.</p> <p>Recent advances in AI, particularly in deep learning and neural networks, have complicated this picture. Modern AI systems like large language models don’t explicitly manipulate symbols according to explicit rules. Instead, they learn statistical patterns from vast amounts of data. Some researchers argue this approach might sidestep Searle’s objections by grounding symbols in patterns of use rather than explicit rules.</p> <h2 id="beyond-symbol-systems">Beyond Symbol Systems?</h2> <p>The physical symbol system hypothesis and the language of thought hypothesis remain influential in cognitive science, but they’re not the only approaches. Alternative frameworks include:</p> <p><strong>Connectionism</strong>: This approach models cognition as emerging from networks of simple processing units (artificial neurons) rather than explicit symbol manipulation. Connectionists argue that intelligence emerges from the patterns of activation across these networks, not from rule-based symbol processing.</p> <p><strong>Embodied Cognition</strong>: This view holds that cognition is fundamentally shaped by the body’s interactions with the environment. Rather than abstract symbol manipulation happening in a disembodied mind, thinking is grounded in sensorimotor experiences.</p> <p><strong>Predictive Processing</strong>: This newer framework suggests the brain is fundamentally a prediction machine, constantly generating and updating predictions about sensory inputs. Intelligence emerges from the process of minimizing prediction errors rather than manipulating symbols.</p> <p>These alternative approaches don’t necessarily contradict the insights of the physical symbol system hypothesis entirely. They might be seen as offering different levels of explanation or as complementary perspectives on the complex phenomenon of intelligence.</p> <p>The debate continues, and with it, our understanding of what it means to think, to understand, and to be intelligent grows deeper and more nuanced. Perhaps the most fascinating aspect of cognitive science is that in studying minds, we are studying ourselves—turning the very tools we’re investigating back upon the investigators. Whether computational approaches can solve the fundamental challenges posed by Searle and others remains an open question at the frontier of cognitive science and philosophy of mind.</p>]]></content><author><name></name></author><category term="cognitive-science"/><category term="cognitive-science,"/><category term="philosophy-of-mind,"/><category term="symbolic-systems,"/><category term="language-of-thought"/><summary type="html"><![CDATA[Exploration of the physical hypothesis of symbolic systems, the language of thought hypothesis, and philosophical challenges]]></summary></entry><entry><title type="html">Building a Transformer (Cross-Attention and MHA Explained)</title><link href="https://xmarva.github.io/blog/2025/building-a-transformer/" rel="alternate" type="text/html" title="Building a Transformer (Cross-Attention and MHA Explained)"/><published>2025-04-16T15:00:00+00:00</published><updated>2025-04-16T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/building-a-transformer</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/building-a-transformer/"><![CDATA[<p><a href="https://www.kaggle.com/code/qmarva/implementing-transformer-en"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=plastic&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"/></a> <a href="https://colab.research.google.com/drive/1m34XYFZZTt-jbHo2OXlUfxR33zvFbXJZ?usp=sharing"><img src="https://img.shields.io/badge/Colab-F9AB00?style=plastic&amp;logo=google-colab&amp;logoColor=white" alt="Colab"/></a></p> <h2 id="building-a-transformer">Building a Transformer</h2> <p>The Transformer architecture marked a revolutionary step in sequence processing. Unlike traditional models such as RNNs and LSTMs, which handle data sequentially, the Transformer uses an attention mechanism that enables parallel processing of the entire sequence. This significantly accelerates training and improves performance.</p> <p>The key innovation of the Transformer is the use of <strong>self-attention</strong>, which allows the model to effectively take into account the context of each word or token, regardless of its position in the sequence. This architecture has become the foundation of many modern models, including BERT, GPT, and others, and has greatly improved the quality of solutions in the field of natural language processing.</p> <p>We’ll need to install several libraries (and import even more), and the number of dependencies will only grow as we move forward. You can ignore these setup cells and just run them — the main focus should be on the core code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">torchdata</span><span class="o">==</span><span class="mf">0.3</span><span class="p">.</span><span class="mi">0</span> <span class="n">torchtext</span><span class="o">==</span><span class="mf">0.12</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2</span> <span class="n">altair</span><span class="o">==</span><span class="mf">5.5</span><span class="p">.</span><span class="mi">0</span> <span class="n">GPUtil</span><span class="o">==</span><span class="mf">1.4</span><span class="p">.</span><span class="mi">0</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">de_core_news_sm</span>
<span class="err">!</span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">en_core_web_sm</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">GPUtil</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="n">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="kn">import</span> <span class="n">altair</span> <span class="k">as</span> <span class="n">alt</span>
<span class="kn">from</span> <span class="n">altair</span> <span class="kn">import</span> <span class="n">Chart</span>

<span class="n">alt</span><span class="p">.</span><span class="n">data_transformers</span><span class="p">.</span><span class="nf">disable_max_rows</span><span class="p">()</span>
</code></pre></div></div> <h2 id="positional-encoding">Positional Encoding</h2> <p>Transformer models work with numbers. To process text, it must be converted into a numerical format that the model can understand and work with. The first step is to convert text into tokens — for more details, see the notebook on tokenization. Tokens represented as vectors are called <strong>embeddings</strong>.</p> <p><strong>Embeddings</strong> are numerical vectors that capture the semantic meaning of words or subwords.</p> <p>Before the Transformer architecture, sequence-processing models like RNNs and LSTMs handled data sequentially, inherently preserving the order of elements. However, their computational inefficiency due to step-by-step processing and poor parallelization led researchers to seek alternatives.</p> <p>The Transformer overcame these limitations with a fully parallel approach. However, without positional information, the model wouldn’t be able to distinguish between sentences with the same words in different orders. In Russian, word relationships are often expressed via case endings, but in English and many other languages, word order is crucial.</p> <p><strong>Positional Encoding</strong> is the mechanism in the Transformer architecture that enables the model to account for the order of words in a sequence.</p> <p>Positional Encoding adds special signals (positional encodings) to the token embeddings based on their position in the sequence. These encodings have the same dimensionality as the embeddings so that they can be summed together. This additional information allows the model to distinguish, for instance, between the word “cat” at position 1 and “cat” at position 5, even if their semantic embeddings are identical. The encoding uses a formula that combines sine and cosine functions at different frequencies:</p> \[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)\] <p>where \(pos\) is the position, \(d_{\text{model}}\) is the embedding dimensionality, and \(i\) is the index of the vector dimension.</p> <p>The core idea is that these sinusoidal functions allow the model to pay attention to <strong>relative positions</strong>.</p> <h3 id="why-does-this-strange-formula-encode-relative-positions">Why does this strange formula encode relative positions?</h3> <h4 id="first-the-model-can-generalize-to-sequences-longer-than-those-seen-during-training">First, the model can generalize to sequences longer than those seen during training.</h4> <p>Imagine that each position in a sequence is a point on a number line. If we generate signals for position \(pos\) using sine and cosine, then the signals for position \(pos + k\) can be expressed as a combination of the original values. For example, using the angle addition formula:</p> \[\sin(pos + k) = \sin(pos)\cos(k) + \cos(pos)\sin(k)\] <p>A shift of \(k\) positions can be expressed as a weighted sum of the original sine and cosine values. This allows the model to infer that a word “three positions later” is related to the original word, even if it never saw such a long sequence during training.</p> <h4 id="second-the-distance-between-any-two-time-steps-is-consistent-across-the-sequence">Second, the distance between any two time steps is consistent across the sequence.</h4> <p>The logarithmic decay of frequencies in the term \(10000^{2i/d_{\text{model}}}\) ensures that different dimensions of the positional vector capture different levels of positional detail. For small \(i\) (early vector components), the denominator becomes large, causing the sine and cosine arguments to grow slowly with \(pos\). This creates low-frequency oscillations that help distinguish between distant positions — for example, the beginning of the text (positions 1–100) versus the middle (positions 101–200). For larger \(i\), the denominator shrinks, the argument grows faster, and high-frequency oscillations emerge, encoding fine-grained differences between neighboring positions (e.g., 101 and 102).</p> <h4 id="third-this-formula-yields-unique-encodings-for-each-position">Third, this formula yields unique encodings for each position.</h4> <p>Alternating sine and cosine for even and odd indices solves the uniqueness issue. If we used only sine, different positions might accidentally match due to the periodicity of the function (e.g., \(\sin(pos)\) and \(\sin(pos + 2\pi)\)). Adding cosine for neighboring vector components eliminates this symmetry: the combination of \(\sin(f(pos))\) and \(\cos(f(pos))\) across different frequencies \(f\) ensures that each position \(pos\) has a unique vector. The orthogonality of sine and cosine (their dot product is close to zero) minimizes overlap with word embeddings, allowing the model to separately process semantics and position.</p> <hr/> <p>The sum \(\text{Embedding} + PE\) is possible because word embeddings and positional encodings have the same dimensionality \(d_{\text{model}}\). This addition requires no trainable parameters: the model receives a combined signal where the word’s semantics are modulated by its position. Gradients flow through this operation without distortion, as the derivative of a sum is the sum of the derivatives. As a result, during training, the model automatically learns to adjust both the semantic embeddings and the use of positional information (via attention), without conflicting signals.</p> <p>While it’s also possible to use <strong>learned positional embeddings</strong>, the sinusoidal version was chosen in the original paper because it enables the model to extrapolate to sequence lengths not seen during training. Experiments have shown that both versions yield nearly identical results.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span>
                             <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)].</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h2 id="attention-self-attention-multi-head-attention">Attention, Self-Attention, Multi-Head Attention</h2> <h3 id="attention">Attention</h3> <p><strong>Attention</strong> is a mechanism that allows a model to weigh the importance of different elements in the input sequence.</p> <p>It can be described as a function that takes a <strong>query</strong> and a set of <strong>key-value</strong> pairs, and produces an output — a weighted sum of the values. The weight assigned to each value is computed based on a compatibility function between the query and the corresponding key.</p> <p>Imagine you’re at a table with 50 experts. At the start, none of them knows anything about themselves or each other, but their goal during the meeting is to figure out:</p> <ul> <li>\(V\): what they themselves know (their <strong>Value</strong> — knowledge/opinion),</li> <li>\(K\): the best way to describe what they’re good at (their <strong>Key</strong>),</li> <li>\(Q\): the best way to express what information they’re looking for (their <strong>Query</strong>).</li> </ul> <p>If we used only \(Q\) and \(K\), the model wouldn’t be able to transform the discovered dependencies into new features. The matrix \(V\) adds flexibility, allowing the model to reweight values according to context.</p> <p>Let’s say you’re one of those experts. You have a question (query), such as:</p> <blockquote> <p>“I need an opinion on Japanese cars.”</p> </blockquote> <p>You look around. Each expert has published a short description (key), for example:</p> <ul> <li>“I’m a mechanic specializing in Japanese cars”</li> <li>“I’m a chef who knows Italian cuisine”</li> <li>“I’m a driver who owned a Subaru”</li> </ul> <p>You compare your query against the keys of the others. If someone’s key matches well, you pay more attention to their value (opinion). You’ll likely give the most weight to the mechanic and less to the driver.</p> <p>As training progresses, you refine your query. Maybe next time, you realize you’re not interested in Japanese cars, but in <strong>Italian sewing machines</strong>. And it turns out the chef, initially thinking they specialize in Italian food, actually knows sewing machines well.</p> <p>So, you update the attention weights accordingly and learn to listen to the right expert.</p> <hr/> <h3 id="self-attention"><strong>Self-Attention</strong></h3> <p><strong>Self-attention</strong> is a type of attention mechanism used in Transformers where the queries, keys, and values come from the same sequence. The original Transformer uses <strong>Scaled Dot-Product Attention</strong>, which works as follows:</p> <ol> <li><strong>Create query, key, and value vectors</strong>. For each input vector \(x\) (e.g., a word embedding), three vectors are computed:</li> </ol> \[Q = xW_q,\quad K = xW_k,\quad V = xW_v\] <p>Here, \(W_q\), \(W_k\), and \(W_v\) are trainable weight matrices. The dimensions of \(Q\) and \(K\) must match: \(d_k\).</p> <ol> <li><strong>Compute scores.</strong> For each query vector \(Q_i\) (corresponding to position \(i\)), scores are computed with all keys \(K_j\) using the dot product:</li> </ol> \[\text{score}(i, j) = Q_i \cdot K_j^T\] <ol> <li><strong>Scale the scores.</strong> To prevent large dot product values with high dimensions, the scores are divided by \(\sqrt{d_k}\):</li> </ol> \[\text{scaled\_score}(i, j) = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}\] <ol> <li><strong>Apply Softmax.</strong> Each row of scores is passed through Softmax for normalization:</li> </ol> \[\alpha_{ij} = \text{softmax}\left( \frac{Q_i \cdot K_j^T}{\sqrt{d_k}} \right)\] <p>The resulting \(\alpha_{ij}\) are the attention weights.</p> <ol> <li><strong>Compute the weighted sum of values.</strong> Each value vector \(V_j\) is multiplied by the attention weight \(\alpha_{ij}\) and aggregated:</li> </ol> \[\text{Attention}(Q_i, K, V) = \sum_j \alpha_{ij} V_j\] <ol> <li><strong>Form the output vector.</strong> The result is a vector containing contextual information relevant to position \(i\). For the entire sequence:</li> </ol> \[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V\] <p>In practice, all computations are done in parallel using matrix operations, making the mechanism efficient and scalable.</p> <hr/> <h3 id="multi-head-attention"><strong>Multi-Head Attention</strong></h3> <p><strong>Multi-Head Attention</strong> is an extension of self-attention. While single-head attention focuses on one type of dependency (e.g., syntax or semantics), multi-head attention enables the model to capture multiple aspects of context simultaneously: grammatical relationships, anaphora, semantic parallels, etc.</p> <p>Let the input be an embedding matrix \(X \in \mathbb{R}^{n \times d_{\text{model}}}\), where \(n\) is the sequence length and \(d_{\text{model}}\) is the embedding dimension.</p> <p>The idea remains the same: for each attention head \(h \in {1, \dots, H}\), the input \(X\) is projected into queries, keys, and values via trainable matrices:</p> \[Q_h = X W_h^Q,\quad K_h = X W_h^K,\quad V_h = X W_h^V\] <p>Typically, \(d_k = d_v = \frac{d_{\text{model}}}{H}\) so that concatenating all heads results in the original dimension \(d_{\text{model}}\).</p> <p>Each head performs standard attention:</p> \[\text{Attention}_h(Q_h, K_h, V_h) = \text{softmax}\left( \frac{Q_h K_h^T}{\sqrt{d_k}} \right) V_h\] <p>The outputs from all \(H\) heads are concatenated along the last dimension:</p> \[\text{Concat}( \text{head}_1, \dots, \text{head}_H ) \in \mathbb{R}^{n \times (H \cdot d_v)}\] <p>This combined output is projected back into \(d_{\text{model}}\) using a final linear layer:</p> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_H) W^O\] <p>where \(W^O \in \mathbb{R}^{(H \cdot d_v) \times d_{\text{model}}}\) is a trainable weight matrix of the final output projection layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">d_model</span><span class="p">,</span> <span class="sh">"</span><span class="s">d_model must be divisible by num_heads</span><span class="sh">"</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Q: [batch_size, num_heads, seq_len, head_dim]
</span>        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="n">attn_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_probs</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_k</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_v</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="feedforward">FeedForward</h2> <p>Imagine that after passing through the Multi-Head Attention mechanism, the information for each word or token has become richer and more contextualized. Attention has blended information from different tokens to better understand each one in the context of the sentence. But now, this enriched information needs to be <strong>processed and refined individually</strong> for each token.</p> <p>That’s the role of the <strong>FeedForward Network (FFN)</strong>, which comes after the attention layer in each encoder and decoder block.</p> <p>An FFN consists of two linear transformations. Between these two linear layers, there’s a non-linear activation function — usually <strong>ReLU</strong>. Simply put, it’s a small two-layer neural network.</p> <p>One of the key features of the FFN in the Transformer is that it is <strong>applied position-wise</strong>. This means the <em>same</em> feedforward network is applied <strong>independently</strong> to the representation of <strong>each token</strong> in the sequence.</p> <p>The dimension of the inner layer in the FFN is typically <strong>larger</strong> than the model dimension (\(d_{\text{model}}\)). In the original <em>“Attention Is All You Need”</em> paper, this inner dimension (\(d_{\text{ff}}\)) was <strong>four times larger</strong> than \(d_{\text{model}}\) — that is, 2048 vs. 512 for the base model. However, other ratios may be used, such as doubling the size.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="encoderlayer">EncoderLayer</h2> <p>Most competitive sequence transformation models follow an <strong>encoder-decoder</strong> structure.</p> <p>The <strong>encoder</strong> receives the input and builds its representation (i.e., its features).</p> <p>The encoder is composed of a stack of <strong>identical layers</strong>. The original paper uses a stack of <strong>6 such layers</strong>, though the number can vary. Each encoder layer consists of <strong>two sub-layers</strong>:</p> <ol> <li>A <strong>Multi-Head Self-Attention</strong> mechanism</li> <li>A <strong>Feed-Forward Network (FFN)</strong> — which we discussed earlier</li> </ol> <p>Each of these sub-layers is wrapped in a <strong>residual connection</strong>, followed by a <strong>layer normalization</strong> step.</p> <p>Residual connections help ensure smooth gradient flow when training very deep models and preserve information from the original input sequence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Self attention
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="decoderlayer">DecoderLayer</h2> <p>The <strong>decoder</strong> uses the encoder’s embeddings along with other inputs to generate the <strong>target sequence</strong>.</p> <p>Like the encoder, the decoder is composed of a <strong>stack of identical layers</strong>, typically matching the encoder in depth.</p> <p>In addition to the two sub-layers found in the encoder (Multi-Head Self-Attention and Feed-Forward Network), each decoder layer includes a <strong>third sub-layer</strong>: <strong>Encoder-Decoder Attention</strong>. This allows the decoder to focus on relevant parts of the input sequence — that is, the encoder’s output.</p> <p>The self-attention sub-layer in the decoder is <strong>modified</strong> to prevent attending to <strong>future positions</strong>. This is implemented by <strong>masking</strong> — setting the scores corresponding to illegal connections in the Softmax input to \(-\infty\). This ensures that predictions for position \(i\) depend only on known outputs at positions less than \(i\).</p> <p>As in the encoder, <strong>residual connections</strong> and <strong>layer normalization</strong> are applied around each sub-layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
        <span class="c1"># Self attention (маскированное)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Cross attention (с выходом энкодера)
</span>        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
        
        <span class="c1"># Feed forward
</span>        <span class="n">ffn_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">ffn_output</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="transformer">Transformer</h2> <p>Once all the components of the Transformer — the encoder, decoder, attention mechanisms, and positional encodings — are implemented, the final step is to combine them into a single model that can be trained on sequence pairs (e.g., source text and its translation).</p> <p>The <strong>encoder</strong> and <strong>decoder</strong> are each constructed as a <strong>stack of <code class="language-plaintext highlighter-rouge">num_layers</code> layers</strong>. Each <code class="language-plaintext highlighter-rouge">EncoderLayer</code> in the encoder sequentially refines the input representations: self-attention captures global dependencies, the feed-forward network introduces non-linearity, and residual connections with layer normalization ensure stability.</p> <p>Similarly, each <code class="language-plaintext highlighter-rouge">DecoderLayer</code> applies masked self-attention, cross-attention to the encoder output, and a feed-forward network. Repeating these layers multiple times allows the model to iteratively refine representations — as if it is “re-reading” the data at different levels of abstraction.</p> <p>The <strong>final output layer</strong> <code class="language-plaintext highlighter-rouge">fc_out</code> projects from the model dimension \(d_{\text{model}}\) to the size of the target language vocabulary. This projection interprets the decoder’s output vectors as <strong>logits</strong> — unnormalized scores for each token in the vocabulary:</p> \[\text{output} = W_{\text{out}} \cdot \text{dec\_output} + b_{\text{out}}\] <p>A Softmax (not explicitly shown in code, but implied in the loss function) is applied to these logits to produce a probability distribution over the vocabulary, from which the next word is selected.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">fc_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">src_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">encoder_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">src_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
        
        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">decoder_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">tgt_emb</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_out</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h2 id="testing-transformer-just-run-it">Testing Transformer (just run it)</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_transformer</span><span class="p">():</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">src_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># Generate synthetic data
</span>    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
    
    <span class="c1"># Generate masks (example)
</span>    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># No masking
</span>    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)).</span><span class="nf">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Causal mask
</span>
    <span class="c1"># Initialize the model
</span>    <span class="n">transformer</span> <span class="o">=</span> <span class="nc">Transformer</span><span class="p">(</span>
        <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">src_vocab_size</span><span class="p">,</span>
        <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">tgt_vocab_size</span><span class="p">,</span>
        <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span>
    <span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">1. Positional Encoding Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Before PE: mean=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x_pe</span> <span class="o">=</span> <span class="nf">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">After PE: mean=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, std=</span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="nf">std</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PE Shape: </span><span class="si">{</span><span class="n">x_pe</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should be [1, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s">])</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">2. Multi-Head Attention Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">mha</span> <span class="o">=</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">k</span> <span class="o">=</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="nf">mha</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Attention output shape: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min value: </span><span class="si">{</span><span class="n">attn_output</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">3. Encoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">encoder_layer</span> <span class="o">=</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">enc_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">enc_output</span> <span class="o">=</span> <span class="nf">encoder_layer</span><span class="p">(</span><span class="n">enc_input</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder output shape: </span><span class="si">{</span><span class="n">enc_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">enc_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Data changed: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">enc_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span><span class="si">}</span><span class="s"> (should be False)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">4. Decoder Layer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">decoder_layer</span> <span class="o">=</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="n">dec_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">dec_output</span> <span class="o">=</span> <span class="nf">decoder_layer</span><span class="p">(</span><span class="n">dec_input</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder output shape: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (should match </span><span class="si">{</span><span class="n">dec_input</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Output norm: </span><span class="si">{</span><span class="n">dec_output</span><span class="p">.</span><span class="nf">norm</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">5. Full Transformer Test</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input data:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">src: </span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">src</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tgt: </span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (max=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">, min=</span><span class="si">{</span><span class="n">tgt</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nf">transformer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Output shape check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Expected shape: (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">tgt_vocab_size</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Actual shape:   </span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Gradient check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">dummy_loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="n">dummy_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">has_gradients</span> <span class="o">=</span> <span class="nf">any</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Gradients computed: </span><span class="si">{</span><span class="n">has_gradients</span><span class="si">}</span><span class="s"> (should be True)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">6. Model Parameters Check:</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Encoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">encoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoder embedding params: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">transformer</span><span class="p">.</span><span class="n">decoder_embedding</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Test completed!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine-learning"/><category term="machine-learning,"/><category term="transformers,"/><category term="multihead-attention,"/><category term="positional-encoding,"/><category term="nlp"/><summary type="html"><![CDATA[Learn and implement the most iconic architecture in modern deep learning.]]></summary></entry><entry><title type="html">Understanding Byte-Pair Encoding Algorithm</title><link href="https://xmarva.github.io/blog/2025/tokenization/" rel="alternate" type="text/html" title="Understanding Byte-Pair Encoding Algorithm"/><published>2025-04-01T15:00:00+00:00</published><updated>2025-04-01T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/tokenization</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/tokenization/"><![CDATA[<h2 id="tokenization">Tokenization</h2> <p><a href="https://www.kaggle.com/code/qmarva/1-bpe-tokenization-algorithm-eng?scriptVersionId=231677033"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=plastic&amp;logo=kaggle&amp;logoColor=white" alt="Kaggle"/></a> <a href="https://colab.research.google.com/drive/1lmfuMdC8v-lXL_MuyC0uBewdLLCTQzCO?usp=sharing"><img src="https://img.shields.io/badge/Colab-F9AB00?style=plastic&amp;logo=google-colab&amp;logoColor=white" alt="Colab"/></a></p> <p>Tokenization is a fundamental stage in natural language processing, the task of which is to split text into meaningful units (tokens).</p> <p>These units can be words, parts of words, or even characters. Historically, simple methods were used: splitting by spaces, regular expressions for extracting words and punctuation, manual rules for handling abbreviations. However, such approaches scaled poorly for languages with agglutinative morphology (e.g., Russian or Finnish) and complex word combinations.</p> <p>Traditional tokenization methods like space splitting or manual rules often prove ineffective in real-world scenarios: they struggle with typos, rare words, and multilingual texts. For example, words like “gooood” or mixed languages in a single sentence can break a classical tokenizer.</p> <p>In modern NLP, subword tokenization algorithms like <a href="https://arxiv.org/pdf/1508.07909">BPE (Byte Pair Encoding)</a> dominate, balancing the semantic integrity of tokens with efficient vocabulary usage. In this notebook, we will examine the BPE algorithm in detail and learn to work with tokenizers from the Hugging Face library.</p> <p>First, we will import all libraries and functions needed for this notebook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">spacy</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">defaultdict</span>
</code></pre></div></div> <hr/> <h2 id="loading-data">Loading Data</h2> <p>For demonstration, we will load the parallel English-Russian <a href="https://arxiv.org/abs/1812.10464">Tatoeba</a> corpus from Artetxe et al. (2019) via the <a href="http://huggingface.co/docs/datasets/loading">Hugging Face Datasets</a> library.</p> <p><a href="https://tatoeba.org/en/sentences/index">Tatoeba</a> is a free collection of translated example sentences for language learners, available in over 400 languages. Its name comes from the Japanese phrase «tatoeba» (例えば), meaning “for example.” It is written and maintained by a community of volunteers through open collaboration. Individual contributors are known as Tatoebans.</p> <p>We will use only the English and Russian subsets. All examples in this dataset are short everyday phrases: “Let’s try something.” → “Давайте что-нибудь попробуем!”.</p> <p>This format is convenient for training transformers, which work with sequences of limited length. In this notebook, we will not delve into transformer architecture but focus on text data preprocessing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_translation_dataset</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading Tatoeba en-ru...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">Helsinki-NLP/tatoeba</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="sh">"</span><span class="s">en</span><span class="sh">"</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="sh">"</span><span class="s">ru</span><span class="sh">"</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error while loading dataset: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Dataset structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Data sample:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">EN: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">RU: </span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span>
    
    <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_translation_dataset</span><span class="p">()</span>
</code></pre></div></div> <hr/> <h2 id="data-analysis">Data Analysis</h2> <p>Let’s take a quick look at the dataset to understand what we’re dealing with. We won’t dive deep into data analysis methods but will examine basic statistics.</p> <p>The <code class="language-plaintext highlighter-rouge">analyze_dataset</code> function shows that the average length of English sentences is 7.2 words, Russian — 6.2. The maximum lengths (30 and 28 words) indicate the presence of outliers that may require truncation.</p> <p>The histograms show right-skewed distributions: most sentences are shorter than 15 words. These observations influence model hyperparameter choices, e.g., <code class="language-plaintext highlighter-rouge">max_length=64</code> provides padding headroom even if actual sequences are shorter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>

    <span class="n">en_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
    <span class="n">ru_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Analysis based on first </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s"> samples:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">English sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian sentences:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Average length: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Max length: </span><span class="si">{</span><span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Min length: </span><span class="si">{</span><span class="nf">min</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">)</span><span class="si">}</span><span class="s"> words</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">English Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Russian Sentence Lengths</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Words</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">en_lengths</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">ru_lengths</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="nf">analyze_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="simple-tokenizer">Simple Tokenizer</h2> <p>Now we will write a <code class="language-plaintext highlighter-rouge">BaseTokenizer</code> class for text preprocessing, building a token vocabulary, and collecting token frequency statistics. This class will serve as the foundation for more complex tokenizers and provide a common structure for processing text data.</p> <p>We declare the class using the <a href="https://docs.python.org/3/library/dataclasses.html">@dataclass</a> decorator to auto-generate the constructor. Parameters we need: <code class="language-plaintext highlighter-rouge">language</code> (text language), <code class="language-plaintext highlighter-rouge">vocab_size</code> (max vocabulary size), <code class="language-plaintext highlighter-rouge">min_freq</code> (minimum frequency for including a token in the vocabulary), and <code class="language-plaintext highlighter-rouge">special_tokens</code> (list of special tokens).</p> <p>If <code class="language-plaintext highlighter-rouge">special_tokens</code> are not specified, default values are used: <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called immediately after object initialization. Here, we initialize the <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">id2token</code> dictionaries that map tokens to their numeric IDs. Special tokens must be added to the vocabulary first. For example, <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> gets ID 0, <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> — 1, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">preprocess_text</code> method is used to preprocess text. We will convert text to lowercase and split it into tokens using a regular expression.</p> <p>The pattern <code class="language-plaintext highlighter-rouge">r"\w+[\w']*|['’][a-z]+|[^\w\s]"</code> captures:</p> <ul> <li>Words with apostrophes (e.g., <code class="language-plaintext highlighter-rouge">don't</code> → <code class="language-plaintext highlighter-rouge">["don't"]</code>).</li> <li>Contractions starting with an apostrophe (e.g., <code class="language-plaintext highlighter-rouge">'s</code> → <code class="language-plaintext highlighter-rouge">["'s"]</code>).</li> <li>Individual punctuation marks (e.g., <code class="language-plaintext highlighter-rouge">"!"</code> → <code class="language-plaintext highlighter-rouge">["!"]</code>).</li> </ul> <p>Note that the regex may not cover all edge cases (e.g., emojis or compound symbols), requiring modification for specific tasks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">get_stats</code> method collects token frequency statistics. For each text in the <code class="language-plaintext highlighter-rouge">examples</code> list, the <code class="language-plaintext highlighter-rouge">preprocess_text</code> function is called, then the <code class="language-plaintext highlighter-rouge">Counter</code> is updated.</p> <p>For example, the text <code class="language-plaintext highlighter-rouge">"Hello, world!"</code> returns a counter with keys <code class="language-plaintext highlighter-rouge">["hello", ",", "world", "!"]</code> and their frequencies. This method is used during tokenizer training to select tokens for the vocabulary based on <code class="language-plaintext highlighter-rouge">min_freq</code> and <code class="language-plaintext highlighter-rouge">vocab_size</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <p>Below, we consolidate all code into a single class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseTokenizer</span><span class="p">:</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">min_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span> <span class="ow">or</span> <span class="p">[</span><span class="sh">"</span><span class="s">&lt;PAD&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;UNK&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;BOS&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">&lt;EOS&gt;</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">token</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">\w+[\w</span><span class="sh">'</span><span class="s">]*|[</span><span class="sh">'</span><span class="s">’][a-z]+|[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">examples</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="n">counter</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">counter</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_tokenizer</span> <span class="o">=</span> <span class="nc">BaseTokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
</code></pre></div></div> <p>In reality, this basic approach has many drawbacks. For example, converting text to lowercase may lose case information. Additionally, this tokenization ignores word morphology, leading to issues with rare words or homonyms.</p> <p>Let’s write an <code class="language-plaintext highlighter-rouge">analyze_token_statistics</code> function to count unique tokens and their frequencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BaseTokenizer</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">get_stats</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Token statistics for </span><span class="si">{</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total unique tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Top 10 most frequent tokens:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="nf">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stats</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">en_tokenizer</span><span class="p">)</span>
<span class="n">ru_stats</span> <span class="o">=</span> <span class="nf">analyze_token_statistics</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ru_tokenizer</span><span class="p">)</span>
</code></pre></div></div> <p>The difference in token counts for English (1337) and Russian (2065) stems from language features: Russian has richer morphology (endings, prefixes) and more word forms. The dominance of punctuation (. and , in the top) suggests the need for their pre-filtering or separate handling.</p> <p>Interestingly, the <code class="language-plaintext highlighter-rouge">"</code> token appears more frequently in English (146 times) — likely due to translation specifics in Tatoeba.</p> <p>Critically, this approach does not split words into subword units, leaving rare words intact and inflating vocabulary size. For comparison, we will explore the BPE tokenizer in subsequent experiments.</p> <hr/> <h2 id="bpe-tokenization-algorithm">BPE Tokenization Algorithm</h2> <p>Now let’s examine how the <strong>BPE (Byte Pair Encoding)</strong> tokenizer works. The core idea is to iteratively merge the most frequent character or token pairs, gradually forming a subword vocabulary. This efficiently handles rare and complex words by splitting them into known components.</p> <h3 id="bpetokenizer-class">BPETokenizer Class</h3> <p>First, declare the class with the <code class="language-plaintext highlighter-rouge">@dataclass</code> decorator. Since it inherits from <code class="language-plaintext highlighter-rouge">BaseTokenizer</code>, it already includes parameters <code class="language-plaintext highlighter-rouge">language</code>, <code class="language-plaintext highlighter-rouge">vocab_size</code>, <code class="language-plaintext highlighter-rouge">min_freq</code>, and <code class="language-plaintext highlighter-rouge">special_tokens</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
</code></pre></div></div> <h3 id="initialization">Initialization</h3> <p>The <code class="language-plaintext highlighter-rouge">__post_init__</code> method is called after object creation. Here we:</p> <ol> <li>Call the parent <code class="language-plaintext highlighter-rouge">__post_init__</code> to initialize base structures like <code class="language-plaintext highlighter-rouge">token2id</code>.</li> <li>Add a <code class="language-plaintext highlighter-rouge">merges</code> dictionary to store character pairs and their merged versions (e.g., <code class="language-plaintext highlighter-rouge">('h', 'e')</code> → <code class="language-plaintext highlighter-rouge">'he'</code>).</li> <li>Initialize <code class="language-plaintext highlighter-rouge">vocab</code> with special tokens.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>
</code></pre></div></div> <h3 id="generating-character-pairs">Generating Character Pairs</h3> <p>The <code class="language-plaintext highlighter-rouge">get_pairs</code> method splits a word into consecutive character pairs. For example, the word <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code> returns pairs <code class="language-plaintext highlighter-rouge">[('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')]</code>. These pairs are analyzed during training to find the most frequent combinations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div> <hr/> <h2 id="training-the-tokenizer">Training the Tokenizer</h2> <p>The <code class="language-plaintext highlighter-rouge">train</code> method is the core of BPE. It has several stages:</p> <p><strong>Collect Initial Statistics:</strong></p> <ul> <li>Split each token into characters and count character sequence frequencies. For example, token <code class="language-plaintext highlighter-rouge">"hello"</code> becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>, and its frequency increments the counter for <code class="language-plaintext highlighter-rouge">'h e l l o'</code>.</li> <li>Collect all unique characters from the text.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Adding Characters to Vocabulary:</strong></p> <ul> <li>Each unique character (e.g., <code class="language-plaintext highlighter-rouge">'h'</code>, <code class="language-plaintext highlighter-rouge">'e'</code>) is added to <code class="language-plaintext highlighter-rouge">token2id</code> and <code class="language-plaintext highlighter-rouge">vocab</code> if not already present. This ensures even individual characters have IDs.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Main Merge Loop:</strong></p> <ul> <li>Each iteration counts the frequency of all possible character pairs in the current word representations. For example, the word <code class="language-plaintext highlighter-rouge">'h e l l o'</code> has pairs <code class="language-plaintext highlighter-rouge">('h', 'e')</code>, <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, etc.</li> <li>Select the most frequent pair (e.g., <code class="language-plaintext highlighter-rouge">('l', 'l')</code> for <code class="language-plaintext highlighter-rouge">hello</code>) and create a new token <code class="language-plaintext highlighter-rouge">'ll'</code>.</li> <li>Update <code class="language-plaintext highlighter-rouge">merges</code>, <code class="language-plaintext highlighter-rouge">vocab</code>, <code class="language-plaintext highlighter-rouge">token2id</code>, and <code class="language-plaintext highlighter-rouge">id2token</code>.</li> <li>Recalculate word frequencies by replacing the selected pair with the new token. For example, <code class="language-plaintext highlighter-rouge">'h e l l o'</code> becomes <code class="language-plaintext highlighter-rouge">'h e ll o'</code> after merging <code class="language-plaintext highlighter-rouge">('l', 'l')</code>.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
    <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="c1"># Count pair frequencies
</span>    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
        <span class="k">break</span>  <span class="c1"># No pairs left → stop
</span>
    <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
    <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

    <span class="c1"># Update vocabulary
</span>    <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

    <span class="c1"># Recalculate frequencies with new token
</span>    <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
        <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>
</code></pre></div></div> <p>If <code class="language-plaintext highlighter-rouge">num_merges</code> is too high and pairs are exhausted early, training stops. Progress is printed every 1000 iterations to track vocabulary growth.</p> <hr/> <h3 id="text-tokenization">Text Tokenization</h3> <p>The <code class="language-plaintext highlighter-rouge">tokenize</code> method converts text to token IDs:</p> <ol> <li>Text is split into tokens via <code class="language-plaintext highlighter-rouge">preprocess_text</code>.</li> <li>The <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> special token is prepended.</li> <li>For each token (e.g., <code class="language-plaintext highlighter-rouge">"hello"</code>): <ul> <li>Characters are split into <code class="language-plaintext highlighter-rouge">['h', 'e', 'l', 'l', 'o']</code>.</li> <li>Merges from <code class="language-plaintext highlighter-rouge">merges</code> are applied iteratively. For example, if <code class="language-plaintext highlighter-rouge">('l', 'l')</code> is in <code class="language-plaintext highlighter-rouge">merges</code>, the character list becomes <code class="language-plaintext highlighter-rouge">['h', 'e', 'll', 'o']</code>, then remaining pairs are checked.</li> <li>Unknown characters (e.g., <code class="language-plaintext highlighter-rouge">'#'</code>) are replaced with <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</li> </ul> </li> <li>The <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token is appended.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
            <span class="c1"># Find first available merge pair
</span>            <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                    <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="c1"># Replace pair with new token
</span>            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

        <span class="c1"># Add final symbols to result
</span>        <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>During tokenization, merges are applied left-to-right, and the <strong>first</strong> available pair from <code class="language-plaintext highlighter-rouge">merges</code> is chosen. This can yield different results depending on the merge order. For example, if <code class="language-plaintext highlighter-rouge">merges</code> contains <code class="language-plaintext highlighter-rouge">('h', 'e')</code> and <code class="language-plaintext highlighter-rouge">('e', 'l')</code>, the first encountered pair is merged.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">BaseTokenizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__post_init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">merges</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_pairs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">[(</span><span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">word</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">num_merges</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">all_chars</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">all_chars</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_chars</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">char</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">char</span>
                <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>

        <span class="n">word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">chars</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
                <span class="n">word_freqs</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training BPE tokenizer for </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">language</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_merges</span><span class="p">)):</span>
            <span class="n">pair_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="n">pair_freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_freqs</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">best_pair</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">pair_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">new_token</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">best_pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
            <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">new_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">new_token</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="n">self</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>

            <span class="n">new_word_freqs</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freqs</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">word</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">new_token</span><span class="p">)</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">new_word</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">new_symbols</span><span class="p">)</span>
                <span class="n">new_word_freqs</span><span class="p">[</span><span class="n">new_word</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

            <span class="n">word_freqs</span> <span class="o">=</span> <span class="n">new_word_freqs</span>

            <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Merges completed: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_merges</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Current vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;BOS&gt;</span><span class="sh">'</span><span class="p">]]</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">symbols</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

            <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">pairs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_pairs</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span>
                <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">:</span>
                        <span class="n">pair_to_merge</span> <span class="o">=</span> <span class="n">pair</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                    <span class="k">break</span>

                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">new_symbols</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">pair_to_merge</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">merges</span><span class="p">[</span><span class="n">pair_to_merge</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">new_symbols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">symbols</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">symbols</span> <span class="o">=</span> <span class="n">new_symbols</span>

            <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="n">symbol</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;UNK&gt;</span><span class="sh">'</span><span class="p">])</span>

        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;EOS&gt;</span><span class="sh">'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>When applying the tokenizer to convert text to tokens, the algorithm first splits text into base characters, then iteratively merges character pairs using the built merge dictionary. Each word in the text is represented as a sequence of subwords (or tokens) created during training.</p> <p>The number of merges (<code class="language-plaintext highlighter-rouge">num_merges</code> parameter) determines how many times the algorithm will merge characters into new tokens. More merges create larger, more informative tokens. However, excessive merges can lead to loss of fine-grained details.</p> <p>This algorithm performs well with large text corpora and helps models handle rare or unseen words by replacing them with subwords from more frequent character combinations. Additionally, BPE works with any language, even those with unusual or complex alphabets, as it starts from base characters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>
<span class="n">ru_bpe</span> <span class="o">=</span> <span class="nc">BPETokenizer</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32000</span><span class="p">)</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">80000</span>
<span class="n">train_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">))</span>
<span class="n">en_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>
<span class="n">ru_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">train_samples</span><span class="p">]</span>

<span class="n">en_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">en_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">ru_bpe</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">ru_texts</span><span class="p">,</span> <span class="n">num_merges</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">English vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Russian vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ru_bpe</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">token_ids</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">token_ids</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">en_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span>
<span class="n">ru_sample</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">English tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">en_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">en_sample</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Russian tokenization:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ru_tokens</span> <span class="o">=</span> <span class="nf">test_tokenization</span><span class="p">(</span><span class="n">ru_sample</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">)</span>
</code></pre></div></div> <p>Overall, BPE effectively addresses rare and complex words, improving tokenization quality and NLP model performance.</p> <p>However, even after training, artifacts remain. For example, “useless” splits into [“us”, “el”, “ess”], and “бесполезно” into [“бес”, “пол”, “ез”, “но”]. This stems from the limited number of merges and the lack of explicit morpheme boundary consideration in our educational implementation.</p> <p>In production tokenizers (e.g., Hugging Face’s), such issues are mitigated by pretraining on massive corpora and tens of thousands of merges.</p> <hr/> <h2 id="batch-preparation">Batch Preparation</h2> <p>The <code class="language-plaintext highlighter-rouge">prepare_batch</code> function converts tokenized sequences into tensors suitable for training. Each sentence is padded to a fixed length (<code class="language-plaintext highlighter-rouge">max_length=64</code>) with the <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code> token, and attention masks tell the model to ignore these “empty” positions.</p> <p>For example, a sentence with 24 tokens becomes a vector of length 64, where the last 40 elements are zeros (ID <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>). Masking is critical for transformers, as the attention mechanism would otherwise account for meaningless padding tokens, distorting weights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
                 <span class="n">src_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">tgt_tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span>
                 <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">src_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">tgt_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="n">src_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">src_texts</span><span class="p">]</span>
    <span class="n">tgt_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">tgt_texts</span><span class="p">]</span>

    <span class="n">src_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_padded</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">src_masks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_masks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">src_tokens</span><span class="p">,</span> <span class="n">tgt_tokens</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">src_pad</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="p">[</span><span class="n">src_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>
            <span class="n">src_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">src</span><span class="p">))</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_pad</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="p">[</span><span class="n">tgt_tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;PAD&gt;</span><span class="sh">'</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>
            <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">tgt</span><span class="p">))</span>

        <span class="n">src_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_pad</span><span class="p">)</span>
        <span class="n">tgt_padded</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_pad</span><span class="p">)</span>
        <span class="n">src_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">src_mask</span><span class="p">)</span>
        <span class="n">tgt_masks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tgt_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_padded</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">src_masks</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">tgt_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tgt_masks</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="tokenizer-verification">Tokenizer verification</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Prepared batch shapes:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">value</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Example source tokens:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Corresponding mask:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">:</span> <span class="n">BPETokenizer</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">base_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Base tokenization: </span><span class="si">{</span><span class="n">base_tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Number of merges learned: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample merges (first 5):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span><span class="p">,</span> <span class="n">merged</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">merges</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">pair</span><span class="si">}</span><span class="s"> -&gt; </span><span class="si">{</span><span class="n">merged</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample vocabulary items (first 10):</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">token2id</span><span class="p">.</span><span class="nf">items</span><span class="p">())[:</span><span class="mi">10</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">id2token</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Final tokenization:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Token IDs: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Decoded tokens: </span><span class="si">{</span><span class="n">decoded</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Testing English tokenizer:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">verify_bpe_tokenization</span><span class="p">(</span><span class="n">en_bpe</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div> <hr/> <h2 id="hugging-face-tokenizers">Hugging Face Tokenizers</h2> <p>All this seems quite complex. Our current tokenizer works imperfectly and is slow. Fortunately, programmers avoid reinventing the wheel. In practice, it’s much easier to use a ready-made tokenizer via <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> from the <code class="language-plaintext highlighter-rouge">transformers</code> library.</p> <p>The <code class="language-plaintext highlighter-rouge">opus-mt-en-ru</code> model already has a pretrained BPE vocabulary optimized for the language pair. The tokenizer automatically adds special tokens, handles case, and rare symbols. When processing the dataset, the <code class="language-plaintext highlighter-rouge">map</code> function applies tokenization in parallel to all examples, speeding up work via batching.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Helsinki-NLP/opus-mt-en-ru</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
<span class="p">):</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="n">source_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">en</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>
        <span class="n">target_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="sh">'</span><span class="s">ru</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">'</span><span class="s">translation</span><span class="sh">'</span><span class="p">]]</span>

        <span class="n">source_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">source_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="n">target_encoding</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
            <span class="n">target_texts</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">max_length</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">np</span><span class="sh">'</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">source_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">decoder_attention_mask</span><span class="sh">'</span><span class="p">:</span> <span class="n">target_encoding</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span>
        <span class="n">preprocess_function</span><span class="p">,</span>
        <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="n">column_names</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="comparing-tokenizers">Comparing Tokenizers</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Custom BPE Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shape of prepared batches:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">array</span> <span class="ow">in</span> <span class="n">prepared_data</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> (dtype: </span><span class="si">{</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Sample data from first batch:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Source tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Target tokens (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">tgt_tokens</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Source mask (first example):</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">[</span><span class="sh">'</span><span class="s">src_mask</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hugging Face Tokenizer Data Structure:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataset features: </span><span class="si">{</span><span class="n">processed_dataset</span><span class="p">.</span><span class="n">features</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of examples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">processed_dataset</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">first_example</span> <span class="o">=</span> <span class="n">processed_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">First example details:</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Input IDs shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded input:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">input_ids</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Labels shape:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">]))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Decoded labels:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Attention mask sample:</span><span class="sh">"</span><span class="p">,</span> <span class="n">first_example</span><span class="p">[</span><span class="sh">'</span><span class="s">attention_mask</span><span class="sh">'</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_samples</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">prepared_data</span> <span class="o">=</span> <span class="nf">prepare_batch</span><span class="p">(</span><span class="n">test_samples</span><span class="p">,</span> <span class="n">en_bpe</span><span class="p">,</span> <span class="n">ru_bpe</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="nf">print_custom_bpe_data_shape</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>


<span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="nf">prepare_data_with_hf</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="nf">print_hf_data_details</span><span class="p">(</span><span class="n">processed_data</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine-learning"/><category term="tokenization,"/><category term="bpe,"/><category term="algorithms,"/><category term="nlp"/><summary type="html"><![CDATA[Implement one of the most popular tokenization algorithms and learn how to use ready-made solutions.]]></summary></entry><entry><title type="html">Can AI Achieve True Creativity?</title><link href="https://xmarva.github.io/blog/2025/creative-ai/" rel="alternate" type="text/html" title="Can AI Achieve True Creativity?"/><published>2025-02-13T15:00:00+00:00</published><updated>2025-02-13T15:00:00+00:00</updated><id>https://xmarva.github.io/blog/2025/creative-ai</id><content type="html" xml:base="https://xmarva.github.io/blog/2025/creative-ai/"><![CDATA[<p>The criticism that AI cannot create something fundamentally new often overlooks a key fact: human creativity itself is built on combining existing knowledge, concepts, and imagery.</p> <p>Modern research reveals that when humans invent something novel, the brain doesn’t conjure elements “from nothing” but recombines fragments of prior experiences. Neural networks operate similarly.</p> <p>When GPT-4 generates text, it relies on statistical patterns learned from existing data rather than conscious intent.</p> <h3 id="the-brains-creative-networks-vs-gans">The Brain’s Creative Networks vs. GANs</h3> <p>In one experiment, participants were asked to devise unconventional uses for everyday objects (e.g., a coffee cup). fMRI scans showed two activated networks during creativity:</p> <ul> <li>The default mode network (associated with daydreaming and associations)</li> <li>The frontoparietal network (linked to focus and control)</li> </ul> <p>The dorsolateral prefrontal cortex also activates during musical improvisation. This interaction mirrors generative adversarial networks (GANs), where one component generates ideas and another evaluates their plausibility. Both biological and artificial systems balance freedom and constraints to produce novelty.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/0-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/0-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="fMRI activation during creative tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fMRI activation patterns during creative tasks (Source: Beaty et al., 2016) </div> <h3 id="the-limits-of-perception-and-data">The Limits of Perception and Data</h3> <p>Human perception constrains creativity. We can’t imagine colors beyond the visible spectrum—any new shade is a recombination of known hues. Similarly, Stable Diffusion can’t generate images of objects absent from its training data. It blends learned features, much like the brain combines memories.</p> <p>Blind individuals describe colors through analogies to sounds or textures (e.g., red as “loud noise,” blue as “smooth surface”). This suggests creativity is rooted in sensory experience—a dimension AI lacks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-creative-ai/1-480.webp 480w,/assets/img/posts/2025-02-16-creative-ai/1-800.webp 800w,/assets/img/posts/2025-02-16-creative-ai/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-02-16-creative-ai/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stable Diffusion-generated floral carpet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Floral carpet generated by Stable Diffusion </div> <h3 id="embodied-vs-abstract-learning">Embodied vs. Abstract Learning</h3> <p>A key difference lies in how humans and AI acquire knowledge:</p> <ul> <li>Humans learn through physical interaction (e.g., toddlers touching objects to link tactile and visual data)</li> <li>AI processes abstract tokens/pixels</li> </ul> <p>While LLMs develop semantic relationships (e.g., “apple” vectors near “fruit” and “tree”), they lack embodied experiences stored in the brain’s sensorimotor cortex. When we think “run,” motor neurons activate—a connection AI can’t replicate.</p> <h3 id="the-myth-of-intentional-creativity">The Myth of Intentional Creativity</h3> <p>Critics argue humans possess creative “intent,” but jazz improvisation studies show decreased prefrontal cortex activity during spontaneous creation. Ideas emerge automatically from learned patterns—a process strikingly similar to how neural networks operate.</p> <h3 id="the-originality-debate">The Originality Debate</h3> <p>If human innovation is recombination, demanding absolute novelty from AI is flawed. Both are constrained by their “training data”:</p> <ul> <li>Human brains: Biological experiences</li> <li>AI models: Digital datasets</li> </ul> <p>The distinction lies in complexity and emotional embodiment. While AI manipulates mathematical structures, the brain ties patterns to emotions and bodily states—for now.</p> <p><strong>References</strong><br/> [1] Beaty, R. E., Benedek, M., Silvia, P. J., &amp; Schacter, D. L. (2016). Creative Cognition and Brain Network Dynamics. <em>Trends in Cognitive Sciences</em><br/> [2] De Borst, A. W., &amp; de Gelder, B. (2018). Mental Imagery Follows Similar Cortical Reorganization as Perception: Intra-Modal and Cross-Modal Plasticity in Congenitally Blind. <em>Cerebral Cortex</em><br/> [3] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dea, J. (2013). Distributed Representations of Words and Phrases and their Compositionality<br/> [4] Limb, C. J., &amp; Braun, A. R. (2008). Neural Substrates of Spontaneous Musical Performance: An fMRI Study of Jazz Improvisation</p>]]></content><author><name></name></author><category term="cognitive-science"/><category term="ai,"/><category term="neuroscience,"/><category term="creativity"/><summary type="html"><![CDATA[Exploring the parallels between human creativity and neural networks]]></summary></entry><entry><title type="html">Algebraic Foundations of Low-Rank Adaptation</title><link href="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/" rel="alternate" type="text/html" title="Algebraic Foundations of Low-Rank Adaptation"/><published>2024-12-30T15:09:00+00:00</published><updated>2024-12-30T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/lora-algorithm-for-llms</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/lora-algorithm-for-llms/"><![CDATA[<h2 id="the-paradox-of-scale">The Paradox of Scale</h2> <p>The evolution of language models presents us with an intriguing paradox: while increasing model size enhances general capability, it simultaneously complicates practical deployment through prohibitive computational demands. This tension between capacity and practicality forms the crucible where Low-Rank Adaptation (LoRA) emerges as an elegant solution. To understand its mechanisms, we must first establish fundamental mathematical constructs.</p> <h2 id="matrix-theory-foundations">Matrix Theory Foundations</h2> <h3 id="the-algebraic-scaffolding">The Algebraic Scaffolding</h3> <p>A matrix \(A \in \mathbb{R}^{m \times n}\) represents a linear transformation between vector spaces \(\mathbb{R}^n \to \mathbb{R}^m\). Each element \(a_{ij}\) encodes the transformation coefficient between basis vectors \(e_j\) and \(e_i\). In neural networks, these matrices become learned representations of feature interactions.</p> <p>The <strong>rank</strong> of a matrix, denoted \(\rho(A)\), measures its column space dimensionality through the maximal number of linearly independent columns. Formally:</p> \[\rho(A) = \dim(\text{col}(A)) = \dim(\text{row}(A))\] <p>This duality between row and column space dimensionalities (proven via the Fundamental Theorem of Linear Algebra) becomes crucial for understanding parameter efficiency.</p> <h3 id="rank-constrained-transformations">Rank-Constrained Transformations</h3> <p>Consider two matrices \(B \in \mathbb{R}^{m \times r}\) and \(A \in \mathbb{R}^{r \times n}\). Their product \(BA\) inherently satisfies:</p> \[\rho(BA) \leq \min(\rho(B), \rho(A)) \leq r\] <p>This rank upper bound enables dramatic parameter reduction when \(r \ll \min(m,n)\). For a neural layer with \(m \times n\) weights, replacing full updates with low-rank factors reduces trainable parameters from \(mn\) to \(r(m+n)\) – an efficiency gain of \(\frac{mn}{r(m+n)}\). For typical layers (\(m,n \sim 10^3\), \(r \sim 10^1\)), this yields ~100x parameter reduction.</p> <h2 id="the-low-rank-adaptation-hypothesis">The Low-Rank Adaptation Hypothesis</h2> <h3 id="intrinsic-dimensionality-of-task-adaptation">Intrinsic Dimensionality of Task Adaptation</h3> <p>Modern language models exhibit an intriguing property: while pretrained on broad corpora, task-specific adaptation appears to operate in low-dimensional subspaces. This phenomenon aligns with the <strong>manifold hypothesis</strong>, suggesting high-dimensional data actually resides on lower-dimensional manifolds.</p> <p>Let \(\Delta W \in \mathbb{R}^{m \times n}\) represent weight updates during fine-tuning. The LoRA conjecture posits:</p> \[\rho(\Delta W) \leq r \ll \min(m,n)\] <p>Experimental validation shows task adaptation often requires surprisingly low ranks (\(r=8\) achieves strong performance). This implies that while the original parameter space is vast, task-specific adjustments occupy a small subspace.</p> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Visualize the weight matrix as a point in \(\mathbb{R}^{mn}\). Full fine-tuning moves this point through the high-dimensional space. LoRA constrains movement to a low-dimensional <strong>adaptation manifold</strong> spanned by \(B\) and \(A\):</p> \[\mathcal{M}_r = \{ W + BA \mid B \in \mathbb{R}^{m \times r}, A \in \mathbb{R}^{r \times n} \}\] <p>The approximation error is bounded by the Eckart–Young theorem:</p> \[\min_{\rho(BA)\leq r} \| \Delta W - BA \|_F = \sum_{i=r+1}^{\min(m,n)} \sigma_i(\Delta W)\] <p>where \(\sigma_i\) denotes singular values. Rapidly decaying singular values in \(\Delta W\) (as observed empirically) enable accurate low-rank approximation.</p> <h2 id="algorithmic-implementation">Algorithmic Implementation</h2> <h3 id="parameterization-and-initialization">Parameterization and Initialization</h3> <p>For a pretrained weight matrix \(W_0\), LoRA constructs:</p> \[W = W_0 + \frac{\alpha}{r}BA\] <p>Where:</p> <ul> <li>\(B\) initialized with \(\mathcal{N}(0, \sigma^2)\)</li> <li>\(A\) initialized to zero</li> <li>\(\alpha\): learning rate scaling factor</li> </ul> <p>The initialization strategy ensures \(\Delta W = 0\) at training onset, preserving original model behavior. The \(\alpha/r\) scaling normalizes parameter updates across different ranks, maintaining stable learning dynamics.</p> <h3 id="gradient-dynamics">Gradient Dynamics</h3> <p>Let \(\mathcal{L}\) be the loss function. The gradient through the LoRA parameters becomes:</p> \[\nabla_B \mathcal{L} = \frac{\alpha}{r} (\nabla_{W} \mathcal{L}) A^T \\ \nabla_A \mathcal{L} = \frac{\alpha}{r} B^T (\nabla_{W} \mathcal{L})\] <p>This reveals an important property: gradient signals flow through both low-rank factors, with the scaling term modulating update magnitudes. The rank \(r\) therefore acts as a gradient multiplier – higher ranks enable stronger gradient signals but increase parameter count.</p> <h2 id="practical-considerations-and-variations">Practical Considerations and Variations</h2> <h3 id="rank-selection-tradeoffs">Rank Selection Tradeoffs</h3> <p>The choice of \(r\) balances expressivity vs efficiency:</p> <ul> <li><strong>Lower ranks (r=1-4):</strong> Maximize parameter efficiency, suitable for similar source/target tasks</li> <li><strong>Medium ranks (r=8-16):</strong> General-purpose setting for domain adaptation</li> <li><strong>Higher ranks (r=32+):</strong> Needed for complex task transfers or low-data scenarios</li> </ul> <p>Empirical studies show performance follows logarithmic scaling:</p> \[\text{Performance}(r) \approx \text{Performance}(\text{full}) - c/\log r\] <p>Where \(c\) is task-dependent. This suggests diminishing returns beyond certain ranks.</p> <h3 id="architectural-variants">Architectural Variants</h3> <ol> <li><strong>Bottleneck Adaptation:</strong> Stack multiple low-rank layers (\(W_0 + B_1A_1 + B_2A_2\)) for hierarchical adaptation</li> <li><strong>Sparse LoRA:</strong> Combine with magnitude pruning on \(BA\) product</li> <li><strong>Dynamic Rank Allocation:</strong> Use singular value thresholds to automatically select per-layer ranks</li> <li><strong>LoRA++:</strong> Introduce learned scaling factors per layer instead of fixed \(\alpha/r\)</li> </ol> <h3 id="compositional-adaptation">Compositional Adaptation</h3> <p>For multi-task learning, LoRA enables parameter composition:</p> \[W = W_0 + \sum_{k=1}^K B_kA_k\] <p>Where each \(B_kA_k\) captures task-specific adaptations. During inference, select subsets of adapters via:</p> \[W = W_0 + \sum_{k \in S} B_kA_k\] <p>This facilitates efficient multi-task serving with \(\mathcal{O}(Kr)\) storage instead of \(\mathcal{O}(K)\) full models.</p> <h2 id="theoretical-implications">Theoretical Implications</h2> <h3 id="implicit-regularization">Implicit Regularization</h3> <p>The low-rank constraint acts as a strong regularizer, preventing overfitting to small datasets. Consider the Rademacher complexity for a LoRA-adapted layer:</p> \[\mathcal{R}_n(\mathcal{H}_{\text{LoRA}}) \leq \frac{\alpha \sqrt{2r\log(2mn)}}{n}\] <p>Compared to full fine-tuning’s \(\mathcal{O}(\sqrt{mn/n})\) complexity, LoRA’s bound is significantly tighter, explaining its improved generalization in low-data regimes.</p> <h3 id="information-bottleneck-perspective">Information Bottleneck Perspective</h3> <p>Interpreting through the information bottleneck lens, LoRA enforces:</p> \[\min_{B,A} I(W; BA) \quad \text{s.t.} \quad I(BA; \mathcal{T}) \geq I_c\] <p>Where \(\mathcal{T}\) is the target task and \(I_c\) the required information. The low-rank structure naturally minimizes irrelevant information from \(W\) while preserving task-relevant features.</p> <h2 id="epilogue">Epilogue</h2> <p>LoRA epitomizes the principle that profound solutions often arise from deep mathematical insight rather than brute-force computation. By reconceptualizing adaptation as a low-rank update process, it achieves an elegant synthesis of efficiency and effectiveness – a reminder that in machine learning as in mathematics, constraints often breed creativity.</p> <p>The road ahead suggests intriguing possibilities: could other matrix properties (e.g., sparsity patterns, eigenvalue distributions) inspire new adaptation paradigms? As language models continue evolving, such algebraic perspectives will likely remain essential tools for harnessing their potential.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="nlp,"/><category term="llm,"/><category term="lora"/><summary type="html"><![CDATA[Mathematical exploration of parameter-efficient fine-tuning through matrix rank theory]]></summary></entry><entry><title type="html">LLMs for Those Who Missed Out</title><link href="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/" rel="alternate" type="text/html" title="LLMs for Those Who Missed Out"/><published>2024-04-24T15:09:00+00:00</published><updated>2024-04-24T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out</id><content type="html" xml:base="https://xmarva.github.io/blog/2024/llms-for-those-who-missed-out/"><![CDATA[<p>An LLM (large language model) consists of just two files:</p> <ul> <li>Large file with billions of parameters (weights)</li> <li>Small file with about 500 lines of code to run the model</li> </ul> <p>LLM doesn’t take up much space and doesn’t require extensive computing power.</p> <p>For example, the <a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">Llama-2–70b model</a> from Meta, which is open source and has 70 billion parameters, is just 140 GB and can be run locally on a MacBook without internet access. You can <a href="https://www.llama.com/llama-downloads/">download this model</a> from Meta’s website and use it for free.</p> <p>However, you need significant computing resources to get these parameters (train the model).</p> <h2 id="how-to-train-a-model">How to Train a Model</h2> <h3 id="stage-1-pretraining-training-the-base-model">Stage 1: Pretraining (Training the base model)</h3> <p>Think of this process as compressing the Internet into a neural network, similar to how a ZIP file compresses documents into an archive.</p> <p>Here’s what you need to get a model like Llama-2:</p> <ul> <li>Take a “slice of the internet,” approximately ten terabytes of text in size</li> <li>Use a cluster of ~6,000 GPUs (specialized graphics processors used for heavy computations)</li> <li>Compress the text into the neural network. This takes about 12 days and costs around $2M</li> <li>Acquire the base model (the file with parameters)</li> </ul> <p>The primary function of the base model is to predict the next word. You input a sequence of words, and it outputs the next most likely word based on the texts it was trained on.</p> <p>For example, for the phrase “cat sat on a,” the LLM might predict the word “mat” with 97% probability.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/0-480.webp 480w,/assets/img/posts/2024-04-24-llms/0-800.webp 800w,/assets/img/posts/2024-04-24-llms/0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Image from the presentation illustrating how the neural network works" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image from the presentation illustrating how the neural network works </div> <p>That’s basically how compression works: if you can predict the next word accurately, you can use this information to compress the original data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/1-480.webp 480w,/assets/img/posts/2024-04-24-llms/1-800.webp 800w,/assets/img/posts/2024-04-24-llms/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="I checked, and ChatGPT indeed responds with “mat”" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> I checked, and ChatGPT indeed responds with “mat” </div> <p>However, unlike a ZIP file, where compression is lossless, LLM “compresses” the internet with losses. This is why models can hallucinate: they make up information that doesn’t actually exist but appears plausible (similar to what they have seen in the training data).</p> <p>Moreover, models can sometimes produce completely unexpected things. For example, ChatGPT knows the answer to the question, “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer), but it doesn’t know the answer to the question, “Who is Mary Lee Pfeiffer’s son?”</p> <p>This isn’t a typical database that simply stores information; it’s a different format we don’t fully understand.</p> <p>This clearly illustrates that we don’t quite grasp how this whole thing works and can only see the results it produces.</p> <h3 id="stage-2-finetuning-additional-training">Stage 2: Finetuning (Additional training)</h3> <p>The base model isn’t very practical for everyday use. We don’t just want to receive continuous word sequences; we want to ask questions and get answers.</p> <p>This requires finetuning — a process in which we develop an assistant model that answers questions.</p> <p>The training process for the assistant model is similar to that of the base model, but now we train the model not on internet texts but on data we manually collect. Companies hire people who write questions and answer them.</p> <p>If the training of the base model occurs on huge volumes of often low-quality text, the training of the assistant model involves comparatively smaller volumes (say, 100,000 documents), but they are all of very high quality.</p> <p>After such finetuning, the model understands that when asked a question, it should respond in the style of a helpful assistant.</p> <p>Unlike the training of the base model, which is conducted at most 2–3 times a year, finetuning can be done regularly, even weekly, as it is much less costly.</p> <h3 id="stage-3-optional-comparisons">Stage 3 (optional): Comparisons</h3> <p>In many cases it’s easier not to write a response from scratch but to compare several different answer options and choose the best one.</p> <p>The data from these comparisons is used to train the model further.</p> <p>At OpenAI, this is called RLHF (Reinforcement Learning from Human Feedback).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/2-480.webp 480w,/assets/img/posts/2024-04-24-llms/2-800.webp 800w,/assets/img/posts/2024-04-24-llms/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Difference between Open and Closed Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Difference between Open and Closed Models. Source: Chatbot leaderboards </div> <p>Models with open weights, such as Llama-2, are still inferior compared to proprietary models like GPT-4 and Claude.</p> <p>However, proprietary models cannot be downloaded for personal use or further training; they can only be operated through a web interface or an API (though the introduction of Custom GPTs at OpenAI is a first step towards customization)</p> <h2 id="capabilities-of-llms-using-chatgpt-as-an-example">Capabilities of LLMs (using ChatGPT as an example)</h2> <p>Prompted by an LLM, it can understand what the user needs and use external tools for this:</p> <ul> <li>Search the internet if the user needs up-to-date information (goes to Bing, retrieves a list of the latest links for the query, copies the full text of each link into its context window, and summarizes the information)</li> <li>Use a calculator to do calculations</li> <li>Use a Python library to draw graphs</li> <li>Draw images using DALL-E</li> <li>Write code</li> </ul> <p>Moreover, LLMs can be multimodal. They can recognize text, voice, images, or video and respond with voice, images, or video.</p> <h2 id="can-llms-make-decisions">Can LLMs Make Decisions?</h2> <p>There’s a well-known book by Kahneman, “Thinking, Fast and Slow.”</p> <p>The main idea of the book is that there are two systems in the brain: System 1, which has fast, automatic reactions, and System 2, which is slower, rational, and conscious and makes complex decisions.</p> <p>For example, the answer to question 2+2 comes from System 1 because this knowledge is automatic for us. But calculating 17×24 requires System 2.</p> <p>If we apply these terms, current LLMs only possess System 1. They can only provide the most likely next word in real time.</p> <p>It would be great if we could come to an LLM and say: here’s my question; you can think for 30 minutes, but I need a very accurate and high-quality answer.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2024-04-24-llms/3-480.webp 480w,/assets/img/posts/2024-04-24-llms/3-800.webp 800w,/assets/img/posts/2024-04-24-llms/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2024-04-24-llms/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Thought tree" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Thought tree </div> <p>No models can do that yet. But it would be desirable for a model to have something like a “thought tree,” through which it could navigate, analyze the results, go back and try again until it achieves a result it is most confident about.</p> <h2 id="can-llms-train-themselves">Can LLMs Train Themselves?</h2> <p>There’s a famous case where AlphaGo (a program that plays Go) was trained in two stages:</p> <p>First, it was trained on human games and learned to play very well. Then it began to train itself-playing against itself, trying to maximize the likelihood of winning-and significantly improved its quality.</p> <p>But in LLMs, we’re only at stage 1-training only occurs on materials created by humans.</p> <p>Why is this?</p> <p>In the game of Go, there is a very clear criterion for success-a won game- and you can train the model to maximize the likelihood of winning.</p> <p>In LLMs, the criteria are not so obvious. It’s not clear how to assess whether the result is good.</p> <p>Such criteria can be found in some narrow topics, but in general, it’s still hard to imagine.</p> <h2 id="what-llms-will-be-able-to-do-in-a-few-years">What LLMs Will Be Able to Do in a Few Years</h2> <p>Finally, a small forecast.</p> <p>In a few years, LLMs:</p> <ul> <li>Will have more knowledge than any human on all subjects</li> <li>Can operate on the internet</li> <li>Can use existing software infrastructure (calculator, Python, mouse/keyboard)</li> <li>Can see and generate images and videos</li> <li>Can hear and speak, and also generate music</li> <li>Can think for a long period using System 2</li> <li>Can train themselves in areas where there are clear criteria</li> <li>Can be customized and refined for specific tasks. Many versions will exist in app stores</li> <li>Can interact with other LLMs</li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="nlp,"/><category term="llm"/><summary type="html"><![CDATA[Let's talk about large language models. Once again.]]></summary></entry><entry><title type="html">How to Write Good Python Code</title><link href="https://xmarva.github.io/blog/2023/python-code/" rel="alternate" type="text/html" title="How to Write Good Python Code"/><published>2023-02-07T15:09:00+00:00</published><updated>2023-02-07T15:09:00+00:00</updated><id>https://xmarva.github.io/blog/2023/python-code</id><content type="html" xml:base="https://xmarva.github.io/blog/2023/python-code/"><![CDATA[<p>Python is a fantastic programming language!</p> <p>It can be used for many things, like building websites, exploring data, and teaching machines to learn.</p> <p>If you already know Python or are just beginning, writing code that is strong, easy to read, and easy to keep up with is important.</p> <p>In this bogpost, we’ll look at the basic rules for writing great Python code and share some tips to help you make your programs even better.</p> <h2 id="-use-meaningful-naming-conventions">📚 Use Meaningful Naming Conventions</h2> <p>One of the most important aspects of good Python code is meaningful naming conventions.</p> <p>Choosing descriptive and concise names for variables, functions, and classes can help make your code more readable and understandable.</p> <p>Using proper naming conventions can also help you avoid naming conflicts, reduce the risk of errors, and simplify maintenance.</p> <p>For example, these are bad variable names:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>And these are better ones:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_number</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">second_number</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">sum_of_numbers</span> <span class="o">=</span> <span class="n">first_number</span> <span class="o">+</span> <span class="n">second_number</span>
<span class="n">double_sum</span> <span class="o">=</span> <span class="n">sum_of_numbers</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p>This includes using proper indentation, white space, line breaks, and following a code style guide like the PEP 8 style guide.</p> <p>Clear, organized code makes it easier to understand and modify and reduces the risk of errors.</p> <p>Here’s an example of lousy code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>
<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Sum: </span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Difference: </span><span class="sh">"</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Product: </span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Quotient: </span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
</code></pre></div></div> <p>And here’s an example of good code organization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="n">c</span> <span class="o">-</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> <span class="o">/</span> <span class="n">c</span> <span class="o">/</span> <span class="n">d</span>

<span class="k">def</span> <span class="nf">process_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="nf">calc_diff</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">calc_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="nf">calc_quotient</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Sum</span><span class="sh">"</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Difference</span><span class="sh">"</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Product</span><span class="sh">"</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Quotient</span><span class="sh">"</span><span class="p">:</span> <span class="n">q</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <h2 id="-write-comments">💬 Write Comments</h2> <p>Adding comments to your code is a great way to explain what it does and provide context for other developers.</p> <p>Comments should be used to explain complex code, provide additional information about the purpose of the code, and describe your thought process.</p> <p>Writing comments can also help you better understand your code when you return to it later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># function to calculate sum
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># this function calculates sum of two numbers
</span><span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments are not very descriptive or helpful in understanding the purpose of the functions.</p> <p>The first comment is trivial and adds no additional information. The second comment repeats what the function name already tells us.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calc_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the sum of two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns their sum.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the sum of `a` and `b`
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="k">def</span> <span class="nf">calc_difference</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function calculates the difference between two numbers `a` and `b`.
    The function takes in two keyword arguments, `a` and `b`, and returns the difference of `a` and `b`.
    </span><span class="sh">"""</span>
    <span class="c1"># calculate the difference between `a` and `b`
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
</code></pre></div></div> <p>The comments provide a clear and concise explanation of the purpose and behaviour of each function.</p> <p>The use of docstrings makes it easy to understand what the functions do and what arguments they take in. This makes the code more readable and maintainable.</p> <h2 id="-use-modules-and-packages">🧰 Use Modules and Packages</h2> <p>Modules and packages are a great way to organize your code into reusable blocks.</p> <p>They allow you to group related code together and make it easier to manage, understand, and maintain.</p> <p>The Python Standard Library is an good resource for finding pre-existing modules and packages. You can import it into your programs to save time and effort.</p> <p>Consider a project to build a simple weather application that provides a given city’s current temperature and conditions. We can structure the project as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>weather_app/
    __init__.py
    weather.py
    utils/
        __init__.py
        api.py
        data_processing.py
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">weather.py</code> is the main module that the user interacts with, which provides a single function to get the current weather information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_current_weather</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Gets the current weather information for the given city.

    Args:
        city (str): The city for which to get the weather information.

    Returns:
        dict: The weather information for the given city.
    </span><span class="sh">"""</span>
    <span class="n">weather_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">api</span><span class="p">.</span><span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">)</span>
    <span class="n">processed_data</span> <span class="o">=</span> <span class="n">utils</span><span class="p">.</span><span class="n">data_processing</span><span class="p">.</span><span class="nf">process_weather_data</span><span class="p">(</span><span class="n">weather_data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <p>The utils package contains two modules, <code class="language-plaintext highlighter-rouge">api.py</code> and <code class="language-plaintext highlighter-rouge">data_processing.py</code>, which contain helper functions to retrieve the raw weather data from an API and to process the raw data into a more readable format, respectively.</p> <p>These modules can be reused across different projects, so it makes sense to organize them into a separate package.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># api.py
</span><span class="k">def</span> <span class="nf">get_weather_data</span><span class="p">(</span><span class="n">city</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Retrieves the raw weather data for the given city.

    Args:
        city (str): The city for which to retrieve the weather data.

    Returns:
        dict: The raw weather data for the given city.
    </span><span class="sh">"""</span>
    <span class="c1"># code to retrieve data from API
</span>    <span class="k">return</span> <span class="n">raw_data</span>

<span class="c1"># data_processing.py
</span><span class="k">def</span> <span class="nf">process_weather_data</span><span class="p">(</span><span class="n">raw_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Processes the raw weather data into a more readable format.

    Args:
        raw_data (dict): The raw weather data.

    Returns:
        dict: The processed weather data.
    </span><span class="sh">"""</span>
    <span class="c1"># code to process data
</span>    <span class="k">return</span> <span class="n">processed_data</span>
</code></pre></div></div> <h2 id="-test-your-code">🧪 Test Your Code</h2> <p>Testing your code helps you catch bugs and ensure that your code works as expected.</p> <p>Writing test cases is also an good way to document your code and help others understand it. Try all possible scenarios when testing your code, including edge cases and error conditions.</p> <p>Consider a module <code class="language-plaintext highlighter-rouge">calculator.py</code> that implements a simple calculator with basic arithmetic operations. We can write test cases for each operation using a testing framework such as <code class="language-plaintext highlighter-rouge">unittest</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">unittest</span>
<span class="kn">import</span> <span class="n">calculator</span>

<span class="k">class</span> <span class="nc">TestCalculator</span><span class="p">(</span><span class="n">unittest</span><span class="p">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_addition</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_subtraction</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">subtract</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_multiplication</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_division</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">divide</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">assertEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">unittest</span><span class="p">.</span><span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <p>Each test case tests a single operation in the calculator module and uses the <code class="language-plaintext highlighter-rouge">assertEqual</code> method to verify that the result of the operation is as expected.</p> <p>If any test fails, an error will be raised, and the test result will be reported as failed.</p> <p>For debugging we can use the <code class="language-plaintext highlighter-rouge">print</code> statement to print the intermediate results or the values of variables in the code, or use a debugger such as <code class="language-plaintext highlighter-rouge">pdb</code> to step through the code and inspect the values of variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">calculator</span>
<span class="kn">import</span> <span class="n">pdb</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">calculator</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span> <span class="c1"># Set a breakpoint
</span><span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-document-your-code">📜 Document Your Code</h2> <p>Documenting your code with docstrings can help others understand what it does and how it works.</p> <p>Docstrings should provide a high-level overview of the code, including its purpose, usage, and limitations.</p> <p>They should also be written in a clear and natural language style.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Circle</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Class to represent a circle with a given radius.

    Attributes:
        radius (float): The radius of the circle.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the Circle class with a given radius.

        Args:
            radius (float): The radius of the circle.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">=</span> <span class="n">radius</span>

    <span class="k">def</span> <span class="nf">area</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the area of the circle.

        Returns:
            float: The area of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">radius</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">circumference</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Calculates the circumference of the circle.

        Returns:
            float: The circumference of the circle.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">radius</span>
</code></pre></div></div> <p>The class has a docstring explaining its purpose and the attributes it has.</p> <p>Each method has its docstring explaining what it does and what arguments it takes and returns.</p> <p>This makes the code easier to understand and maintain and more accessible for others to use and build upon.</p> <h2 id="-handle-exceptions-gracefully">💥 Handle Exceptions Gracefully</h2> <p>Handling exceptions in your code is essential for ensuring that it continues to run even when unexpected events occur.</p> <p>Use <code class="language-plaintext highlighter-rouge">try</code> and <code class="language-plaintext highlighter-rouge">except</code> statements to handle exceptions and provide helpful error messages that explain what went wrong and how to fix it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="n">result</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="mi">0</span>
<span class="k">except</span> <span class="nb">ZeroDivisionError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a non-zero value for division</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>The code inside the <code class="language-plaintext highlighter-rouge">try</code> block may raise a <code class="language-plaintext highlighter-rouge">ZeroDivisionError</code> exception.</p> <p>The <code class="language-plaintext highlighter-rouge">except</code> block handles the exception and prints a helpful error message to the user.</p> <p>This way, the program can continue running even when an unexpected error occurs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="c1"># code that may raise an exception
</span>    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">file.txt</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">file</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="k">except</span> <span class="nb">FileNotFoundError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle the exception
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Please provide a valid file path</span><span class="sh">"</span><span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="c1"># handle any other exceptions
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">An unexpected error occurred:</span><span class="sh">"</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, the code inside the try block may raise a <code class="language-plaintext highlighter-rouge">FileNotFoundError</code> or any other exception.</p> <p>The first <code class="language-plaintext highlighter-rouge">except</code> block handles the FileNotFoundError and provides a helpful error message for the user.</p> <p>The second <code class="language-plaintext highlighter-rouge">except</code> block handles any other exceptions that may occur and provides a generic error message.</p> <p>This way the program can continue running even when unexpected errors occur and provide helpful error messages to the user.</p> <h2 id="-use-keyword-arguments">🔑 Use Keyword Arguments</h2> <p>Keyword arguments are a powerful feature of Python that allows you to specify default values for function arguments and make your code more readable and flexible.</p> <p>Using keyword arguments can also help you reduce the number of lines of code in your programs and make them easier to understand.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greet</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">!</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">John</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hello, John!
</span><span class="nf">greet</span><span class="p">(</span><span class="sh">"</span><span class="s">Jane</span><span class="sh">"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Hi</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: Hi, Jane!
</span></code></pre></div></div> <p>In this example, the greet function takes in two arguments: name and message. The message argument has a default value of “Hello”.</p> <p>When we call <code class="language-plaintext highlighter-rouge">greet("John")</code>, the default value of <code class="language-plaintext highlighter-rouge">"Hello"</code> is used for the message argument. But when we call <code class="language-plaintext highlighter-rouge">greet("Jane", message="Hi")</code>, the keyword argument is used instead, and the output is <code class="language-plaintext highlighter-rouge">"Hi, Jane!"</code>.</p> <h2 id="️-follow-the-zen-of-python">🧘‍♀️ Follow the Zen of Python</h2> <p>The Zen of Python is a collection of principles and guidelines for writing good Python code.</p> <p>It includes tips on writing simple, clear, and maintainable code and advice on choosing between different solutions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">this</span>

<span class="k">def</span> <span class="nf">sort_data</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Simple is better than complex
</span>    <span class="n">data</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="c1"># Readability counts
</span><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># Explicit is better than implicit
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Flat is better than nested
</span><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="n">lists</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">:</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Use meaningful names
</span><span class="k">def</span> <span class="nf">calculate_average_score</span><span class="p">(</span><span class="n">scores</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">score</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1"># One obvious way to do it
</span>    <span class="k">return</span> <span class="n">total</span> <span class="o">/</span> <span class="n">count</span>
</code></pre></div></div> <p>We follow the Zen of Python by:</p> <ul> <li>Writing straightforward code (e.g. the <code class="language-plaintext highlighter-rouge">sort_data</code> function)</li> <li>Choosing meaningful names for variables and functions (e.g. <code class="language-plaintext highlighter-rouge">calculate_average_score</code>)</li> <li>Keeping the code flat and avoiding nested structures where possible (e.g. the flatten function)</li> <li>Being explicit and transparent in our code (e.g. using return statements)</li> </ul> <h2 id="-refactor-your-code-regularly">🛠 Refactor Your Code Regularly</h2> <p>Refactoring is improving the structure and quality of your code without changing its external behaviour.</p> <p>It can help you identify areas that need improvement and make your code more maintainable over time. This can be especially important in projects with a long lifespan or requiring continuous updates.</p> <p>So you can simplify complex sections, make your code more efficient, and eliminate any redundant or unnecessary parts. You can also take advantage of new features or libraries that have become available since you wrote the original code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">number</span> <span class="ow">in</span> <span class="n">numbers</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">number</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># Refactored code
</span><span class="k">def</span> <span class="nf">calculate_sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span>
</code></pre></div></div> <p>We have refactored the <code class="language-plaintext highlighter-rouge">calculate_sum</code> function to use the built-in sum function instead of manually iterating over the numbers and adding them up. This code is more efficient and readable and takes advantage of a built-in feature of Python that can perform the same calculation.</p>]]></content><author><name></name></author><category term="code"/><category term="python"/><category term="coding"/><summary type="html"><![CDATA[Let's look at the basic rules for writing great Python code]]></summary></entry></feed>